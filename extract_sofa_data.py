#!/usr/bin/env python3
"""
é«˜æ•ˆæå– SOFA/SOFA2/susp_inf æ•°æ®å¹¶ç”Ÿæˆ Sepsis-3 äº‹ä»¶

ä½¿ç”¨ DuckDB åŠ é€Ÿçš„ pyricu è¿›è¡Œæ•°æ®æå–ï¼Œæ˜¾è‘—æå‡æ€§èƒ½ã€‚
æ•´åˆäº† Sepsis-3 äº‹ä»¶æ£€æµ‹ï¼Œä¸€æ¬¡æ€§å®Œæˆæ•°æ®æå–å’Œé˜Ÿåˆ—å®šä¹‰ã€‚

ä½¿ç”¨æ–¹æ³•:
    # æå–å•ä¸ªæ•°æ®åº“ï¼ˆé»˜è®¤ miivï¼‰
    python extract_sofa_data.py --limit 5000
    
    # æå–å¤šä¸ªæ•°æ®åº“
    python extract_sofa_data.py --databases miiv,eicu,aumc --limit 5000
    
    # æå–å…¨éƒ¨æ‚£è€…
    python extract_sofa_data.py --limit -1
    
    # æŒ‡å®šè¾“å‡ºç›®å½•ï¼ˆä¼šè‡ªåŠ¨åˆ›å»ºå­ç›®å½• miiv/, eicu/, aumc/ï¼‰
    python extract_sofa_data.py --databases miiv,eicu,aumc --output sofa2_analysis/data
    
    # è·³è¿‡ sepsis æ£€æµ‹ï¼ˆä»…æå–æ•°æ®ï¼‰
    python extract_sofa_data.py --limit 5000 --no-sepsis
"""

import sys
import time
import argparse
from pathlib import Path
import os
import logging

import pandas as pd

# è®¾ç½®è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent
SRC_PATH = PROJECT_ROOT / "pyricu" / "src"
if str(SRC_PATH) not in sys.path:
    sys.path.insert(0, str(SRC_PATH))

# ç¦ç”¨è‡ªåŠ¨ç¼“å­˜æ¸…é™¤ä»¥æå‡æ€§èƒ½
os.environ['PYRICU_AUTO_CLEAR_CACHE'] = 'False'

# è®¾ç½® pyricu æ—¥å¿—çº§åˆ«ä¸º WARNINGï¼Œéšè— INFO æ—¥å¿—
logging.getLogger('pyricu').setLevel(logging.WARNING)

from pyricu import load_concepts


# æ•°æ®åº“é…ç½®
DATABASE_CONFIG = {
    'miiv': {
        'data_path': '/home/1_publicData/icu_databases/mimiciv/3.1',
        'id_column': 'stay_id',
        'icustays_file': 'icustays.parquet',
        'total_patients': 94458,
    },
    'eicu': {
        'data_path': '/home/1_publicData/icu_databases/eicu/2.0.1',
        'id_column': 'patientunitstayid',
        'icustays_file': 'patient.parquet',
        'total_patients': 200859,
    },
    'aumc': {
        'data_path': '/home/1_publicData/icu_databases/aumc/1.0.2',
        'id_column': 'admissionid',
        'icustays_file': 'admissions.parquet',
        'total_patients': 23106,
    },
}


def parse_args():
    parser = argparse.ArgumentParser(
        description="é«˜æ•ˆæå– SOFA/SOFA2/susp_inf æ•°æ®å¹¶ç”Ÿæˆ Sepsis-3 äº‹ä»¶",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        '--limit', '-l',
        type=int,
        default=5000,
        help='æ‚£è€…æ•°é‡é™åˆ¶ï¼Œ-1 è¡¨ç¤ºå…¨éƒ¨ (é»˜è®¤: 5000)'
    )
    parser.add_argument(
        '--output', '-o',
        type=str,
        default='sofa2_analysis/data',
        help='è¾“å‡ºç›®å½• (é»˜è®¤: sofa2_analysis/data)'
    )
    parser.add_argument(
        '--databases', '-d',
        type=str,
        default='miiv',
        help='æ•°æ®åº“åç§°ï¼Œé€—å·åˆ†éš” (é»˜è®¤: miivï¼Œå¯é€‰: miiv,eicu,aumc)'
    )
    parser.add_argument(
        '--interval',
        type=str,
        default='1h',
        help='æ—¶é—´é—´éš” (é»˜è®¤: 1h)'
    )
    parser.add_argument(
        '--workers', '-w',
        type=int,
        default=None,
        help='å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•° (é»˜è®¤: è‡ªåŠ¨)'
    )
    parser.add_argument(
        '--no-progress',
        action='store_true',
        help='ç¦ç”¨è¿›åº¦æ¡'
    )
    parser.add_argument(
        '--no-sepsis',
        action='store_true',
        help='è·³è¿‡ Sepsis-3 äº‹ä»¶æ£€æµ‹ï¼ˆä»…æå–åŸå§‹æ•°æ®ï¼‰'
    )
    parser.add_argument(
        '--time-window',
        type=float,
        default=48.0,
        help='è„“æ¯’ç—‡æ£€æµ‹æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰ï¼Œåªè€ƒè™‘å…¥é™¢åè¿™æ®µæ—¶é—´å†…çš„è„“æ¯’ç—‡ (é»˜è®¤: 48)'
    )
    return parser.parse_args()


def get_patient_ids(database: str, limit: int = None) -> tuple:
    """è·å–æ‚£è€… ID åˆ—è¡¨
    
    Returns:
        tuple: (patient_ids, id_column, data_path)
    """
    config = DATABASE_CONFIG[database]
    data_path = Path(config['data_path'])
    id_column = config['id_column']
    icustays_file = data_path / config['icustays_file']
    
    if not icustays_file.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° ICU å…¥é™¢æ–‡ä»¶: {icustays_file}")
    
    df = pd.read_parquet(icustays_file, columns=[id_column])
    ids = df[id_column].dropna().astype(int).unique().tolist()
    ids.sort()
    
    if limit and limit > 0:
        ids = ids[:limit]
    
    return ids, id_column, data_path


def standardize(table: pd.DataFrame, value_column: str) -> pd.DataFrame:
    """ç¡®ä¿ stay_id/charttime/value åˆ—å­˜åœ¨ä¸”å‘½åä¸€è‡´"""
    if table is None or getattr(table, "empty", True):
        return pd.DataFrame(columns=["stay_id", "charttime", value_column])

    df = table.copy(deep=False)
    id_candidates = ["stay_id", "subject_id", "patientunitstayid", "admissionid", "patientid"]
    # æ·»åŠ  AUMC çš„æ—¶é—´åˆ—å (measuredat, start) å’Œ eICU çš„ offset åˆ—å
    time_candidates = ["charttime", "measuredat", "start", "index_var", "time", "starttime", 
                       "diagnosisoffset", "infusionoffset", "labresultoffset", "observationoffset",
                       "nursingchartoffset", "respchartoffset", "intakeoutputoffset"]
    # eICU offset åˆ—æ˜¯åˆ†é’Ÿä¸ºå•ä½ï¼Œéœ€è¦è½¬æ¢ä¸ºå°æ—¶
    eicu_offset_cols = ["diagnosisoffset", "infusionoffset", "labresultoffset", "observationoffset",
                        "nursingchartoffset", "respchartoffset", "intakeoutputoffset"]
    
    id_col = next((c for c in id_candidates if c in df.columns), df.columns[0])
    time_col = next((c for c in time_candidates if c in df.columns), None)
    value_col = value_column if value_column in df.columns else df.columns[-1]

    keep = [id_col]
    if time_col:
        keep.append(time_col)
    keep.append(value_col)

    df = df[keep].rename(columns={id_col: "stay_id", value_col: value_column})
    if time_col:
        df = df.rename(columns={time_col: "charttime"})
        # å¦‚æœæ˜¯ eICU çš„ offset åˆ—ï¼ˆåˆ†é’Ÿï¼‰ï¼Œè½¬æ¢ä¸ºå°æ—¶
        if time_col in eicu_offset_cols:
            df["charttime"] = df["charttime"] / 60.0  # åˆ†é’Ÿè½¬å°æ—¶
    else:
        df["charttime"] = pd.NA

    return df.dropna(subset=["stay_id"]).reset_index(drop=True)


def earliest_positive(events: pd.DataFrame, column: str) -> pd.Series:
    """è¿”å›æ¯ä¸ª stay çš„ç¬¬ä¸€ä¸ªé˜³æ€§æ—¶é—´æˆ³"""
    if events is None or column not in events.columns:
        return pd.Series(dtype="float64", name=f"{column}_onset")
    df = events[events[column].astype(bool)]
    if df.empty:
        return pd.Series(dtype="float64", name=f"{column}_onset")
    return df.groupby("stay_id")["charttime"].min().rename(f"{column}_onset")


def run_sepsis_detection(sofa: pd.DataFrame, sofa2: pd.DataFrame, susp_inf: pd.DataFrame, output_dir: Path, time_window_hours: float = 48.0):
    """è¿è¡Œ Sepsis-3 æ£€æµ‹å¹¶ä¿å­˜ç»“æœ
    
    Args:
        sofa: SOFA è¯„åˆ†æ•°æ®
        sofa2: SOFA-2 è¯„åˆ†æ•°æ®
        susp_inf: ç–‘ä¼¼æ„ŸæŸ“æ•°æ®
        output_dir: è¾“å‡ºç›®å½•
        time_window_hours: æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰ï¼Œåªè€ƒè™‘ ICU å…¥é™¢åè¿™ä¸ªæ—¶é—´èŒƒå›´å†…çš„è„“æ¯’ç—‡
                          é»˜è®¤ 48 å°æ—¶ï¼Œç¡®ä¿ä¸åŒæ•°æ®åº“çš„å…¬å¹³æ¯”è¾ƒ
    """
    from pyricu.sepsis import sep3 as sep3_detector
    from pyricu.sepsis_sofa2 import sep3_sofa2 as sep3_sofa2_detector
    
    print(f"\nğŸ”¬ è¿è¡Œ Sepsis-3 æ£€æµ‹ (æ—¶é—´çª—å£: å…¥é™¢å {time_window_hours:.0f} å°æ—¶å†…)...")
    start_time = time.time()
    
    # æ ‡å‡†åŒ–æ•°æ®
    sofa_df = standardize(sofa, "sofa")
    sofa2_df = standardize(sofa2, "sofa2")
    susp_df = standardize(susp_inf, "susp_inf")
    
    # ğŸ”§ å…³é”®ä¿®æ”¹ï¼šåªä¿ç•™å…¥é™¢å time_window_hours å°æ—¶å†…çš„æ•°æ®
    # charttime æ˜¯ç›¸å¯¹äº ICU å…¥é™¢çš„å°æ—¶æ•°ï¼Œ0 = å…¥é™¢æ—¶åˆ»
    print(f"   ç­›é€‰æ—¶é—´çª—å£: charttime âˆˆ [0, {time_window_hours}] å°æ—¶")
    
    sofa_before = len(sofa_df)
    sofa2_before = len(sofa2_df)
    susp_before = len(susp_df)
    
    sofa_df = sofa_df[(sofa_df['charttime'] >= 0) & (sofa_df['charttime'] <= time_window_hours)].copy()
    sofa2_df = sofa2_df[(sofa2_df['charttime'] >= 0) & (sofa2_df['charttime'] <= time_window_hours)].copy()
    susp_df = susp_df[(susp_df['charttime'] >= 0) & (susp_df['charttime'] <= time_window_hours)].copy()
    
    print(f"   SOFA æ•°æ®: {sofa_before:,} â†’ {len(sofa_df):,} è¡Œ ({len(sofa_df)/sofa_before*100:.1f}%)")
    print(f"   SOFA2 æ•°æ®: {sofa2_before:,} â†’ {len(sofa2_df):,} è¡Œ ({len(sofa2_df)/sofa2_before*100:.1f}%)")
    print(f"   susp_inf æ•°æ®: {susp_before:,} â†’ {len(susp_df):,} è¡Œ ({len(susp_df)/susp_before*100:.1f}%)")
    
    # è¿è¡Œæ£€æµ‹å™¨
    sep3_events = sep3_detector(
        sofa=sofa_df,
        susp_inf=susp_df,
        id_cols=["stay_id"],
        index_col="charttime",
    )
    
    sep3_sofa2_events = sep3_sofa2_detector(
        sofa2=sofa2_df,
        susp_inf_df=susp_df,
        id_cols=["stay_id"],
        index_col="charttime",
    )
    
    elapsed = time.time() - start_time
    print(f"   æ£€æµ‹è€—æ—¶: {elapsed:.2f} ç§’")
    
    # ä¿å­˜ sep3 äº‹ä»¶
    print("\nğŸ’¾ ä¿å­˜ Sepsis-3 äº‹ä»¶...")
    sep3_events.to_parquet(output_dir / "sep3_events.parquet", index=False)
    sep3_sofa2_events.to_parquet(output_dir / "sep3_sofa2_events.parquet", index=False)
    
    sep3_count = sep3_events['sep3'].sum() if 'sep3' in sep3_events.columns else 0
    sep3_sofa2_count = sep3_sofa2_events['sep3_sofa2'].sum() if 'sep3_sofa2' in sep3_sofa2_events.columns else 0
    
    print(f"   ä¼ ç»Ÿ SOFA sepsis äº‹ä»¶: {sep3_count:,}")
    print(f"   SOFA-2 sepsis äº‹ä»¶:    {sep3_sofa2_count:,}")
    
    # ç”Ÿæˆé˜Ÿåˆ—æ¯”è¾ƒ
    print("\nğŸ“Š ç”Ÿæˆé˜Ÿåˆ—æ¯”è¾ƒ...")
    sofa_onset = earliest_positive(sep3_events, "sep3")
    sofa2_onset = earliest_positive(sep3_sofa2_events, "sep3_sofa2")
    
    comparison = pd.concat([sofa_onset, sofa2_onset], axis=1)
    comparison["status"] = comparison.apply(
        lambda row: (
            "both" if pd.notna(row.get("sep3_onset")) and pd.notna(row.get("sep3_sofa2_onset"))
            else "sofa_only" if pd.notna(row.get("sep3_onset"))
            else "sofa2_only" if pd.notna(row.get("sep3_sofa2_onset"))
            else "neither"
        ),
        axis=1,
    )
    comparison["onset_delta_h"] = comparison.apply(
        lambda row: row["sep3_sofa2_onset"] - row["sep3_onset"]
        if pd.notna(row.get("sep3_onset")) and pd.notna(row.get("sep3_sofa2_onset"))
        else pd.NA,
        axis=1,
    )
    comparison = comparison.reset_index()
    comparison.to_parquet(output_dir / "sepsis_cohort_comparison.parquet", index=False)
    
    # è¾“å‡ºç»Ÿè®¡
    print("\n" + "=" * 70)
    print("ğŸ“Š é˜Ÿåˆ—æ¯”è¾ƒç»Ÿè®¡")
    print("=" * 70)
    summary = comparison["status"].value_counts()
    for status, count in summary.items():
        print(f"   {status}: {count:,}")
    
    return {
        "sep3_events": sep3_events,
        "sep3_sofa2_events": sep3_sofa2_events,
        "comparison": comparison,
    }


def extract_single_database(
    database: str,
    output_dir: Path,
    limit: int,
    interval: str,
    workers: int,
    no_progress: bool,
    no_sepsis: bool,
    time_window_hours: float = 48.0,
) -> dict:
    """æå–å•ä¸ªæ•°æ®åº“çš„æ•°æ®
    
    Args:
        time_window_hours: è„“æ¯’ç—‡æ£€æµ‹æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰ï¼Œåªè€ƒè™‘å…¥é™¢åè¿™æ®µæ—¶é—´å†…çš„è„“æ¯’ç—‡
    """
    
    print(f"\n{'=' * 70}")
    print(f"ğŸ¥ å¤„ç†æ•°æ®åº“: {database.upper()}")
    print("=" * 70)
    
    # è·å–æ‚£è€… ID å’Œé…ç½®
    patient_ids, id_column, data_path = get_patient_ids(database, limit)
    config = DATABASE_CONFIG[database]
    
    print(f"\nğŸ“Š é…ç½®ä¿¡æ¯:")
    print(f"   æ•°æ®åº“: {database}")
    print(f"   æ•°æ®è·¯å¾„: {data_path}")
    print(f"   ID åˆ—: {id_column}")
    print(f"   æ‚£è€…æ•°é‡: {len(patient_ids):,}")
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")
    print(f"   æ—¶é—´é—´éš”: {interval}")
    print(f"   Sepsisæ£€æµ‹: {'ç¦ç”¨' if no_sepsis else 'å¯ç”¨'}")
    
    # ç¡®å®šå¹¶è¡Œé…ç½®
    if workers:
        actual_workers = workers
    elif len(patient_ids) < 2000:
        actual_workers = 1  # å°è§„æ¨¡ç”¨å•çº¿ç¨‹
    elif len(patient_ids) < 10000:
        actual_workers = 8
    else:
        actual_workers = 16
    
    backend = "thread" if actual_workers == 1 else "process"
    
    # è®¡ç®—åˆ†å—å¤§å°
    # å…³é”®å‘ç°ï¼šå½“ chunk æ•°é‡ > workers æ—¶ï¼Œä¼šå¯¼è‡´å¡æ­»é—®é¢˜
    # è§£å†³æ–¹æ¡ˆï¼šç¡®ä¿ chunk æ•°é‡ â‰¤ workersï¼Œæ¯ä¸ª worker å¤„ç†ä¸€ä¸ª chunk
    chunk_size = None
    if actual_workers > 1 and len(patient_ids) > 500:
        # ç¡®ä¿ chunk æ•°é‡ = workersï¼Œé¿å…å¤šè½®è°ƒåº¦å¯¼è‡´çš„å¡æ­»
        chunk_size = max(1000, (len(patient_ids) + actual_workers - 1) // actual_workers)
    
    print(f"   å¹¶è¡Œå·¥ä½œè¿›ç¨‹: {actual_workers}")
    print(f"   å¹¶è¡Œåç«¯: {backend}")
    print(f"   åˆ†å—å¤§å°: {chunk_size if chunk_size else 'ç¦ç”¨'}")
    
    # å‡†å¤‡åŠ è½½å‚æ•°
    loader_kwargs = {
        "database": database,
        "data_path": str(data_path),
        "interval": interval,
        "merge": False,
        "keep_components": True,
        "use_sofa2": True,
        "progress": not no_progress,
        "parallel_workers": actual_workers,
        "parallel_backend": backend,
        "concept_workers": 1,
    }
    
    if chunk_size:
        loader_kwargs["chunk_size"] = chunk_size
    
    # å¼€å§‹æå–
    print(f"\nğŸ”„ å¼€å§‹æå–æ•°æ®...")
    # åŠ è½½åŸºç¡€æ¦‚å¿µ
    concepts = ["sofa", "sofa2", "susp_inf"]
    
    start_time = time.time()
    
    results = load_concepts(
        concepts,
        patient_ids={id_column: patient_ids},
        **loader_kwargs,
    )
    
    extract_elapsed = time.time() - start_time
    
    # æå–ç»“æœ
    def extract_frame(name: str) -> pd.DataFrame:
        if isinstance(results, dict):
            frame = results.get(name)
        else:
            frame = None
        
        if frame is None:
            return pd.DataFrame()
        if hasattr(frame, "dataframe"):
            return frame.dataframe()
        if hasattr(frame, "data"):
            return frame.data.copy()
        return frame if isinstance(frame, pd.DataFrame) else pd.DataFrame()
    
    sofa = extract_frame("sofa")
    sofa2 = extract_frame("sofa2")
    susp_inf = extract_frame("susp_inf")
    
    # ä¿å­˜åŸå§‹æ•°æ®
    print(f"\nğŸ’¾ ä¿å­˜åŸå§‹æ•°æ®...")
    sofa.to_parquet(output_dir / "sofa.parquet", index=False)
    sofa2.to_parquet(output_dir / "sofa2.parquet", index=False)
    susp_inf.to_parquet(output_dir / "susp_inf.parquet", index=False)
    
    # è¾“å‡ºæå–ç»Ÿè®¡
    print("\n" + "=" * 70)
    print(f"ğŸ“Š [{database.upper()}] æ•°æ®æå–ç»“æœ")
    print("=" * 70)
    print(f"   SOFA:     {len(sofa):>10,} è¡Œ")
    print(f"   SOFA2:    {len(sofa2):>10,} è¡Œ")
    print(f"   susp_inf: {len(susp_inf):>10,} è¡Œ")
    print(f"   æ€»è®¡:     {len(sofa) + len(sofa2) + len(susp_inf):>10,} è¡Œ")
    print()
    print(f"â±ï¸  æå–è€—æ—¶: {extract_elapsed:.2f} ç§’ ({extract_elapsed/60:.1f} åˆ†é’Ÿ)")
    print(f"ğŸ“ˆ é€Ÿåº¦: {len(patient_ids)/extract_elapsed:.1f} æ‚£è€…/ç§’")
    
    # è¿è¡Œ Sepsis-3 æ£€æµ‹
    sepsis_results = None
    if not no_sepsis:
        sepsis_results = run_sepsis_detection(sofa, sofa2, susp_inf, output_dir, time_window_hours)
    
    # æ€»è€—æ—¶
    total_elapsed = time.time() - start_time
    
    print(f"\nâœ… [{database.upper()}] å®Œæˆ!")
    print(f"   æ€»è€—æ—¶: {total_elapsed:.2f} ç§’ ({total_elapsed/60:.1f} åˆ†é’Ÿ)")
    print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶ ({output_dir}):")
    print("   - sofa.parquet")
    print("   - sofa2.parquet")
    print("   - susp_inf.parquet")
    if not no_sepsis:
        print("   - sep3_events.parquet")
        print("   - sep3_sofa2_events.parquet")
        print("   - sepsis_cohort_comparison.parquet")
    
    # é¢„ä¼°å…¨åº“æ—¶é—´
    if limit and limit > 0:
        total_patients = config['total_patients']
        estimated_full = total_elapsed * (total_patients / len(patient_ids))
        print(f"\nğŸ“Š é¢„ä¼°å…¨åº“ ({total_patients:,} æ‚£è€…) å¤„ç†æ—¶é—´: {estimated_full/60:.1f} åˆ†é’Ÿ")
    
    return {
        "database": database,
        "sofa": sofa,
        "sofa2": sofa2,
        "susp_inf": susp_inf,
        "sepsis_results": sepsis_results,
        "extract_elapsed": extract_elapsed,
        "total_elapsed": total_elapsed,
        "patient_count": len(patient_ids),
    }


def main():
    args = parse_args()
    
    # è§£ææ•°æ®åº“åˆ—è¡¨
    databases = [db.strip() for db in args.databases.split(',')]
    
    # éªŒè¯æ•°æ®åº“åç§°
    for db in databases:
        if db not in DATABASE_CONFIG:
            print(f"âŒ æœªçŸ¥æ•°æ®åº“: {db}")
            print(f"   æ”¯æŒçš„æ•°æ®åº“: {', '.join(DATABASE_CONFIG.keys())}")
            sys.exit(1)
    
    limit = None if args.limit < 0 else args.limit
    
    print("=" * 70)
    print("ğŸš€ SOFA/SOFA2/susp_inf æ•°æ®æå– & Sepsis-3 äº‹ä»¶æ£€æµ‹")
    print("=" * 70)
    print(f"\nğŸ“‹ å¾…å¤„ç†æ•°æ®åº“: {', '.join(databases)}")
    
    all_results = {}
    global_start = time.time()
    
    for database in databases:
        # æ¯ä¸ªæ•°æ®åº“çš„è¾“å‡ºå­ç›®å½•
        output_dir = PROJECT_ROOT / args.output / database
        output_dir.mkdir(parents=True, exist_ok=True)
        
        result = extract_single_database(
            database=database,
            output_dir=output_dir,
            limit=limit,
            interval=args.interval,
            workers=args.workers,
            no_progress=args.no_progress,
            no_sepsis=args.no_sepsis,
            time_window_hours=args.time_window,
        )
        all_results[database] = result
    
    # æ€»ç»“
    global_elapsed = time.time() - global_start
    
    print("\n" + "=" * 70)
    print("ğŸ‰ å…¨éƒ¨å®Œæˆ!")
    print("=" * 70)
    print(f"\nğŸ“Š æ±‡æ€»ç»Ÿè®¡:")
    for db, result in all_results.items():
        print(f"   [{db.upper()}] {result['patient_count']:,} æ‚£è€…, "
              f"è€—æ—¶ {result['total_elapsed']:.1f}s")
    print(f"\nâ±ï¸  æ€»è€—æ—¶: {global_elapsed:.2f} ç§’ ({global_elapsed/60:.1f} åˆ†é’Ÿ)")
    
    return all_results


if __name__ == "__main__":
    main()
