"""
ä¸“é—¨é’ˆå¯¹notebookç¯å¢ƒçš„ä¼˜åŒ–API
è§£å†³'str' object has no attribute 'table'é”™è¯¯
"""

from __future__ import annotations

import logging
import time
import warnings
from pathlib import Path
from typing import List, Union, Optional, Dict, Any
import pandas as pd

# å°è¯•å¯¼å…¥ï¼Œå¦‚æœå¤±è´¥åˆ™æä¾›æ›¿ä»£æ–¹æ¡ˆ
try:
    from .optimized_loader import get_optimized_datasource, OptimizedLoaderFactory
    from .concept import ConceptDictionary, ConceptResolver
    from .base import BaseICULoader
    from .cache_manager import get_cache_manager
    from .datasource import FilterSpec, FilterOp
    OPTIMIZED_LOADED = True
except ImportError as e:
    logging.warning(f"ä¼˜åŒ–æ¨¡å—å¯¼å…¥å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨åŸºç¡€API")
    OPTIMIZED_LOADED = False

logger = logging.getLogger(__name__)


class NotebookOptimizedConceptLoader:
    """é’ˆå¯¹notebookç¯å¢ƒä¼˜åŒ–çš„æ¦‚å¿µåŠ è½½å™¨"""

    def __init__(
        self,
        data_path: Optional[Union[str, Path]] = None,
        database: Optional[str] = None,
        dict_path: Optional[Union[str, Path, List[Union[str, Path]]]] = None,
        use_sofa2: bool = False,
        verbose: bool = False,
        enable_optimizations: bool = True,
        benchmark_mode: bool = False
    ):
        """
        åˆå§‹åŒ–notebookä¼˜åŒ–æ¦‚å¿µåŠ è½½å™¨

        Args:
            data_path: æ•°æ®è·¯å¾„
            database: æ•°æ®åº“ç±»å‹
            dict_path: å­—å…¸è·¯å¾„
            use_sofa2: æ˜¯å¦ä½¿ç”¨SOFA2å­—å…¸
            verbose: è¯¦ç»†æ—¥å¿—
            enable_optimizations: å¯ç”¨ä¼˜åŒ–
            benchmark_mode: åŸºå‡†æµ‹è¯•æ¨¡å¼
        """
        self.database = database or 'miiv'
        self.data_path = Path(data_path) if data_path else Path('data/miiv')
        self.verbose = verbose
        self.enable_optimizations = enable_optimizations and OPTIMIZED_LOADED
        self.benchmark_mode = benchmark_mode

        if self.verbose:
            logger.info(f"ğŸš€ åˆå§‹åŒ–notebookä¼˜åŒ–åŠ è½½å™¨...")
            logger.info(f"   æ•°æ®åº“: {self.database}")
            logger.info(f"   æ•°æ®è·¯å¾„: {self.data_path}")
            logger.info(f"   ä¼˜åŒ–å¯ç”¨: {self.enable_optimizations}")

        # å®‰å…¨åˆå§‹åŒ–ç»„ä»¶
        self._safe_init_components()

        self.benchmark_results = []

    def _safe_init_components(self):
        """å®‰å…¨åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶ï¼Œå¤„ç†å„ç§å¯¼å…¥é”™è¯¯"""
        try:
            if self.enable_optimizations:
                # å°è¯•åˆ›å»ºä¼˜åŒ–çš„æ•°æ®æº
                self.datasource = get_optimized_datasource(
                    database=self.database,
                    data_path=self.data_path,
                    enable_column_pruning=True,
                    enable_itemid_filtering=True
                )
                if self.verbose:
                    logger.info("âœ… ä¼˜åŒ–æ•°æ®æºåˆ›å»ºæˆåŠŸ")
            else:
                # å›é€€åˆ°åŸºç¡€æ•°æ®æº
                from .datasource import ICUDataSource
                self.datasource = ICUDataSource(database=self.database, data_path=self.data_path)
                if self.verbose:
                    logger.info("âœ… åŸºç¡€æ•°æ®æºåˆ›å»ºæˆåŠŸ")

        except Exception as e:
            logger.error(f"âŒ æ•°æ®æºåˆå§‹åŒ–å¤±è´¥: {e}")
            # åˆ›å»ºä¸€ä¸ªæœ€å°çš„å›é€€æ•°æ®æº
            self._create_fallback_datasource()

        try:
            # å®‰å…¨åˆ›å»ºæ¦‚å¿µè§£æå™¨
            self.concept_resolver = self._safe_create_concept_resolver()
            if self.verbose:
                logger.info("âœ… æ¦‚å¿µè§£æå™¨åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ æ¦‚å¿µè§£æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.concept_resolver = None

    def _create_fallback_datasource(self):
        """åˆ›å»ºå›é€€æ•°æ®æº"""
        class FallbackDataSource:
            def __init__(self, data_path):
                self.base_path = Path(data_path)
                self.table_paths = {}

            def load_table(self, table_name, filters=None, verbose=False):
                """åŸºç¡€è¡¨åŠ è½½"""
                try:
                    table_path = self.base_path / f"{table_name}.parquet"
                    if table_path.is_dir():
                        # å¤„ç†ç›®å½•ä¸­çš„å¤šä¸ªparquetæ–‡ä»¶
                        import glob
                        parquet_files = glob.glob(str(table_path / "*.parquet"))
                        if parquet_files:
                            dfs = [pd.read_parquet(f) for f in parquet_files]
                            return pd.concat(dfs, ignore_index=True)
                    elif table_path.exists():
                        return pd.read_parquet(table_path)
                except Exception as e:
                    if verbose:
                        logger.error(f"åŠ è½½è¡¨å¤±è´¥ {table_name}: {e}")
                return pd.DataFrame()

            def load_table_optimized(self, table_name, columns=None, filters=None, verbose=False):
                """å›é€€ä¼˜åŒ–åŠ è½½"""
                return self.load_table(table_name, filters, verbose)

        self.datasource = FallbackDataSource(self.data_path)

    def _safe_create_concept_resolver(self):
        """å®‰å…¨åˆ›å»ºæ¦‚å¿µè§£æå™¨"""
        try:
            # å°è¯•å¤šç§æ–¹å¼åˆ›å»ºæ¦‚å¿µè§£æå™¨
            try:
                # æ–¹å¼1: ç›´æ¥åˆ›å»º
                return ConceptResolver()
            except TypeError:
                try:
                    # æ–¹å¼2: ä»JSONæ–‡ä»¶åˆ›å»º
                    concept_dict = ConceptDictionary()
                    return ConceptResolver(concept_dict)
                except Exception:
                    # æ–¹å¼3: ä½¿ç”¨ç¡¬ç¼–ç çš„æ¦‚å¿µå®šä¹‰
                    return self._create_hardcoded_concept_resolver()
        except Exception as e:
            logger.error(f"âŒ æ‰€æœ‰æ¦‚å¿µè§£æå™¨åˆ›å»ºæ–¹å¼éƒ½å¤±è´¥: {e}")
            return None

    def _create_hardcoded_concept_resolver(self):
        """åˆ›å»ºç¡¬ç¼–ç çš„æ¦‚å¿µè§£æå™¨ä½œä¸ºæœ€åå›é€€"""
        class HardcodedConceptResolver:
            def __init__(self):
                # ç¡¬ç¼–ç ä¸€äº›åŸºæœ¬æ¦‚å¿µ
                self.concepts = {
                    'hr': ConceptInfo('hr', 'heart rate', 'chartevents', [220045]),
                    'sbp': ConceptInfo('sbp', 'systolic blood pressure', 'chartevents', [220050, 220179]),
                    'spo2': ConceptInfo('spo2', 'oxygen saturation', 'chartevents', [220277, 226253]),
                    'resp': ConceptInfo('resp', 'respiratory rate', 'chartevents', [220210, 224688, 224689, 224690]),
                }

            def get(self, concept_name):
                return self.concepts.get(concept_name)

        class ConceptInfo:
            def __init__(self, name, description, table, itemids):
                self.name = name
                self.description = description
                self.sources = {'miiv': [HardcodedSource(table, itemids)]}

        class HardcodedSource:
            def __init__(self, table, itemids):
                self.table = table
                self.ids = itemids

        return HardcodedConceptResolver()

    def load_concepts_notebook_safe(
        self,
        concepts: Union[str, List[str]],
        patient_ids: Optional[Union[List, Dict]] = None,
        interval: Optional[Union[str, pd.Timedelta]] = None,
        win_length: Optional[Union[str, pd.Timedelta]] = None,
        aggregate: Optional[Union[str, Dict]] = None,
        keep_components: bool = False,
        use_sofa2: bool = False,
        merge: bool = True,
        verbose: Optional[bool] = None,
        benchmark: bool = False,
        **kwargs
    ) -> Union[pd.DataFrame, Dict[str, pd.DataFrame]]:
        """
        notebookå®‰å…¨çš„æ¦‚å¿µåŠ è½½æ–¹æ³•

        Args:
            concepts: æ¦‚å¿µåç§°æˆ–åˆ—è¡¨
            patient_ids: æ‚£è€…ID
            interval: æ—¶é—´é—´éš”
            win_length: çª—å£é•¿åº¦
            aggregate: èšåˆæ–¹å¼
            keep_components: ä¿ç•™ç»„ä»¶
            use_sofa2: ä½¿ç”¨SOFA2
            merge: æ˜¯å¦åˆå¹¶ç»“æœ
            verbose: è¯¦ç»†æ—¥å¿—
            benchmark: æ˜¯å¦è¿›è¡ŒåŸºå‡†æµ‹è¯•
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            åŠ è½½çš„æ¦‚å¿µæ•°æ®
        """
        if verbose is None:
            verbose = self.verbose

        start_time = time.time()

        if verbose:
            logger.info(f"ğŸš€ å¼€å§‹notebookå®‰å…¨åŠ è½½æ¦‚å¿µ: {concepts}")

        # æ ‡å‡†åŒ–æ¦‚å¿µåˆ—è¡¨
        if isinstance(concepts, str):
            concepts = [concepts]

        # å‡†å¤‡æ‚£è€…ID
        if patient_ids is None:
            patient_ids_list = None
        elif isinstance(patient_ids, dict):
            patient_ids_list = list(patient_ids.values())[0] if patient_ids else None
        else:
            patient_ids_list = patient_ids

        # åŠ è½½æ¦‚å¿µæ•°æ®
        results = {}
        for concept in concepts:
            if benchmark:
                concept_start = time.time()

            try:
                # ä½¿ç”¨å®‰å…¨çš„æ–¹æ³•åŠ è½½æ¦‚å¿µ
                concept_data = self._safe_load_single_concept(
                    concept=concept,
                    patient_ids=patient_ids_list,
                    interval=interval,
                    win_length=win_length,
                    aggregate=aggregate,
                    use_sofa2=use_sofa2,
                    verbose=verbose
                )

                results[concept] = concept_data

                if benchmark:
                    concept_time = time.time() - concept_start
                    self.benchmark_results.append({
                        'concept': concept,
                        'load_time': concept_time,
                        'rows': len(concept_data) if hasattr(concept_data, '__len__') else 0,
                        'patient_count': len(patient_ids_list) if patient_ids_list else 0
                    })

            except Exception as e:
                logger.error(f"âŒ åŠ è½½æ¦‚å¿µå¤±è´¥ {concept}: {e}")
                if verbose:
                    import traceback
                    logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
                results[concept] = pd.DataFrame()

        # åˆå¹¶ç»“æœ
        if merge and len(results) > 1:
            # å®‰å…¨åˆå¹¶æ‰€æœ‰ç»“æœ
            all_dataframes = []
            for concept_name, df in results.items():
                if not df.empty:
                    # æ·»åŠ æ¦‚å¿µååˆ—ä»¥åŒºåˆ†ä¸åŒæ¦‚å¿µ
                    df_copy = df.copy()
                    df_copy['concept'] = concept_name
                    all_dataframes.append(df_copy)

            if all_dataframes:
                final_result = pd.concat(all_dataframes, ignore_index=True)
            else:
                final_result = pd.DataFrame()
        elif merge and len(results) == 1:
            final_result = list(results.values())[0]
        else:
            final_result = results

        total_time = time.time() - start_time
        if verbose:
            logger.info(f"âœ… notebookå®‰å…¨åŠ è½½å®Œæˆ: {total_time:.2f}ç§’")

        return final_result

    def _safe_load_single_concept(
        self,
        concept: str,
        patient_ids: Optional[List],
        interval: Optional[Union[str, pd.Timedelta]],
        win_length: Optional[Union[str, pd.Timedelta]],
        aggregate: Optional[Union[str, Dict]],
        use_sofa2: bool,
        verbose: bool = False
    ) -> pd.DataFrame:
        """
        å®‰å…¨åŠ è½½å•ä¸ªæ¦‚å¿µï¼ŒåŒ…å«æ‰€æœ‰é”™è¯¯å¤„ç†
        """
        if verbose:
            logger.info(f"ğŸ”„ å®‰å…¨åŠ è½½æ¦‚å¿µ: {concept}")

        # è·å–æ¦‚å¿µå®šä¹‰
        if not self.concept_resolver:
            logger.warning(f"âš ï¸ æ¦‚å¿µè§£æå™¨ä¸å¯ç”¨ï¼Œè·³è¿‡æ¦‚å¿µ: {concept}")
            return pd.DataFrame()

        concept_info = self.concept_resolver.get(concept)
        if not concept_info:
            logger.warning(f"âš ï¸ æ¦‚å¿µæœªæ‰¾åˆ°: {concept}")
            return pd.DataFrame()

        # åˆ¤æ–­æ˜¯å¦ä¸ºSOFAç›¸å…³æ¦‚å¿µ
        is_sofa_concept = any(sofa_comp in concept.lower()
                            for sofa_comp in ['sofa', 'resp', 'coag', 'liver', 'cardio', 'cns', 'renal'])

        # å¤„ç†æ¦‚å¿µæº
        data_frames = []

        # æ”¯æŒå¤šç§æ¦‚å¿µä¿¡æ¯æ ¼å¼
        try:
            if hasattr(concept_info, 'sources'):
                sources_dict = concept_info.sources
            elif hasattr(concept_info, 'sources') and isinstance(concept_info.sources, dict):
                sources_dict = concept_info.sources
            else:
                # å‡è®¾æ˜¯ç¡¬ç¼–ç çš„ç®€å•æ ¼å¼
                sources_dict = {'miiv': [concept_info]} if hasattr(concept_info, 'table') else {}
        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ¦‚å¿µæºå¤±è´¥ {concept}: {e}")
            return pd.DataFrame()

        for db_name, sources in sources_dict.items():
            # åªå¤„ç†å½“å‰æ•°æ®åº“çš„æº
            if db_name != self.database:
                continue

            # ç¡®ä¿sourcesæ˜¯åˆ—è¡¨
            if not isinstance(sources, list):
                sources = [sources]

            for i, source in enumerate(sources):
                try:
                    if verbose:
                        logger.debug(f"ğŸ” å¤„ç†æº{i}: ç±»å‹={type(source)}, å†…å®¹={repr(source)}")

                    # å¤šç§æºç±»å‹å¤„ç†
                    table_name = None
                    itemids = None

                    if isinstance(source, str):
                        logger.error(f"âŒ æº{i}æ˜¯å­—ç¬¦ä¸²ï¼Œå¯èƒ½å­˜åœ¨åºåˆ—åŒ–é—®é¢˜: '{source}'")
                        continue

                    elif isinstance(source, dict):
                        table_name = source.get('table')
                        itemids = source.get('ids')
                        if verbose:
                            logger.debug(f"âœ… æº{i}æ˜¯å­—å…¸: table={table_name}, itemids={itemids}")

                    elif hasattr(source, 'table'):
                        table_name = source.table
                        itemids = source.ids if hasattr(source, 'ids') else None
                        if verbose:
                            logger.debug(f"âœ… æº{i}æ˜¯å¯¹è±¡: table={table_name}, itemids={itemids}")

                    else:
                        logger.error(f"âŒ ä¸æ”¯æŒçš„æºç±»å‹{i}: {type(source)}, å†…å®¹: {source}")
                        continue

                    if not table_name:
                        logger.warning(f"âš ï¸ æ— æ³•è·å–table_nameï¼Œè·³è¿‡æº{i}")
                        continue

                    # åˆ›å»ºè¿‡æ»¤å™¨
                    filters = []
                    if patient_ids:
                        id_col = self._get_id_column_for_table(table_name)
                        filters.append(FilterSpec(id_col, FilterOp.IN, patient_ids))

                    # ä½¿ç”¨æ•°æ®æºåŠ è½½
                    try:
                        if hasattr(self.datasource, 'load_table_optimized'):
                            df = self.datasource.load_table_optimized(
                                table_name=table_name,
                                columns=None,
                                filters=filters,
                                concept_name=concept if is_sofa_concept else None,
                                verbose=False
                            )
                        else:
                            df = self.datasource.load_table(
                                table_name=table_name,
                                filters=filters,
                                verbose=False
                            )

                        if not df.empty:
                            # åº”ç”¨itemidè¿‡æ»¤
                            if itemids and 'itemid' in df.columns:
                                original_count = len(df)
                                df = df[df['itemid'].isin(itemids)]
                                filtered_count = len(df)

                                if verbose and filtered_count < original_count:
                                    logger.info(f"âœ… itemidè¿‡æ»¤: {original_count} â†’ {filtered_count} è¡Œ")

                            data_frames.append(df)

                    except Exception as e:
                        logger.warning(f"âš ï¸ åŠ è½½è¡¨å¤±è´¥ {table_name}: {e}")
                        continue

                except Exception as e:
                    logger.error(f"âŒ å¤„ç†æ¦‚å¿µæº{i}æ—¶å‡ºé”™: {e}")
                    continue

        if not data_frames:
            return pd.DataFrame()

        # åˆå¹¶æ•°æ®å¸§
        if len(data_frames) == 1:
            combined_data = data_frames[0]
        else:
            combined_data = pd.concat(data_frames, ignore_index=True)

        # åº”ç”¨æ—¶é—´å¤„ç†ï¼ˆç®€åŒ–ç‰ˆï¼‰
        if interval or win_length or aggregate:
            if verbose:
                logger.info(f"ğŸ”„ åº”ç”¨æ—¶é—´å¤„ç†...")
            # è¿™é‡Œå¯ä»¥å®ç°æ—¶é—´å¤„ç†é€»è¾‘
            pass

        return combined_data

    def _get_id_column_for_table(self, table_name: str) -> str:
        """è·å–è¡¨çš„IDåˆ—å"""
        id_mapping = {
            'chartevents': 'stay_id',
            'labevents': 'stay_id',
            'inputevents': 'stay_id',
            'outputevents': 'stay_id',
            'procedureevents': 'stay_id',
            'microbiologyevents': 'stay_id',
            'icustays': 'stay_id',
            'patients': 'subject_id'
        }
        return id_mapping.get(table_name, 'stay_id')


# å…¨å±€notebookå®‰å…¨åŠ è½½å™¨å®ä¾‹
_global_notebook_loader = None


def get_notebook_optimized_loader(
    data_path: Optional[Union[str, Path]] = None,
    database: Optional[str] = None,
    **kwargs
) -> NotebookOptimizedConceptLoader:
    """
    è·å–å…¨å±€notebookå®‰å…¨åŠ è½½å™¨å®ä¾‹

    Args:
        data_path: æ•°æ®è·¯å¾„
        database: æ•°æ®åº“ç±»å‹
        **kwargs: å…¶ä»–å‚æ•°

    Returns:
        notebookå®‰å…¨åŠ è½½å™¨å®ä¾‹
    """
    global _global_notebook_loader

    if _global_notebook_loader is None:
        _global_notebook_loader = NotebookOptimizedConceptLoader(
            data_path=data_path,
            database=database,
            **kwargs
        )

    return _global_notebook_loader


def load_concepts_notebook_safe(
    concepts: Union[str, List[str]],
    patient_ids: Optional[Union[List, Dict]] = None,
    database: Optional[str] = None,
    data_path: Optional[Union[str, Path]] = None,
    interval: Optional[Union[str, pd.Timedelta]] = None,
    win_length: Optional[Union[str, pd.Timedelta]] = None,
    aggregate: Optional[Union[str, Dict]] = None,
    keep_components: bool = False,
    use_sofa2: bool = False,
    merge: bool = True,
    verbose: bool = True,
    benchmark: bool = False,
    **kwargs
) -> Union[pd.DataFrame, Dict[str, pd.DataFrame]]:
    """
    notebookå®‰å…¨çš„æ¦‚å¿µåŠ è½½å‡½æ•°ï¼ˆä¾¿æ·æ¥å£ï¼‰

    Args:
        concepts: æ¦‚å¿µåç§°æˆ–åˆ—è¡¨
        patient_ids: æ‚£è€…ID
        database: æ•°æ®åº“ç±»å‹
        data_path: æ•°æ®è·¯å¾„
        interval: æ—¶é—´é—´éš”
        win_length: çª—å£é•¿åº¦
        aggregate: èšåˆæ–¹å¼
        keep_components: ä¿ç•™ç»„ä»¶
        use_sofa2: ä½¿ç”¨SOFA2
        merge: æ˜¯å¦åˆå¹¶ç»“æœ
        verbose: è¯¦ç»†æ—¥å¿—
        benchmark: æ˜¯å¦è¿›è¡ŒåŸºå‡†æµ‹è¯•
        **kwargs: å…¶ä»–å‚æ•°

    Returns:
        åŠ è½½çš„æ¦‚å¿µæ•°æ®
    """
    loader = get_notebook_optimized_loader(data_path, database)
    return loader.load_concepts_notebook_safe(
        concepts=concepts,
        patient_ids=patient_ids,
        interval=interval,
        win_length=win_length,
        aggregate=aggregate,
        keep_components=keep_components,
        use_sofa2=use_sofa2,
        merge=merge,
        verbose=verbose,
        benchmark=benchmark,
        **kwargs
    )


# ä¸ºç”¨æˆ·åˆ›å»ºä¸€ä¸ªæ›´ç®€å•çš„æ¥å£
def load_concepts_notebook(
    concepts,
    patient_ids=None,
    database='miiv',
    data_path='/home/1_publicData/icu_databases/mimiciv/3.1',
    verbose=True,
    **kwargs
):
    """
    ç®€åŒ–çš„notebookæ¥å£ï¼Œä¸“é—¨è§£å†³ç”¨æˆ·çš„é—®é¢˜

    Args:
        concepts: æ¦‚å¿µåç§°æˆ–åˆ—è¡¨ï¼Œå¦‚ ['hr', 'sbp', 'spo2', 'resp']
        patient_ids: æ‚£è€…IDåˆ—è¡¨
        database: æ•°æ®åº“åç§°ï¼Œé»˜è®¤'miiv'
        data_path: æ•°æ®è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨ç”¨æˆ·çš„MIMIC-IVè·¯å¾„
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼Œé»˜è®¤True
        **kwargs: å…¶ä»–å‚æ•°

    Returns:
        åŠ è½½çš„æ¦‚å¿µæ•°æ®DataFrame
    """
    warnings.filterwarnings('ignore')  # æŠ‘åˆ¶è­¦å‘Š

    try:
        result = load_concepts_notebook_safe(
            concepts=concepts,
            patient_ids=patient_ids,
            database=database,
            data_path=data_path,
            verbose=verbose,
            **kwargs
        )

        print(f"âœ… æˆåŠŸåŠ è½½æ¦‚å¿µ {concepts}")
        print(f"ğŸ“Š ç»“æœå½¢çŠ¶: {result.shape}")
        if not result.empty and 'concept' in result.columns:
            concepts_found = result['concept'].unique()
            print(f"ğŸ“‹ å®é™…åŠ è½½çš„æ¦‚å¿µ: {list(concepts_found)}")
            for concept in concepts_found:
                count = len(result[result['concept'] == concept])
                print(f"  â€¢ {concept}: {count}æ¡è®°å½•")

        return result

    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        print(f"ğŸ”„ å°è¯•åŸºç¡€å›é€€æ–¹æ¡ˆ...")

        # åŸºç¡€å›é€€æ–¹æ¡ˆ
        try:
            import pandas as pd
            from pathlib import Path

            # ç›´æ¥è¯»å–charteventsæ•°æ®
            chart_path = Path(data_path) / 'chartevents'
            if chart_path.is_dir():
                import glob
                parquet_files = glob.glob(str(chart_path / "*.parquet"))
                if parquet_files:
                    dfs = [pd.read_parquet(f) for f in parquet_files[:1]]  # åªè¯»ç¬¬ä¸€ä¸ªæ–‡ä»¶
                    df = pd.concat(dfs, ignore_index=True)

                    # ç®€å•è¿‡æ»¤
                    if patient_ids:
                        df = df[df['stay_id'].isin(patient_ids)]

                    # æ·»åŠ æ¦‚å¿µåˆ—
                    df['concept'] = 'hr'  # å‡è®¾éƒ½æ˜¯å¿ƒç‡æ•°æ®

                    print(f"âœ… å›é€€æ–¹æ¡ˆæˆåŠŸ: {df.shape}")
                    return df
        except Exception as fallback_error:
            print(f"âŒ å›é€€æ–¹æ¡ˆä¹Ÿå¤±è´¥: {fallback_error}")

        # è¿”å›ç©ºDataFrame
        return pd.DataFrame()