"""Concept dictionary utilities inspired by ricu."""

from __future__ import annotations

import copy
import json
import logging
import re
import operator
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib
from dataclasses import dataclass, field, replace, asdict
from pathlib import Path
from threading import RLock, local as thread_local
from typing import Dict, Iterable, List, Mapping, MutableMapping, Optional, Union

import numpy as np
import pandas as pd

from .config import DataSourceConfig
from .datasource import FilterOp, FilterSpec, ICUDataSource
from .table import ICUTable, WinTbl
from .concept_callbacks import ConceptCallbackContext, execute_concept_callback
from . import ricu_compat

logger = logging.getLogger(__name__)

# å…¨å±€è°ƒè¯•å¼€å…³ - è®¾ç½®ä¸º False å¯ä»¥å‡å°‘è¾“å‡º
DEBUG_MODE = False

# Concepts that require hourly maxima (vasoactive infusion rates)
VASO_RATE_CONCEPTS = {"dopa_rate", "dobu_rate", "epi_rate", "norepi_rate", "adh_rate"}

def _debug(msg: str) -> None:
    if DEBUG_MODE:
        logger.debug(msg)

def _safe_serialize(value):
    if isinstance(value, (str, int, float, bool)) or value is None:
        return value
    if isinstance(value, (list, tuple, set)):
        return [_safe_serialize(v) for v in value]
    if isinstance(value, dict):
        return {str(k): _safe_serialize(v) for k, v in value.items()}
    if hasattr(value, "tolist"):
        try:
            return value.tolist()
        except Exception:
            return str(value)
    return str(value)

def _default_id_columns_for_db(db_name: Optional[str]) -> List[str]:
    """Return canonical identifier columns for a given database."""

    db = (db_name or "").lower()
    mapping = {
        "eicu": ["patientunitstayid"],
        "eicu_demo": ["patientunitstayid"],
        "aumc": ["admissionid"],
        "hirid": ["patientid"],
        "sic": ["caseid"],
        "miiv": ["stay_id"],
        "mimic_demo": ["stay_id"],
    }

    if db.startswith("mimic"):
        return ["stay_id"]
    return mapping.get(db, ["stay_id"])

@dataclass
class ConceptSource:
    """Describe how to load a concept for a specific data source."""

    table: Optional[str] = None
    sub_var: Optional[str] = None
    ids: Optional[List[object]] = None
    value_var: Optional[str] = None
    unit_var: Optional[str] = None
    index_var: Optional[str] = None
    dur_var: Optional[str] = None  # æŒç»­æ—¶é—´åˆ—ï¼Œå¯èƒ½æ˜¯durationæˆ–endtime
    regex: Optional[str] = None
    class_name: Optional[str] = None
    callback: Optional[str] = None
    interval: Optional[pd.Timedelta] = None
    target: Optional[str] = None
    params: Dict[str, object] = field(default_factory=dict)

    @classmethod
    def from_mapping(cls, mapping: Mapping[str, object]) -> "ConceptSource":
        payload = dict(mapping)

        table = payload.pop("table", None)
        sub_var = payload.pop("sub_var", None)
        if isinstance(sub_var, bool):
            sub_var = None
        ids = payload.pop("ids", None)

        if ids is not None:
            if isinstance(ids, bool):
                ids_list = None
            elif isinstance(ids, (str, int, float)):
                ids_list = [ids]
            elif isinstance(ids, Iterable):
                ids_list = list(ids)
            else:
                raise TypeError("Concept source 'ids' must be scalar or iterable")
        else:
            ids_list = None

        value_var = payload.pop("value_var", payload.pop("val_var", None))
        if isinstance(value_var, bool):
            value_var = None
        unit_var = payload.pop("unit_var", payload.pop("unit", None))
        if isinstance(unit_var, bool):
            unit_var = None
        index_var = payload.pop("index_var", payload.pop("time_var", None))
        if isinstance(index_var, bool):
            index_var = None
        dur_var = payload.pop("dur_var", None)
        if isinstance(dur_var, bool):
            dur_var = None

        regex = payload.pop("regex", None)
        class_name = payload.pop("class", payload.pop("class_name", None))
        callback = payload.pop("callback", None)
        interval = payload.pop("interval", None)
        target = payload.pop("target", None)

        return cls(
            table=str(table) if table is not None else None,
            sub_var=str(sub_var) if sub_var is not None else None,
            ids=ids_list,
            value_var=str(value_var) if value_var is not None else None,
            unit_var=str(unit_var) if unit_var is not None else None,
            index_var=str(index_var) if index_var is not None else None,
            dur_var=str(dur_var) if dur_var is not None else None,
            regex=str(regex) if regex is not None else None,
            class_name=str(class_name) if class_name is not None else None,
            callback=str(callback) if callback is not None else None,
            interval=_maybe_timedelta(interval),
            target=str(target) if target is not None else None,
            params=payload,
        )

@dataclass
class ConceptDefinition:
    """Full description of a concept across multiple data sources."""

    name: str
    sources: Dict[str, List[ConceptSource]]
    units: Optional[List[str]] = None
    minimum: Optional[float] = None
    maximum: Optional[float] = None
    description: Optional[str] = None
    category: Optional[str] = None
    target: Optional[str] = None
    interval: Optional[pd.Timedelta] = None
    aggregate: Optional[object] = None
    class_name: Optional[str] = None
    callback: Optional[str] = None
    sub_concepts: List[str] = field(default_factory=list)
    family: Optional[str] = None
    depends_on: List[str] = field(default_factory=list)
    levels: Optional[List[object]] = None
    keep_components: Optional[bool] = None
    omop_id: Optional[int] = None

    @classmethod
    def from_name_and_payload(
        cls,
        name: str,
        payload: Mapping[str, object],
    ) -> "ConceptDefinition":
        raw_sources = payload.get("sources", {})
        sources: Dict[str, List[ConceptSource]] = {}
        for src_name, entries in raw_sources.items():
            sources[src_name] = [
                ConceptSource.from_mapping(entry) for entry in entries
            ]

        unit_value = payload.get("unit")
        if isinstance(unit_value, str):
            units: Optional[List[str]] = [unit_value]
        elif isinstance(unit_value, Iterable):
            units = [str(item) for item in unit_value]
        else:
            units = None

        raw_concepts = payload.get("concepts")
        if raw_concepts is None:
            sub_concepts: List[str] = []
        elif isinstance(raw_concepts, (list, tuple)):
            sub_concepts = [str(item) for item in raw_concepts]
        else:
            sub_concepts = [str(raw_concepts)]

        depends_raw = payload.get("depends_on", [])
        if isinstance(depends_raw, str):
            depends_list = [depends_raw]
        elif isinstance(depends_raw, Iterable):
            depends_list = [str(item) for item in depends_raw]
        else:
            depends_list = []

        return cls(
            name=name,
            sources=sources,
            units=units,
            minimum=_maybe_float(payload.get("min")),
            maximum=_maybe_float(payload.get("max")),
            description=payload.get("description"),
            category=payload.get("category"),
            target=payload.get("target"),
            interval=_maybe_timedelta(payload.get("interval")),
            aggregate=payload.get("aggregate"),
            class_name=payload.get("class") or payload.get("class_name"),
            callback=payload.get("callback"),
            sub_concepts=sub_concepts,
            levels=payload.get("levels"),
            keep_components=payload.get("keep_components"),
            omop_id=_maybe_int(payload.get("omopid")),
            family=payload.get("family"),
            depends_on=depends_list,
        )

    def for_data_source(self, config: DataSourceConfig) -> List[ConceptSource]:
        candidates: List[ConceptSource] = []
        keys = [config.name, *config.class_prefix]
        for key in keys:
            if key in self.sources:
                candidates.extend(self.sources[key])
        return candidates

class ConceptDictionary:
    """Container for all concept definitions."""

    def __init__(self, concepts: Mapping[str, ConceptDefinition]):
        self._concepts = dict(concepts)

    def __contains__(self, name: object) -> bool:
        return name in self._concepts

    def __getitem__(self, name: str) -> ConceptDefinition:
        return self._concepts[name]

    def get(self, name: str, default=None) -> Optional[ConceptDefinition]:
        """Get a concept by name, returning default if not found."""
        return self._concepts.get(name, default)

    def items(self):
        return self._concepts.items()

    def keys(self):
        return self._concepts.keys()

    def values(self):
        return self._concepts.values()

    def copy(self) -> "ConceptDictionary":
        """Create a shallow copy of this dictionary."""
        return ConceptDictionary(self._concepts.copy())

    def update(self, other: "ConceptDictionary") -> None:
        """Merge another dictionary into this one with per-concept granularity."""
        if not isinstance(other, ConceptDictionary):
            raise TypeError("Can only update from another ConceptDictionary")

        for name, incoming in other._concepts.items():
            if name not in self._concepts:
                self._concepts[name] = incoming
                continue

            current = self._concepts[name]

            merged_sources: Dict[str, List[ConceptSource]] = copy.deepcopy(current.sources)
            for source_name, entries in incoming.sources.items():
                merged_sources[source_name] = copy.deepcopy(entries)

            def _pick(new_value, old_value, *, allow_empty: bool = False):
                if allow_empty:
                    return copy.deepcopy(new_value) if new_value is not None else copy.deepcopy(old_value)
                if isinstance(new_value, list):
                    return copy.deepcopy(new_value) if new_value else copy.deepcopy(old_value)
                return new_value if new_value not in (None,) else old_value

            merged_definition = ConceptDefinition(
                name=name,
                sources=merged_sources,
                units=_pick(incoming.units, current.units, allow_empty=True),
                minimum=incoming.minimum if incoming.minimum is not None else current.minimum,
                maximum=incoming.maximum if incoming.maximum is not None else current.maximum,
                description=incoming.description if incoming.description is not None else current.description,
                category=incoming.category if incoming.category is not None else current.category,
                target=incoming.target if incoming.target is not None else current.target,
                interval=incoming.interval if incoming.interval is not None else current.interval,
                aggregate=incoming.aggregate if incoming.aggregate is not None else current.aggregate,
                class_name=incoming.class_name if incoming.class_name is not None else current.class_name,
                callback=incoming.callback if incoming.callback is not None else current.callback,
                sub_concepts=_pick(incoming.sub_concepts, current.sub_concepts),
                levels=_pick(incoming.levels, current.levels),
                keep_components=(
                    incoming.keep_components
                    if incoming.keep_components is not None
                    else current.keep_components
                ),
                omop_id=incoming.omop_id if incoming.omop_id is not None else current.omop_id,
                family=incoming.family if incoming.family is not None else current.family,
                depends_on=_pick(incoming.depends_on, current.depends_on, allow_empty=True),
            )

            self._concepts[name] = merged_definition

    @classmethod
    def from_payload(cls, payload: Mapping[str, object]) -> "ConceptDictionary":
        concepts = {
            name: ConceptDefinition.from_name_and_payload(name, definition)
            for name, definition in payload.items()
        }
        return cls(concepts)

    @classmethod
    def from_json(cls, file_path: str | Path) -> "ConceptDictionary":
        path = Path(file_path)
        with path.open("r", encoding="utf8") as handle:
            raw_dict = json.load(handle)
        return cls.from_payload(raw_dict)
    
    @classmethod
    def from_multiple_json(cls, file_paths: List[str | Path]) -> "ConceptDictionary":
        """ä»å¤šä¸ª JSON æ–‡ä»¶åŠ è½½æ¦‚å¿µå­—å…¸å¹¶åˆå¹¶
        
        Args:
            file_paths: JSON æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œåé¢çš„æ–‡ä»¶ä¼šè¦†ç›–å‰é¢çš„åŒåæ¦‚å¿µ
            
        Returns:
            åˆå¹¶åçš„æ¦‚å¿µå­—å…¸
            
        Examples:
            >>> dict1 = ConceptDictionary.from_multiple_json([
            ...     'data/concept-dict.json',
            ...     'data/sofa2-dict.json'
            ... ])
        """
        merged_payload = {}
        for file_path in file_paths:
            path = Path(file_path)
            with path.open("r", encoding="utf8") as handle:
                raw_dict = json.load(handle)
            # åˆå¹¶ï¼Œåé¢çš„è¦†ç›–å‰é¢çš„
            merged_payload.update(raw_dict)
        return cls.from_payload(merged_payload)

class ConceptResolver:
    """Resolve concept definitions into concrete tabular data."""

    def __init__(self, dictionary: ConceptDictionary, cache_dir: Optional[Path] = None) -> None:
        self.dictionary = dictionary
        # Cache for icustays table to avoid repeated loading
        self._icustays_cache: Optional[pd.DataFrame] = None
        # Cache for ID mappings (stay_id <-> subject_id)
        self._id_mapping_cache: Optional[pd.DataFrame] = None
        # Cache for loaded tables to avoid repeated loading
        # Key: (table_name, frozenset(patient_ids), frozenset(filters))
        self._table_cache: Dict[tuple, pd.DataFrame] = {}
        self._cache_lock = RLock()
        self._concept_cache: Dict[str, ICUTable] = {}
        # ğŸš€ æ–°å¢ï¼šæ¦‚å¿µæ•°æ®ç¼“å­˜ï¼ˆé¿å…é‡å¤åŠ è½½ç›¸åŒæ¦‚å¿µï¼Œå¦‚urineï¼‰
        # Key: (concept_name, patient_ids_hash, interval, aggregate)
        self._concept_data_cache: Dict[tuple, pd.DataFrame] = {}
        # å¤šçº¿ç¨‹æ”¯æŒï¼šä½¿ç”¨çº¿ç¨‹å±€éƒ¨å­˜å‚¨é¿å…å¾ªç¯ä¾èµ–è¯¯æŠ¥
        self._thread_local = thread_local()
        # ğŸ”§ åµŒå¥—è°ƒç”¨æ·±åº¦è·Ÿè¸ªï¼šé˜²æ­¢é€’å½’æ¦‚å¿µçš„å†…éƒ¨è°ƒç”¨æ¸…é™¤ç¼“å­˜
        self._load_depth = 0
        self.cache_dir = cache_dir if cache_dir else None
        self.cache_schema_version = "1"
        self.dictionary_signature = self._compute_dictionary_signature()

    def available_concepts(self) -> List[str]:
        return sorted(self.dictionary.keys())

    def _compute_dictionary_signature(self) -> str:
        payload: Dict[str, object] = {}
        for name, definition in self.dictionary.items():
            payload[name] = {
                "callback": definition.callback,
                "aggregate": definition.aggregate,
                "sub_concepts": definition.sub_concepts,
                "sources": {
                    key: [asdict(source) for source in sources]
                    for key, sources in definition.sources.items()
                },
            }
        encoded = json.dumps(payload, sort_keys=True, default=_safe_serialize).encode("utf-8")
        return hashlib.sha1(encoded).hexdigest()

    def clear_table_cache(self) -> None:
        """Clear cached source tables."""
        with self._cache_lock:
            self._table_cache.clear()
            self._concept_cache.clear()
            self._concept_data_cache.clear()  # ğŸš€ æ¸…é™¤æ¦‚å¿µæ•°æ®ç¼“å­˜
            # æ¸…é™¤å½“å‰çº¿ç¨‹çš„inflighté›†åˆ
            if hasattr(self._thread_local, 'inflight'):
                self._thread_local.inflight.clear()

    def _get_inflight(self) -> set:
        """è·å–å½“å‰çº¿ç¨‹çš„inflighté›†åˆï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        if not hasattr(self._thread_local, 'inflight'):
            self._thread_local.inflight = set()
        return self._thread_local.inflight

    def _should_fill_gaps(self, concept_name: str, definition: ConceptDefinition) -> bool:
        category = (definition.category or "").lower() if definition.category else ""
        concept = concept_name.lower()

        raw_class = getattr(definition, "class_name", None)
        class_names: List[str] = []
        if isinstance(raw_class, str):
            class_names = [raw_class.lower()]
        elif isinstance(raw_class, Iterable):
            class_names = [str(item).lower() for item in raw_class if item]
        else:
            class_names = []

        # Never fill gaps for logical/boolean concepts (abx, samp, etc.)
        # These are event indicators that should remain sparse
        if "lgl_cncpt" in class_names:
            return False
        
        # ğŸ”§ CRITICAL FIX 2024-12: Do NOT fill gaps for medication rate concepts
        # These concepts (norepi_rate, dobu_rate, etc.) have interval data (start/end times)
        # and are already correctly expanded by expand() in _apply_aggregation.
        # Global fill_gaps with ffill would incorrectly fill across DISCONTINUOUS time segments.
        # Example: Patient with norepi from hour 8-150 and hour 980-982 would have
        # hours 151-979 incorrectly filled with the value from hour 150.
        # This caused pyricu coverage (90%) >> ricu coverage (36%) for norepi_rate.
        # Solution: disable global fill_gaps; ricu handles this per-segment in expand().
        if concept.endswith('_rate') or concept.endswith('_equiv'):
            return False  # Changed from True to False
        
        # ğŸ”§ CRITICAL FIX: Do NOT fill gaps for vent_ind
        # R ricu's vent_ind callback only returns time points where ventilation is active.
        # It does NOT fill gaps between ventilation windows.
        # The expand() function in sofa_resp handles vent_ind expansion, not fill_gaps.
        # Filling gaps would create NaN rows for non-ventilated time points,
        # which causes row inflation (67 â†’ 157 rows for patient 30009597).
        if concept == 'vent_ind':
            return False
        
        # ğŸ”§ CRITICAL FIX 2024-12: Do NOT fill gaps for urine
        # R ricu's fill_gaps for urine only fills the FIRST continuous segment (~50 hours),
        # then only keeps original data points for later segments.
        # Simple fill_gaps fills the entire range (min_time to max_time), which is wrong.
        # The urine24 callback handles the proper ricu-style segmented fill logic.
        # ONLY fill for urine24 if needed (but the callback does its own fill)
        if concept == 'urine':
            return False
        
        # urine24 doesn't need fill_gaps either - callback handles it
        if concept == 'urine24':
            return False
        
        # All other concepts: no fill_gaps by default
        return False
    
    def _get_fill_method(self, concept_name: str, definition: ConceptDefinition) -> str:
        """Determine fill method for fill_gaps.
        
        Returns:
            - 'ffill': Forward fill for medication rate concepts (locf)
            - 'none': Only fill time points, do NOT fill values (keep NaN)
        """
        concept = concept_name.lower()
        
        # Medication rate concepts need locf (last observation carried forward)
        if concept.endswith('_rate') or concept.endswith('_equiv'):
            return 'ffill'
        
        # âš ï¸ CRITICAL FIX: For urine/vent_ind, use 'none' to match ricu
        # ricu does NOT fill missing urine values with 0 - it keeps them as NaN
        # Only the time grid is filled, not the data values
        # This prevents false coverage (pyricu 100% vs ricu 2.74% for urine)
        
        # Default to none (only fill time grid, keep NaN for missing values)
        return 'none'
    
    def _expand_patient_ids(
        self, 
        patient_ids: Optional[Union[Dict[str, List], List]], 
        target_id_var: str,
        data_source: ICUDataSource,
        verbose: bool = False
    ) -> Optional[Dict[str, List]]:
        """è‡ªåŠ¨æ‰©å±• patient_ids ä»¥æ”¯æŒä¸åŒè¡¨çš„ ID åˆ—
        
        å¦‚æœç”¨æˆ·åªæä¾›äº† stay_idï¼Œä½†è¡¨éœ€è¦ subject_idï¼ˆæˆ–åä¹‹ï¼‰ï¼Œ
        è‡ªåŠ¨æŸ¥è¯¢ icustays è¡¨è·å–æ˜ å°„å…³ç³»ã€‚
        
        Args:
            patient_ids: ç”¨æˆ·æä¾›çš„æ‚£è€…IDï¼ˆdictæˆ–listï¼‰
            target_id_var: ç›®æ ‡è¡¨éœ€è¦çš„IDåˆ—åï¼ˆå¦‚ 'subject_id' æˆ– 'stay_id'ï¼‰
            data_source: æ•°æ®æº
            verbose: æ˜¯å¦æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
            
        Returns:
            æ‰©å±•åçš„ patient_ids å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„IDæ˜ å°„
            
        Examples:
            >>> # ç”¨æˆ·åªæä¾› stay_id
            >>> patient_ids = {'stay_id': [30018045]}
            >>> # è¡¨éœ€è¦ subject_id
            >>> expanded = _expand_patient_ids(patient_ids, 'subject_id', ds)
            >>> # ç»“æœ: {'stay_id': [30018045], 'subject_id': [18369403]}
        """
        if verbose and DEBUG_MODE:
            _debug(f'  ğŸ” _expand_patient_ids è¢«è°ƒç”¨')
            _debug(f'     patient_ids: {patient_ids}')
            _debug(f'     target_id_var: {target_id_var}')
        
        if not patient_ids:
            return patient_ids
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        if not isinstance(patient_ids, dict):
            # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œæ ¹æ®æ•°æ®åº“ç±»å‹é€‰æ‹©åˆé€‚çš„IDåˆ—å
            db_name = data_source.config.name if hasattr(data_source, 'config') and hasattr(data_source.config, 'name') else ''
            
            _debug(f'  patient_idsç±»å‹: {type(patient_ids)}')
            _debug(f'  db_name: {db_name}')
            
            if db_name in ['eicu', 'eicu_demo']:
                # eICUä½¿ç”¨patientunitstayid
                patient_ids = {'patientunitstayid': list(patient_ids)}
                _debug(f'  è½¬æ¢ä¸º: {patient_ids}')
            elif db_name in ['aumc']:
                # AUMCä½¿ç”¨admissionid
                patient_ids = {'admissionid': list(patient_ids)}
                _debug(f'  è½¬æ¢ä¸º: {patient_ids}')
            elif db_name in ['hirid']:
                # HiRIDä½¿ç”¨patientid
                patient_ids = {'patientid': list(patient_ids)}
                _debug(f'  è½¬æ¢ä¸º: {patient_ids}')
            else:
                # MIMIC-IVç­‰ä½¿ç”¨stay_id
                patient_ids = {'stay_id': list(patient_ids)}
                _debug(f'  è½¬æ¢ä¸º: {patient_ids}')
        else:
            patient_ids = dict(patient_ids)  # å¤åˆ¶ï¼Œé¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        
        # å¦‚æœå·²ç»åŒ…å«ç›®æ ‡IDï¼Œç›´æ¥è¿”å›
        if target_id_var in patient_ids and patient_ids[target_id_var]:
            return patient_ids
        
        # éœ€è¦è¿›è¡Œ ID è½¬æ¢
        # æ”¯æŒçš„è½¬æ¢ï¼šstay_id <-> subject_id
        if target_id_var == 'subject_id' and 'stay_id' in patient_ids:
            # éœ€è¦ä» stay_id è·å– subject_id
            source_var = 'stay_id'
            source_values = patient_ids['stay_id']
        elif target_id_var == 'stay_id' and 'subject_id' in patient_ids:
            # éœ€è¦ä» subject_id è·å– stay_id
            source_var = 'subject_id'
            source_values = patient_ids['subject_id']
        else:
            # æ— æ³•è½¬æ¢ï¼Œè¿”å›åŸå§‹å€¼
            return patient_ids
        
        if not source_values:
            return patient_ids
        
        # åŠ è½½æˆ–ä½¿ç”¨ç¼“å­˜çš„ ID æ˜ å°„è¡¨
        if self._id_mapping_cache is None:
            try:
                # eICU doesn't use icustays table
                db_name = data_source.config.name if hasattr(data_source, 'config') and hasattr(data_source.config, 'name') else ''
                if db_name in ['eicu', 'eicu_demo']:
                    # eICU uses patientunitstayid as the primary ID, no mapping needed
                    return patient_ids
                
                from .datasource import FilterSpec, FilterOp
                # åŠ è½½ icustays è¡¨ï¼ˆåªéœ€è¦ stay_id å’Œ subject_idï¼‰
                filters = [
                    FilterSpec(
                        column=source_var,
                        op=FilterOp.IN,
                        value=source_values,
                    )
                ]
                icustays_table = data_source.load_table(
                    'icustays', 
                    columns=['stay_id', 'subject_id'],
                    filters=filters,
                    verbose=False
                )
                if hasattr(icustays_table, 'data'):
                    self._id_mapping_cache = icustays_table.data[['stay_id', 'subject_id']].drop_duplicates()
                else:
                    self._id_mapping_cache = icustays_table[['stay_id', 'subject_id']].drop_duplicates()
                    
                if verbose:
                    if DEBUG_MODE: print(f"   ğŸ”— åŠ è½½ ID æ˜ å°„è¡¨: {len(self._id_mapping_cache)} æ¡è®°å½•")
            except Exception as e:
                if verbose:
                    print(f"   âš ï¸  æ— æ³•åŠ è½½ icustays è¿›è¡Œ ID è½¬æ¢: {e}")
                return patient_ids
        
        # ä»æ˜ å°„è¡¨ä¸­è·å–ç›®æ ‡ID
        mapping_df = self._id_mapping_cache
        mask = mapping_df[source_var].isin(source_values)
        target_values = mapping_df.loc[mask, target_id_var].unique().tolist()
        
        if target_values:
            patient_ids[target_id_var] = target_values
            if verbose:
                if DEBUG_MODE: print(f"   ğŸ”— ID è½¬æ¢: {source_var}={len(source_values)}ä¸ª â†’ {target_id_var}={len(target_values)}ä¸ª")
        
        return patient_ids

    def load_concepts(
        self,
        concept_names: Iterable[str],
        data_source: ICUDataSource,
        *,
        merge: bool = True,
        aggregate: Optional[Union[str, bool, Mapping[str, object]]] = None,
        patient_ids: Optional[Iterable[object]] = None,
        verbose: bool = True,
        interval: Optional[pd.Timedelta] = None,  # Default 1 hour interval
        align_to_admission: bool = True,  # Align time to ICU admission as anchor
        ricu_compatible: bool = True,  # é»˜è®¤å¯ç”¨ricu.Rå…¼å®¹æ ¼å¼
        concept_workers: int = 1,
        _batch_loading: bool = False,  # ğŸ”§ æ‰¹é‡åŠ è½½æ¨¡å¼æ ‡å¿—ï¼Œå‡å°‘è¯Šæ–­è¾“å‡º
        _skip_concept_cache: bool = False,  # ğŸ”§ è·³è¿‡æ¦‚å¿µç¼“å­˜ï¼Œç”¨äºå›è°ƒå†…éƒ¨åŠ è½½
        **kwargs,  # Additional parameters for callbacks (e.g., win_length, worst_val_fun)
    ):
        names = [name for name in concept_names]
        required_names = self._expand_dependencies(names)  # Ensure dependencies are expanded
        tables: Dict[str, ICUTable] = {}
        aggregators = self._normalise_aggregators(aggregate, required_names)
        
        # ğŸ”§ åµŒå¥—è°ƒç”¨æ·±åº¦è·Ÿè¸ªï¼šé€’å½’æ¦‚å¿µä¼šåµŒå¥—è°ƒç”¨ load_concepts
        # åªæœ‰é¡¶å±‚è°ƒç”¨æ‰åº”è¯¥æ¸…é™¤ç¼“å­˜
        is_top_level = self._load_depth == 0
        self._load_depth += 1
        
        # ğŸš€ æ€§èƒ½ä¼˜åŒ–: ä¸è¦æ¸…ç©º _concept_cacheï¼Œä¿ç•™ç”¨äºé€’å½’è°ƒç”¨çš„ç¼“å­˜
        # åªåœ¨é¡¶å±‚è°ƒç”¨æ—¶åˆå§‹åŒ–ï¼ˆæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼‰
        if not hasattr(self, '_concept_cache') or self._concept_cache is None:
            self._concept_cache = {}
        # åˆå§‹åŒ–å½“å‰çº¿ç¨‹çš„inflighté›†åˆ
        self._get_inflight().clear()

        # å­˜å‚¨æ‚£è€…IDç”¨äºricuæ ¼å¼è½¬æ¢
        self._last_patient_ids = list(patient_ids) if patient_ids else None
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šåœ¨mergeæ¨¡å¼ä¸‹ï¼Œè®¾ç½®æ ‡å¿—ä»¥ä¿ç•™NaNè¡Œï¼ŒåŒ¹é…ricuçš„å®Œæ•´æ—¶é—´ç½‘æ ¼é£æ ¼
        if merge and len(names) > 1:
            kwargs = dict(kwargs)  # å¤åˆ¶kwargsé¿å…ä¿®æ”¹åŸå§‹å­—å…¸
            kwargs['_keep_na_rows'] = True
            # è®¾ç½®æ‰¹é‡åŠ è½½æ ‡å¿—ä»¥å‡å°‘è¯Šæ–­è¾“å‡º
            if len(names) > 3:  # åªåœ¨åŠ è½½å¤šä¸ªæ¦‚å¿µæ—¶å¯ç”¨
                _batch_loading = True
                kwargs['_batch_loading'] = True

        if merge and len(names) > 1 and any(
            aggregators[name] is False for name in names
        ):
            raise ValueError(
                "Aggregation must be enabled for all concepts when merge=True."
            )

        # ğŸ”§ CRITICAL FIX: Match R ricu's default interval behavior
        # R ricu uses interval=hours(1L) by default when aggregation is enabled
        # If user specifies aggregate but not interval, default to 1 hour
        if interval is None and aggregate is not None and aggregate is not False:
            # Check if any aggregator is not False
            has_aggregation = any(agg is not False for agg in aggregators.values())
            if has_aggregation:
                interval = pd.Timedelta(hours=1)
        
        total = len(names)

        for name in names:
            if name not in self.dictionary:
                raise KeyError(f"Concept '{name}' not present in dictionary")

        def _resolve(name: str, position: int) -> tuple[str, ICUTable]:
            if verbose and logger.isEnabledFor(logging.INFO):
                logger.info("â¡ï¸  [%d/%d] åŠ è½½æ¦‚å¿µ '%s'", position, total, name)

            concept_table = self._ensure_concept_loaded(
                name,
                data_source,
                aggregators,
                patient_ids,
                verbose,
                interval,
                align_to_admission,
                kwargs,
                _skip_concept_cache=_skip_concept_cache,
            )
            if verbose and logger.isEnabledFor(logging.INFO):
                if isinstance(concept_table, ICUTable):
                    row_count = len(concept_table.data)
                elif isinstance(concept_table, pd.DataFrame):
                    row_count = len(concept_table)
                else:
                    row_count = "N/A"
                logger.info("âœ…  æ¦‚å¿µ '%s' å·²åŠ è½½ (è¡Œæ•°: %s)", name, row_count)
            return name, concept_table

        try:
            results: Dict[str, ICUTable] = {}
            if concept_workers > 1 and total > 1:
                with ThreadPoolExecutor(max_workers=concept_workers) as executor:
                    future_map = {
                        executor.submit(_resolve, name, idx): name
                        for idx, name in enumerate(names, start=1)
                    }
                    for future in as_completed(future_map):
                        name, concept_table = future.result()
                        results[name] = concept_table
            else:
                for idx, name in enumerate(names, start=1):
                    name, concept_table = _resolve(name, idx)
                    results[name] = concept_table

            tables = {
                name: results[name]
                for name in names
            }

            if not merge:
                # å¦‚æœæ˜¯ricu_compatibleæ¨¡å¼ä¸”åªæœ‰ä¸€ä¸ªæ¦‚å¿µï¼Œè¿”å›ricu.Ræ ¼å¼çš„DataFrame
                if ricu_compatible and len(tables) == 1:
                    concept_name = list(tables.keys())[0]
                    logger.debug("è°ƒè¯•ï¼šè°ƒç”¨_to_ricu_formatå¤„ç†æ¦‚å¿µ %s", concept_name)
                    # è®¡ç®—interval_hours
                    interval_hours = 1.0
                    if interval is not None:
                        if hasattr(interval, 'total_seconds'):
                            interval_hours = interval.total_seconds() / 3600.0
                        elif isinstance(interval, (int, float)):
                            interval_hours = float(interval)
                    return self._to_ricu_format(tables[concept_name], concept_name, interval_hours=interval_hours)
                return tables

            # å¦‚æœæ˜¯ricu_compatibleæ¨¡å¼ï¼Œä½¿ç”¨å¢å¼ºçš„ricué£æ ¼åˆå¹¶
            if ricu_compatible:
                return self._to_ricu_format_merged_enhanced(tables, names, interval)

            merged = self._merge_tables(tables)
            return merged
        finally:
            # ğŸ”§ åµŒå¥—è°ƒç”¨æ·±åº¦è·Ÿè¸ªï¼šå‡å°‘æ·±åº¦è®¡æ•°å™¨
            self._load_depth -= 1
            # ğŸ”§ åªæœ‰é¡¶å±‚è°ƒç”¨æ‰æ¸…é™¤ç¼“å­˜ï¼Œé¿å…é€’å½’æ¦‚å¿µå†…éƒ¨è°ƒç”¨æ¸…é™¤å¤–å±‚æ‰€éœ€çš„ç¼“å­˜
            if is_top_level:
                with self._cache_lock:
                    self._concept_cache.clear()
                    self._concept_data_cache.clear()
                    # æ¸…é™¤å½“å‰çº¿ç¨‹çš„inflighté›†åˆ
                    self._get_inflight().clear()

    def _load_single_concept(
        self,
        concept_name: str,
        data_source: ICUDataSource,
        *,
        aggregator: object,
        patient_ids: Optional[Iterable[object]],
        verbose: bool = True,
        interval: Optional[pd.Timedelta] = None,
        align_to_admission: bool = True,
        **kwargs,  # Additional parameters for callbacks
    ) -> ICUTable:
        # ğŸ”§ æ‰¹é‡åŠ è½½æ¨¡å¼ï¼šå‡å°‘è¯Šæ–­è¾“å‡º
        batch_loading = kwargs.get('_batch_loading', False)
        if batch_loading:
            verbose = False  # æ‰¹é‡åŠ è½½æ—¶æŠ‘åˆ¶verboseè¾“å‡º
        definition = self.dictionary[concept_name]
        
        # ğŸ”§ FIX: å¯¹äº rec_cncpt æ¦‚å¿µï¼ˆå¦‚ tgcsï¼‰ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®åº“ç‰¹å®šçš„ç›´æ¥ source å®šä¹‰
        # å¦‚æœæœ‰ï¼Œåº”è¯¥ä½¿ç”¨ç›´æ¥å®šä¹‰è€Œä¸æ˜¯é€’å½’åŠ è½½å­æ¦‚å¿µ
        # ä¾‹å¦‚ï¼šeICU çš„ tgcs åº”è¯¥ä» nursecharting è¡¨çš„ 'GCS Total' ç›´æ¥è¯»å–
        # è€Œä¸æ˜¯é€šè¿‡ egcs + mgcs + vgcs è®¡ç®—ï¼ˆå› ä¸º eICU æ²¡æœ‰å•ç‹¬çš„ GCS ç»„ä»¶æ•°æ®ï¼‰
        use_recursive = False
        has_direct_source = False  # æ ‡è®°æ˜¯å¦æœ‰ç›´æ¥çš„è¡¨ source
        if definition.sub_concepts:
            # æ£€æŸ¥å½“å‰æ•°æ®æºæ˜¯å¦æœ‰ç›´æ¥çš„è¡¨å®šä¹‰
            db_name = data_source.config.name if hasattr(data_source, 'config') and hasattr(data_source.config, 'name') else ''
            
            if db_name and hasattr(definition, 'sources') and db_name in definition.sources:
                db_sources = definition.sources[db_name]
                if isinstance(db_sources, list):
                    for db_source in db_sources:
                        db_source_dict = db_source.__dict__ if hasattr(db_source, '__dict__') else db_source
                        # å¦‚æœ source æœ‰ table å®šä¹‰ï¼ˆè€Œä¸ä»…ä»…æ˜¯ conceptsï¼‰ï¼Œåˆ™ä½¿ç”¨ç›´æ¥åŠ è½½
                        if 'table' in db_source_dict and db_source_dict.get('table'):
                            has_direct_source = True
                            break
            
            # åªæœ‰å½“æ²¡æœ‰ç›´æ¥ source å®šä¹‰æ—¶ï¼Œæ‰ä½¿ç”¨é€’å½’åŠ è½½
            use_recursive = not has_direct_source
        
        if use_recursive:
            return self._load_recursive_concept(
                concept_name,
                definition,
                data_source,
                aggregator=aggregator,
                patient_ids=patient_ids,
                verbose=verbose,  # ä¼ é€’verboseå‚æ•°
                interval=interval,  # Pass interval
                align_to_admission=align_to_admission,  # Pass align flag
                **kwargs,  # Pass kwargs to recursive concept
            )
        
        # Check if this concept has a concept-level callback
        # Skip callback if _bypass_callback flag is set (to avoid infinite recursion)
        # ğŸ”§ FIX: ä¹Ÿè·³è¿‡ callback å¦‚æœè¿™æ˜¯ä¸€ä¸ªæœ‰ç›´æ¥ source çš„ rec_cncpt
        # ä¾‹å¦‚ eICU çš„ tgcs - æœ‰ sum_components callback ä½†åº”è¯¥ç›´æ¥ä»è¡¨åŠ è½½
        skip_callback = kwargs.get('_bypass_callback', False) or has_direct_source
        if definition.callback and not skip_callback:
            # Try to execute the callback if it's registered
            try:
                # Create empty tables dict - callback will load dependencies if needed
                tables = {}
                
                callback_context = ConceptCallbackContext(
                    concept_name=concept_name,
                    target=definition.target,
                    interval=interval,
                    resolver=self,
                    data_source=data_source,
                    patient_ids=patient_ids,
                    kwargs=kwargs,
                )
                result = execute_concept_callback(definition.callback, tables, callback_context)
                if result is not None:
                    return result
            except NotImplementedError:
                pass
            
            # If callback not found or failed, raise error
            raise NotImplementedError(
                f"Concept '{concept_name}' relies on a concept-level callback "
                f"'{definition.callback}' that is not yet supported."
            )
        
        config = data_source.config
        sources = definition.for_data_source(config)
        if not sources:
            # For optional sub-concepts (e.g., mech_vent in eICU), return empty table
            # instead of raising error - let callback handle missing concepts
            if kwargs.get('_allow_missing_concept', False):
                # Return empty ICUTable with database-appropriate default ID columns
                db_name = config.name if hasattr(config, 'name') else 'unknown'
                default_id_cols = _default_id_columns_for_db(db_name)
                
                empty_df = pd.DataFrame(columns=default_id_cols)
                return ICUTable(
                    data=empty_df,
                    id_columns=default_id_cols,
                    index_column=None,
                    value_column=None,
                )
            
            raise KeyError(
                f"No source configuration for concept '{concept_name}' "
                f"in data source '{config.name}'"
            )

        frames: List[pd.DataFrame] = []
        id_columns: List[str] = []
        index_column: Optional[str] = None
        unit_column: Optional[str] = None
        time_columns: List[str] = []

        for source in sources:
            if source.class_name == "fun_itm":
                return self._load_fun_item(
                    concept_name,
                    definition,
                    source,
                    data_source,
                    aggregator=aggregator,
                    patient_ids=patient_ids,
                    **kwargs,  # Pass kwargs to fun_item
                )

            if source.table is None:
                raise NotImplementedError(
                    f"Concept '{concept_name}' relies on a functional item "
                    "that is not yet supported."
                )

            if source.ids is not None and not source.sub_var:
                raise ValueError(
                    f"Concept '{concept_name}' requires 'sub_var' when specifying ids."
                )
            
            if hasattr(source, 'regex') and source.regex and not source.sub_var:
                raise ValueError(
                    f"Concept '{concept_name}' requires 'sub_var' when specifying regex."
                )
            
            table_cfg = data_source.config.get_table(source.table)
            defaults = table_cfg.defaults
            
            # Build filters for sub_var (only for ids, NOT regex)
            # Regex filtering is handled later after table loading (see line ~1428)
            filters = []
            if source.ids is not None:
                filters.append(FilterSpec(
                    column=source.sub_var,
                    op=FilterOp.IN,
                    value=source.ids,
                ))
            
            # ä¿®å¤ï¼šæ·»åŠ æ‚£è€…è¿‡æ»¤å™¨
            # å³ä½¿ defaults.id_var ä¸º Noneï¼Œä»å°è¯•æ·»åŠ æ‚£è€…è¿‡æ»¤å™¨
            # å¯¹äº MIMIC-IV hosp è¡¨ï¼ˆå¦‚ microbiologyeventsï¼‰ï¼Œä½¿ç”¨ subject_id
            # å¯¹äº eICU è¡¨ï¼Œä½¿ç”¨ patientunitstayid
            effective_id_var = defaults.id_var
            if patient_ids:
                if not effective_id_var:
                    # å¦‚æœæ²¡æœ‰é…ç½® id_varï¼Œå°è¯•æ£€æµ‹å¸¸è§çš„IDåˆ—
                    # å…ˆæ£€æŸ¥æ•°æ®åº“ç±»å‹
                    db_name = data_source.config.name if hasattr(data_source, 'config') and hasattr(data_source.config, 'name') else ''
                    
                    if db_name in ['eicu', 'eicu_demo']:
                        # eICUä½¿ç”¨patientunitstayid
                        effective_id_var = 'patientunitstayid'
                    elif db_name in ['aumc']:
                        # AUMCä½¿ç”¨admissionid
                        effective_id_var = 'admissionid'
                    elif db_name in ['hirid']:
                        # HiRIDä½¿ç”¨patientid
                        effective_id_var = 'patientid'
                    elif source.table in ['patients', 'admissions']:
                        # MIMIC-IV patients/admissions è¡¨ä½¿ç”¨ subject_id
                        effective_id_var = 'subject_id'
                    elif source.table in ['microbiologyevents', 'd_labitems', 'prescriptions']:
                        # MIMIC-IV hospè¡¨ä½¿ç”¨subject_idï¼ˆlabeventsé™¤å¤–ï¼Œå®ƒåŒæ—¶æ”¯æŒstay_idå’Œsubject_idï¼‰
                        effective_id_var = 'subject_id'
                    elif source.table in ['inputevents', 'chartevents', 'outputevents', 'procedureevents']:
                        # MIMIC-IV icuè¡¨ä½¿ç”¨stay_id
                        effective_id_var = 'stay_id'
                
                if effective_id_var:
                    # ğŸ”— è‡ªåŠ¨æ‰©å±• patient_idsï¼šå¦‚æœç”¨æˆ·åªæä¾›äº† stay_id ä½†è¡¨éœ€è¦ subject_idï¼ˆæˆ–åä¹‹ï¼‰ï¼Œ
                    # è‡ªåŠ¨æŸ¥è¯¢ icustays è·å–æ˜ å°„å…³ç³»
                    expanded_patient_ids = self._expand_patient_ids(
                        patient_ids, 
                        effective_id_var, 
                        data_source,
                        verbose=verbose
                    )
                    
                    # DEBUG
                    # patient_idså¯èƒ½æ˜¯dict(åŒ…å«stay_idå’Œsubject_id)æˆ–åˆ—è¡¨
                    if isinstance(expanded_patient_ids, dict):
                        # ä½¿ç”¨å¯¹åº”åˆ—çš„ID
                        id_values = expanded_patient_ids.get(effective_id_var)
                        
                        # DEBUG
                        if id_values:
                            # âœ… å…³é”®ä¿®å¤ï¼šå¯¹äº hospital tablesï¼ˆå¦‚ labeventsï¼‰ï¼Œå¦‚æœä½¿ç”¨ subject_id è¿‡æ»¤
                            # éœ€è¦åœ¨ metadata ä¸­ä¿å­˜åŸå§‹çš„ stay_idï¼Œä¾› datasource åœ¨ join åç²¾ç¡®è¿‡æ»¤
                            metadata = None
                            db_name = data_source.config.name if hasattr(data_source, 'config') and hasattr(data_source.config, 'name') else ''
                            hospital_tables = ['labevents', 'prescriptions', 'microbiologyevents', 'emar', 'pharmacy']
                            
                            if (db_name in ['miiv', 'mimic_demo'] and 
                                source.table in hospital_tables and 
                                effective_id_var == 'subject_id' and 
                                'stay_id' in expanded_patient_ids):
                                # ä¿å­˜åŸå§‹ stay_id åˆ° metadata
                                original_stay_ids = expanded_patient_ids.get('stay_id')
                                if original_stay_ids:
                                    metadata = {'original_stay_ids': original_stay_ids}
                                    if DEBUG_MODE:
                                        print(f"   ğŸ’¾ åœ¨ subject_id è¿‡æ»¤å™¨ä¸­é™„åŠ åŸå§‹ stay_id: {len(original_stay_ids)} ä¸ª")
                            
                            filters.append(
                                FilterSpec(
                                    column=effective_id_var,
                                    op=FilterOp.IN,
                                    value=id_values,
                                    metadata=metadata,
                                )
                            )
                    else:
                        # åŸæœ‰é€»è¾‘ï¼šexpanded_patient_ids æ˜¯åˆ—è¡¨ï¼ˆç†è®ºä¸Šä¸ä¼šåˆ°è¿™é‡Œï¼Œå› ä¸ºå·²ç»è½¬æ¢ä¸ºdictäº†ï¼‰
                        filters.append(
                            FilterSpec(
                                column=effective_id_var,
                                op=FilterOp.IN,
                                value=expanded_patient_ids,
                            )
                        )
            
            # ğŸ”„ è¡¨çº§ç¼“å­˜ç­–ç•¥ï¼š
            # - ç¼“å­˜é”®ï¼š(è¡¨å, æ‚£è€…IDè¿‡æ»¤å™¨)
            # - ä¸åŒ…æ‹¬ sub_var/ids è¿‡æ»¤å™¨ï¼Œå› ä¸ºä¸åŒæ¦‚å¿µå¯èƒ½æœ‰ä¸åŒçš„ sub_var è¿‡æ»¤
            # - ç¼“å­˜ ICUTable å¯¹è±¡ï¼ˆåŒ…å«å…ƒæ•°æ®ï¼‰
            # - ä»ç¼“å­˜åŠ è½½åï¼Œä»éœ€åº”ç”¨ sub_var/ids è¿‡æ»¤å™¨
            
            # åˆ†ç¦»æ‚£è€…IDè¿‡æ»¤å™¨å’Œå…¶ä»–è¿‡æ»¤å™¨
            patient_filter_in_filters = None
            other_filters_list = []
            for f in filters:
                # åˆ¤æ–­æ˜¯å¦ä¸ºæ‚£è€…IDè¿‡æ»¤å™¨ï¼ˆä½¿ç”¨ effective_id_var æˆ–å¸¸è§çš„ ID åˆ—ï¼‰
                is_patient_filter = (
                    (effective_id_var and f.column == effective_id_var and f.op == FilterOp.IN) or
                    (f.column in ['subject_id', 'stay_id', 'hadm_id'] and f.op == FilterOp.IN)
                )
                if is_patient_filter:
                    patient_filter_in_filters = f
                else:
                    other_filters_list.append(f)
            
            # åˆ›å»ºç¼“å­˜é”®
            patient_filter_key = None
            if patient_filter_in_filters:
                # ä½¿ç”¨sorted tupleä½œä¸ºkey
                patient_filter_key = (
                    patient_filter_in_filters.column,
                    tuple(sorted(patient_filter_in_filters.value))
                )
            
            cache_key = (source.table, patient_filter_key)
            
            # è·³è¿‡éœ€è¦ç‰¹æ®Šå¤„ç†è¡¨çš„ç¼“å­˜
            # labevents/admissionsç­‰éœ€è¦subject_idâ†’stay_idæ˜ å°„ï¼Œç¼“å­˜ä¼šä¿å­˜æ˜ å°„å‰çš„æ•°æ®å¯¼è‡´patientè¿‡æ»¤å¤±æ•ˆ
            skip_cache_for_special_tables = source.table in ['labevents', 'microbiologyevents', 'inputevents', 'admissions']
            
            # å°è¯•ä»ç¼“å­˜è·å–
            cached_table = None
            if not skip_cache_for_special_tables:
                with self._cache_lock:
                    cached_table = self._table_cache.get(cache_key)
            if cached_table is not None:
                if verbose or DEBUG_MODE:
                    if DEBUG_MODE: print(f"   â™»ï¸  ä½¿ç”¨ç¼“å­˜çš„è¡¨: {source.table} (è·³è¿‡ {len(patient_filter_in_filters.value) if patient_filter_in_filters else 0} ä¸ªæ‚£è€…çš„åŠ è½½)")
                # ä»ç¼“å­˜è·å–ICUTableå¯¹è±¡
                frame = cached_table.data.copy()
                
                if DEBUG_MODE:
                    print(f"   ğŸ” ç¼“å­˜æ•°æ®: {len(frame)} è¡Œ, åˆ—={list(frame.columns)[:5]}")
                
                # åº”ç”¨å…¶ä»–è¿‡æ»¤å™¨ï¼ˆå¦‚ sub_var/idsï¼‰
                for f in other_filters_list:
                    before_count = len(frame)
                    frame = f.apply(frame)
                    if DEBUG_MODE:
                        print(f"   ç¼“å­˜åˆ†æ”¯è¿‡æ»¤ {f.column}: {before_count:,} â†’ {len(frame):,} è¡Œ")
                
                # é‡æ–°æ„å»º table å¯¹è±¡ï¼ˆä½¿ç”¨è¿‡æ»¤åçš„ frameï¼‰
                table = ICUTable(
                    data=frame,
                    id_columns=cached_table.id_columns,
                    index_column=cached_table.index_column,
                    value_column=cached_table.value_column,
                    unit_column=cached_table.unit_column,
                )
            else:
                # ä»æ•°æ®æºåŠ è½½
                try:
                    # ğŸ”§ æ„å»ºéœ€è¦çš„åˆ—åˆ—è¡¨ï¼šåŸºäº source çš„ value_var, sub_var, unit_var, index_var
                    # è¿™ç¡®ä¿äº†åƒ eICU vitalperiodic çš„ sao2 ç­‰ç‰¹å®šå€¼åˆ—ä¼šè¢«åŠ è½½
                    extra_columns: List[str] = []
                    if getattr(source, 'sub_var', None):
                        extra_columns.append(source.sub_var)
                    if getattr(source, 'value_var', None):
                        extra_columns.append(source.value_var)
                    if getattr(source, 'index_var', None):
                        extra_columns.append(source.index_var)
                    if getattr(source, 'unit_var', None):
                        extra_columns.append(source.unit_var)
                    
                    # Load table with filters and required columns
                    table = data_source.load_table(
                        source.table, 
                        columns=extra_columns if extra_columns else None,
                        filters=filters, 
                        verbose=verbose
                    )
                    
                    # ğŸ” DEBUG: æ£€æŸ¥table.data
                    if DEBUG_MODE:
                        print(f"   ğŸ” table.dataç±»å‹: {type(table.data)}, é•¿åº¦: {len(table.data) if hasattr(table.data, '__len__') else 'N/A'}")
                        if hasattr(table.data, 'columns'):
                            print(f"       åˆ—: {list(table.data.columns)}")
                        if hasattr(table.data, 'head'):
                            print(f"       å‰3è¡Œ:\\n{table.data.head(3)}")
                    
                    frame = table.data.copy()
                    
                    # ğŸ” DEBUG: æ£€æŸ¥ datasource è¿”å›çš„æ•°æ®ï¼ˆåªåœ¨è°ƒè¯•æ¨¡å¼ä¸‹æ˜¾ç¤ºï¼‰
                    if DEBUG_MODE and source.table in ['labevents', 'microbiologyevents', 'inputevents']:
                        has_stay_id = 'stay_id' in frame.columns
                        has_subject_id = 'subject_id' in frame.columns
                        print(f"   ğŸ“Š [{source.table}] datasourceè¿”å›: {len(frame)}è¡Œ, stay_id={has_stay_id}, subject_id={has_subject_id}")
                        if has_stay_id:
                            print(f"       stay_id å”¯ä¸€å€¼: {frame['stay_id'].nunique()} ä¸ª")
                    # å…¨å±€è°ƒè¯•ï¼šåœ¨åŠ è½½ä»»ä½•è¡¨åæ‰“å° AUMC numericitems çš„è´Ÿæ—¶é—´è®¡æ•°ï¼ˆä¾¿äºæ’æŸ¥ï¼‰
                    if DEBUG_MODE and source.table == 'numericitems':
                        if 'measuredat' in frame.columns:
                            try:
                                negc = int((frame['measuredat'] < 0).sum())
                                print(f"   ğŸ [LOAD] {source.table}: rows={len(frame)}, neg_measuredat={negc}")
                            except Exception:
                                pass
                    
                    # è°ƒè¯•ï¼šæ£€æŸ¥è¿‡æ»¤æ˜¯å¦æˆåŠŸï¼ˆåªåœ¨è°ƒè¯•æ¨¡å¼ä¸‹æ˜¾ç¤ºï¼‰
                    if DEBUG_MODE and patient_ids and table.id_columns:
                        id_col = table.id_columns[0] if table.id_columns else None
                        if id_col and id_col in frame.columns:
                            unique_ids = frame[id_col].unique()
                            print(f"   ğŸ” è¡¨ {source.table} åŠ è½½å: {len(frame)} è¡Œ, å”¯ä¸€{id_col}: {len(unique_ids)}ä¸ª")
                            if len(unique_ids) <= 10:
                                print(f"       IDåˆ—è¡¨: {sorted(unique_ids)}")
                    
                    # æ€§èƒ½ä¼˜åŒ–ï¼šå¯¹äºAUMC/HiRIDç­‰é«˜é¢‘æ•°æ®ï¼Œåœ¨è¡¨åŠ è½½åç«‹å³é™é‡‡æ ·
                    # æ£€æµ‹æ•°æ®åº“ç±»å‹å’Œæ•°æ®é¢‘ç‡
                    db_name = data_source.config.name if hasattr(data_source, 'config') and hasattr(data_source.config, 'name') else ''
                    is_high_freq_db = db_name in ['aumc', 'hirid']
                    
                    if is_high_freq_db and table.index_column and len(frame) > 1000:
                        time_col = table.index_column
                        is_numeric_time = pd.api.types.is_numeric_dtype(frame[time_col])
                        
                        # ä½¿ç”¨intervalå‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰
                        target_interval = kwargs.get('interval', pd.Timedelta(hours=1))
                        if isinstance(target_interval, str):
                            target_interval = pd.Timedelta(target_interval)
                        
                        # æ£€æµ‹å½“å‰æ•°æ®é¢‘ç‡
                        need_resample = False
                        if len(frame) > 100:
                            frame_sorted = frame.sort_values(time_col)
                            time_diffs = frame_sorted[time_col].diff().dropna()
                            if len(time_diffs) > 10:
                                median_diff = time_diffs.median()
                                if is_numeric_time:
                                    # æ•°å€¼æ—¶é—´ï¼ˆå°æ—¶ï¼‰
                                    target_hours = target_interval.total_seconds() / 3600.0
                                    # AUMCæ•°æ®é¢‘ç‡å¾ˆé«˜ï¼Œä¸­ä½å·®é€šå¸¸<0.1å°æ—¶
                                    if median_diff < target_hours * 0.5:  # å¦‚æœä¸­ä½é—´éš”å°äºç›®æ ‡é—´éš”çš„ä¸€åŠ
                                        need_resample = True
                                else:
                                    # datetimeæ—¶é—´
                                    if median_diff < target_interval * 0.5:
                                        need_resample = True
                        
                        # æ‰§è¡Œé™é‡‡æ ·
                        if need_resample:
                            if verbose:
                                print(f"   âš¡ æ£€æµ‹åˆ°é«˜é¢‘æ•°æ®ï¼ˆ{source.table}ï¼‰ï¼Œé™é‡‡æ ·åˆ° {target_interval}")
                            
                            id_cols = table.id_columns if table.id_columns else []
                            value_col = table.value_column
                            
                            # å¦‚æœid_colsä¸ºç©ºä½†frameä¸­æœ‰æ˜æ˜¾çš„IDåˆ—ï¼Œå°è¯•æ¨æ–­
                            if not id_cols:
                                potential_id_cols = ['admissionid', 'patientunitstayid', 'stay_id', 'patientid']
                                for col in potential_id_cols:
                                    if col in frame.columns:
                                        id_cols = [col]
                                        if verbose:
                                            print(f"   â„¹ï¸  æ¨æ–­IDåˆ—: {col}")
                                        break
                            
                            if value_col and value_col in frame.columns:
                                if is_numeric_time:
                                    # æ•°å€¼æ—¶é—´ï¼šå››èˆäº”å…¥åˆ°interval
                                    interval_hours = target_interval.total_seconds() / 3600.0
                                    # å¯¹äºæŸäº›é«˜é¢‘æ•°æ®åº“ï¼ˆAUMC/HiRIDï¼‰ï¼Œæ•°å€¼æ—¶é—´åˆ—å•ä½ä¸ºåˆ†é’Ÿï¼ˆè€Œä¸æ˜¯å°æ—¶ï¼‰
                                    # å› æ­¤éœ€è¦åœ¨åŸå§‹å•ä½ä¸Šè¿›è¡Œå–æ•´ï¼Œä»¥ä¿ç•™è´Ÿæ—¶é—´ç‚¹å¹¶é¿å…å•ä½é”™ä½ã€‚
                                    if db_name in ['aumc', 'hirid']:
                                        # åŸå§‹å•ä½ä¸ºåˆ†é’Ÿï¼šå°† interval ä»å°æ—¶è½¬æ¢ä¸ºåˆ†é’Ÿ
                                        native_interval = interval_hours * 60.0
                                    else:
                                        native_interval = interval_hours
                                    # ä½¿ç”¨å‘ä¸‹å–æ•´ä¿ç•™å…¥ICUå‰çš„è´Ÿæ—¶é—´ç‚¹ï¼ˆé¿å… .round() å°†å°äº0çš„å€¼å››èˆäº”å…¥åˆ°0ï¼‰
                                    frame[time_col + '_rounded'] = np.floor(frame[time_col] / native_interval) * native_interval
                                    
                                    # èšåˆï¼šæ ¹æ®æ•°æ®ç±»å‹é€‰æ‹©èšåˆå‡½æ•°
                                    # å¯¹äºè¾“å‡ºç±»æ•°æ®ï¼ˆå°¿é‡ç­‰ï¼‰ä½¿ç”¨sumï¼Œå…¶ä»–ä½¿ç”¨mean
                                    agg_func = 'sum' if 'urine' in value_col.lower() or 'output' in value_col.lower() else 'mean'
                                    group_cols = id_cols + [time_col + '_rounded']
                                    
                                    # ä¿ç•™æ‰€æœ‰åˆ—ï¼Œä¸åªæ˜¯value_col
                                    # å¯¹äºæ•°å€¼åˆ—ä½¿ç”¨agg_funcï¼Œå…¶ä»–åˆ—ä½¿ç”¨first
                                    agg_dict = {}
                                    for col in frame.columns:
                                        # è·³è¿‡åˆ†ç»„åˆ—ï¼ˆIDåˆ—å’Œæ—¶é—´åˆ—å·²ç»åœ¨group_colsä¸­ï¼‰
                                        # ğŸ”§ FIX: ä¹Ÿè·³è¿‡åŸå§‹æ—¶é—´åˆ—ï¼Œé¿å…é‡å‘½ååäº§ç”Ÿé‡å¤åˆ—
                                        if col in group_cols or col == time_col + '_rounded' or col == time_col:
                                            continue
                                        # valueåˆ—ï¼šå…ˆæ£€æŸ¥ç±»å‹ï¼Œåªæœ‰æ•°å€¼å‹æ‰èƒ½èšåˆ
                                        elif col == value_col:
                                            if pd.api.types.is_numeric_dtype(frame[col]) and not pd.api.types.is_bool_dtype(frame[col]):
                                                agg_dict[col] = agg_func
                                            else:
                                                agg_dict[col] = 'first'  # objectç±»å‹ç”¨first
                                        # å…¶ä»–æ•°å€¼åˆ—ä½¿ç”¨èšåˆå‡½æ•°ï¼ˆæ’é™¤å¸ƒå°”ç±»å‹ï¼‰
                                        elif pd.api.types.is_numeric_dtype(frame[col]) and not pd.api.types.is_bool_dtype(frame[col]):
                                            agg_dict[col] = agg_func
                                        # å…¶ä»–åˆ—ï¼ˆåŒ…æ‹¬objectã€stringç­‰ï¼‰ä½¿ç”¨first
                                        else:
                                            agg_dict[col] = 'first'
                                    
                                    if agg_dict:  # åªæœ‰å½“æœ‰åˆ—éœ€è¦èšåˆæ—¶æ‰æ‰§è¡Œ
                                        try:
                                            frame = frame.groupby(group_cols, as_index=False).agg(agg_dict)
                                            frame = frame.rename(columns={time_col + '_rounded': time_col})
                                        except Exception as e:
                                            if verbose:
                                                print(f"   âš ï¸  èšåˆå¤±è´¥: {e}")
                                                print(f"       group_cols={group_cols}")
                                                print(f"       agg_dict={agg_dict}")
                                                print(f"       frameåˆ—ç±»å‹:")
                                                for col, dtype in frame.dtypes.items():
                                                    print(f"         {col}: {dtype}")
                                            raise
                                    else:
                                        # æ²¡æœ‰éœ€è¦èšåˆçš„åˆ—ï¼Œåªä¿ç•™å”¯ä¸€çš„æ—¶é—´ç‚¹
                                        frame = frame.drop_duplicates(subset=group_cols)
                                        frame = frame.rename(columns={time_col + '_rounded': time_col})
                                else:
                                    # datetimeæ—¶é—´ï¼šä½¿ç”¨resample
                                    if id_cols:
                                        resampled_groups = []
                                        agg_func = 'sum' if 'urine' in value_col.lower() or 'output' in value_col.lower() else 'mean'
                                        
                                        for group_id, group_df in frame.groupby(id_cols):
                                            group_df = group_df.set_index(time_col)
                                            
                                            # èšåˆæ‰€æœ‰æ•°å€¼åˆ—
                                            numeric_cols = group_df.select_dtypes(include=[np.number]).columns.tolist()
                                            if value_col in numeric_cols:
                                                # value_colä½¿ç”¨ç‰¹å®šçš„èšåˆå‡½æ•°
                                                agg_dict = {value_col: agg_func}
                                                # å…¶ä»–æ•°å€¼åˆ—ä½¿ç”¨mean
                                                for col in numeric_cols:
                                                    if col != value_col:
                                                        agg_dict[col] = 'mean'
                                            else:
                                                agg_dict = {col: 'mean' for col in numeric_cols}
                                            
                                            resampled = group_df[numeric_cols].resample(target_interval).agg(agg_dict)
                                            resampled = resampled.reset_index()
                                            
                                            # æ·»åŠ IDåˆ—
                                            if isinstance(group_id, tuple):
                                                for i, col in enumerate(id_cols):
                                                    resampled[col] = group_id[i]
                                            else:
                                                resampled[id_cols[0]] = group_id
                                            
                                            resampled_groups.append(resampled)
                                        
                                        if resampled_groups:
                                            frame = pd.concat(resampled_groups, ignore_index=True)
                                    else:
                                        frame = frame.set_index(time_col)
                                        agg_func = 'sum' if 'urine' in value_col.lower() or 'output' in value_col.lower() else 'mean'
                                        numeric_cols = frame.select_dtypes(include=[np.number]).columns.tolist()
                                        agg_dict = {col: agg_func if col == value_col else 'mean' for col in numeric_cols}
                                        frame = frame[numeric_cols].resample(target_interval).agg(agg_dict).reset_index()
                                
                                if verbose:
                                    print(f"   âœ“ é™é‡‡æ ·å®Œæˆï¼š{len(table.data)} è¡Œ â†’ {len(frame)} è¡Œ")
                                
                                # æ›´æ–°tableå¯¹è±¡ä»¥åæ˜ é™é‡‡æ ·åçš„æ•°æ®
                                table = ICUTable(
                                    data=frame,
                                    id_columns=table.id_columns,
                                    index_column=table.index_column,
                                    value_column=table.value_column,
                                    unit_column=table.unit_column,
                                )
                    
                    
                    # ä»…å½“æœ‰æ‚£è€…è¿‡æ»¤å™¨ä¸”ä¸æ˜¯ç‰¹æ®Šå¤„ç†è¡¨æ—¶æ‰ç¼“å­˜
                    # labevents/admissionsç­‰éœ€è¦subject_idâ†’stay_idæ˜ å°„ï¼Œä¸åº”ç¼“å­˜åŸå§‹subject_idçº§åˆ«æ•°æ®
                    if patient_filter_in_filters and not skip_cache_for_special_tables:
                        # ç¼“å­˜åªåº”ç”¨äº†æ‚£è€…è¿‡æ»¤å™¨çš„è¡¨
                        patient_only_table = data_source.load_table(
                            source.table,
                            filters=[patient_filter_in_filters],
                            verbose=False
                        )
                        with self._cache_lock:
                            self._table_cache[cache_key] = patient_only_table
                        if verbose:
                            if DEBUG_MODE: print(f"   ğŸ’¾ ç¼“å­˜è¡¨ {source.table}: {len(patient_filter_in_filters.value)} ä¸ªæ‚£è€…")
                except (KeyError, FileNotFoundError, ValueError) as e:
                    # å¦‚æœè¡¨ä¸å­˜åœ¨ï¼Œè·³è¿‡è¿™ä¸ªæº
                    if DEBUG_MODE or logger.isEnabledFor(logging.DEBUG):
                        logger.debug(f"Table '{source.table}' not available: {type(e).__name__}: {str(e)[:100]}")
                    continue
            
            # MIMIC-IV ç‰¹æ®Šå¤„ç†ï¼špatients è¡¨åªæœ‰ subject_idï¼Œéœ€è¦ä¸ icustays å…³è”è·å– stay_id
            if source.table == 'patients' and 'subject_id' in frame.columns and 'stay_id' not in frame.columns:
                try:
                    # åŠ è½½ icustays è¡¨ä»¥è·å– subject_id -> stay_id çš„æ˜ å°„
                    icustay_filters = []
                    if patient_ids:
                        # patient_ids å¯èƒ½æ˜¯ dict æˆ– list
                        if isinstance(patient_ids, dict):
                            stay_ids = patient_ids.get('stay_id', [])
                        else:
                            stay_ids = patient_ids
                        if stay_ids:
                            icustay_filters.append(
                                FilterSpec(column='stay_id', op=FilterOp.IN, value=stay_ids)
                            )
                    
                    icustays = data_source.load_table('icustays', filters=icustay_filters if icustay_filters else None, verbose=False)
                    if hasattr(icustays, 'data'):
                        icu_df = icustays.data[['subject_id', 'stay_id']].drop_duplicates()
                    else:
                        icu_df = icustays[['subject_id', 'stay_id']].drop_duplicates()
                    
                    # ä¸ patients è¡¨åšå†…è¿æ¥
                    frame = frame.merge(icu_df, on='subject_id', how='inner')
                    
                    # åˆ é™¤ subject_id åˆ—ï¼Œåªä¿ç•™ stay_id
                    if 'stay_id' in frame.columns:
                        frame = frame.drop(columns=['subject_id'], errors='ignore')
                    
                    if verbose or DEBUG_MODE:
                        print(f"   ğŸ”— patients è¡¨ä¸ icustays å…³è”: {len(frame)} è¡Œ")
                        
                    # æ›´æ–° table å¯¹è±¡
                    table = ICUTable(
                        data=frame,
                        id_columns=['stay_id'],
                        index_column=table.index_column if table.index_column in frame.columns else None,
                        value_column=table.value_column,
                        unit_column=table.unit_column,
                    )
                except Exception as e:
                    if DEBUG_MODE:
                        print(f"   âš ï¸  patients è¡¨å…³è”å¤±è´¥: {e}")
            
            # MIMIC-IVç‰¹æ®Šå¤„ç†ï¼šè‹¥è¡¨ä¸ºlabevents/microbiologyevents/inputeventsï¼Œä»…æœ‰subject_idï¼ŒæŒ‰æ—¶é—´çª—å£æ˜ å°„åˆ°å¯¹åº”ICU stay
            if DEBUG_MODE:
                print(f"   ğŸ“Š åŠ è½½åæ•°æ®: {source.table}, è¡Œæ•°={len(frame)}, itemidè¿‡æ»¤={source.ids}")
                if source.ids and source.sub_var and source.sub_var in frame.columns:
                    print(f"       - {source.sub_var} å”¯ä¸€å€¼: {sorted(frame[source.sub_var].unique())[:10]}")
                print(f"       - frameåˆ—: {list(frame.columns)}")
                print(f"       - frameå‰3è¡Œ:\\n{frame.head(3)}")
            if DEBUG_MODE:
                if DEBUG_MODE: print(f"   ğŸ” è°ƒè¯• {source.table}: 'subject_id' in frame={('subject_id' in frame.columns)}, 'stay_id' in frame={('stay_id' in frame.columns)}, defaults.id_var={defaults.id_var}")
            if source.table in ['labevents', 'microbiologyevents', 'inputevents'] and 'subject_id' in frame.columns and 'stay_id' not in frame.columns:
                if DEBUG_MODE: print(f"   â¡ï¸  è¿›å…¥ MIMIC-IV ç‰¹æ®Šå¤„ç†: {source.table}")
                try:
                    # ä»…åŠ è½½ç›¸å…³stayçš„icustaysï¼Œå¹¶æºå¸¦intime/outtimeç”¨äºçª—å£è¿‡æ»¤
                    icustay_filters = []
                    # ä¿å­˜expanded_patient_idsåˆ°å½“å‰ä½œç”¨åŸŸ,é¿å…åç»­locals()æ£€æŸ¥å¤±æ•ˆ
                    current_expanded_patient_ids = None
                    
                    # ğŸ”¥ å…³é”®ä¿®å¤: ä½¿ç”¨åŸå§‹ stay_id è€Œä¸æ˜¯ subject_id
                    # è¿™æ ·é¿å…åŠ è½½åŒä¸€æ‚£è€…çš„æ‰€æœ‰ICUå…¥ä½è®°å½•
                    if patient_ids:
                        # patient_ids æœ¬èº«å°±æ˜¯ stay_id åˆ—è¡¨
                        icustay_filters.append(
                            FilterSpec(column='stay_id', op=FilterOp.IN, value=patient_ids)
                        )
                        if DEBUG_MODE: print(f"   ğŸ¯ [icustays] ä½¿ç”¨åŸå§‹ stay_id è¿‡æ»¤: {len(patient_ids)} ä¸ª, IDs={patient_ids}")
                    
                    icustays = data_source.load_table('icustays', filters=icustay_filters if icustay_filters else None, verbose=verbose)
                    if hasattr(icustays, 'data'):
                        # åŒ…å«hadm_idä»¥ä¾¿åŒ¹é…åŒä¸€ä½é™¢çš„æ•°æ®
                        cols = ['subject_id', 'stay_id', 'hadm_id', 'intime', 'outtime']
                        icu_df = icustays.data[[c for c in cols if c in icustays.data.columns]].drop_duplicates()
                    else:
                        cols = ['subject_id', 'stay_id', 'hadm_id', 'intime', 'outtime']
                        icu_df = icustays[[c for c in cols if c in icustays.columns]].drop_duplicates()
                    
                    if DEBUG_MODE: print(f"   âœ… [icustays] åŠ è½½å: {len(icu_df)} stays, stay_id={sorted(icu_df['stay_id'].unique())[:10]}")
                    
                    # ğŸ”¥ CRITICAL FIX: ä¸ºäº†å®ç° rolling joinï¼Œéœ€è¦åŠ è½½åŒä¸€ hadm_id ä¸‹çš„æ‰€æœ‰ stays
                    # è¿™æ ·æ‰èƒ½æ­£ç¡®åˆ¤æ–­æ•°æ®ç‚¹å±äºå“ªä¸ª stay
                    if 'hadm_id' in icu_df.columns and 'hadm_id' in frame.columns and len(icu_df) > 0:
                        target_hadm_ids = icu_df['hadm_id'].unique().tolist()
                        # åŠ è½½åŒä¸€ hadm_id ä¸‹çš„æ‰€æœ‰ staysï¼ˆç”¨äº rolling join æ—¶é—´è¾¹ç•Œåˆ¤æ–­ï¼‰
                        all_stays_in_hadm = data_source.load_table(
                            'icustays',
                            filters=[FilterSpec(column='hadm_id', op=FilterOp.IN, value=target_hadm_ids)],
                            verbose=False
                        )
                        if hasattr(all_stays_in_hadm, 'data'):
                            all_stays_df = all_stays_in_hadm.data[[c for c in cols if c in all_stays_in_hadm.data.columns]].drop_duplicates()
                        else:
                            all_stays_df = all_stays_in_hadm[[c for c in cols if c in all_stays_in_hadm.columns]].drop_duplicates()
                        
                        if len(all_stays_df) > len(icu_df):
                            if DEBUG_MODE: print(f"   ğŸ”„ [Rolling Joinå‡†å¤‡] åŒä¸€ hadm_id ä¸‹æœ‰æ›´å¤š stays: {len(icu_df)} â†’ {len(all_stays_df)}")
                            # ç”¨å®Œæ•´çš„ stays åˆ—è¡¨æ›¿æ¢ icu_dfï¼Œç”¨äºåç»­ rolling join
                            icu_df = all_stays_df

                    # é€‰æ‹©ç”¨äºæ—¶é—´åŒ¹é…çš„åˆ—
                    time_col = None
                    if index_column and index_column in frame.columns:
                        time_col = index_column
                    else:
                        # å¯¹äº inputeventsï¼Œä¼˜å…ˆä½¿ç”¨ starttime
                        if source.table == 'inputevents':
                            for cand in ['starttime', 'charttime', 'storetime']:
                                if cand in frame.columns:
                                    time_col = cand
                                    break
                        else:
                            for cand in ['charttime', 'storetime', 'specimen_time']:
                                if cand in frame.columns:
                                    time_col = cand
                                    break

                    if time_col is not None:
                        # è§„èŒƒæ—¶é—´ç±»å‹
                        frame[time_col] = pd.to_datetime(frame[time_col], errors='coerce', utc=True).dt.tz_localize(None)
                        icu_df['intime'] = pd.to_datetime(icu_df['intime'], errors='coerce', utc=True).dt.tz_localize(None)
                        icu_df['outtime'] = pd.to_datetime(icu_df['outtime'], errors='coerce', utc=True).dt.tz_localize(None)

                        # å…ˆæŒ‰subject_idåˆå¹¶ï¼Œå¦‚æœæœ‰hadm_idåˆ™åŒæ—¶åŒ¹é…
                        # ä¿®å¤ï¼šåªä¿ç•™åŒä¸€ä½é™¢ï¼ˆhadm_idï¼‰çš„æ•°æ®ï¼Œé¿å…æ··å…¥æ‚£è€…å…¶ä»–ä½é™¢çš„å†å²æ•°æ®
                        if 'hadm_id' in frame.columns and 'hadm_id' in icu_df.columns:
                            # åŒæ—¶åŒ¹é…subject_idå’Œhadm_idï¼Œç¡®ä¿åªå–åŒä¸€æ¬¡ä½é™¢çš„æ•°æ®
                            tmp = frame.merge(icu_df, on=['subject_id', 'hadm_id'], how='inner')
                        else:
                            # å¦‚æœæ²¡æœ‰hadm_idï¼Œåªèƒ½æŒ‰subject_idåŒ¹é…ï¼ˆå¯èƒ½æ··å…¥å…¶ä»–ä½é™¢æ•°æ®ï¼‰
                            tmp = frame.merge(icu_df, on='subject_id', how='inner')
                        
                        # CRITICAL FIX: å®ç° ricu çš„ rolling join é€»è¾‘
                        # å½“åŒä¸€ä¸ª hadm_id/subject_id æœ‰å¤šä¸ª stay_id æ—¶ï¼Œæ•°æ®ä¼šè¢«å¤åˆ¶åˆ°æ‰€æœ‰åŒ¹é…çš„ stay_id
                        # éœ€è¦æ ¹æ®æ—¶é—´å°†æ•°æ®åªä¿ç•™åœ¨æ­£ç¡®çš„ stay_id ä¸‹
                        # ricu ä½¿ç”¨ roll = -Inf (å‘å‰æ»šåŠ¨)ï¼šæ•°æ®åˆ†é…ç»™æ—¶é—´ä¹‹åæœ€è¿‘çš„ stay_id
                        target_stay_ids = set(patient_ids) if patient_ids else None
                        
                        if time_col is not None and 'stay_id' in tmp.columns and 'intime' in tmp.columns and len(tmp) > 0:
                            # è·å–æ‰€æœ‰å”¯ä¸€çš„ stay_id åŠå…¶ intimeï¼ŒæŒ‰ intime æ’åº
                            stay_info = tmp[['stay_id', 'intime']].drop_duplicates().sort_values('intime')
                            
                            if len(stay_info) > 1:
                                # æœ‰å¤šä¸ª stay_idï¼Œéœ€è¦å®ç° rolling join
                                stays_list = stay_info['stay_id'].tolist()
                                intimes_list = stay_info['intime'].tolist()
                                
                                if DEBUG_MODE:
                                    print(f"      ğŸ”„ [Rolling Join] æ£€æµ‹åˆ°å¤šä¸ª stay_id: {stays_list}")
                                    print(f"      ğŸ”„ [Rolling Join] å¯¹åº” intime: {intimes_list}")
                                    print(f"      ğŸ”„ [Rolling Join] ç›®æ ‡ stay_id: {target_stay_ids}")
                                
                                # ä¸ºæ¯ä¸ª stay_id è®¡ç®—å…¶æœ‰æ•ˆæ—¶é—´èŒƒå›´
                                # stay_i çš„æœ‰æ•ˆèŒƒå›´æ˜¯: [prev_stay_outtime, next_stay_intime)
                                # ä½†ä½¿ç”¨ roll = -Inf æ„å‘³ç€ï¼šdata_time < next_stay_intime
                                
                                result_frames = []
                                for i, (stay_id, intime) in enumerate(zip(stays_list, intimes_list)):
                                    # åªå¤„ç†ç”¨æˆ·è¯·æ±‚çš„ stay_id
                                    if target_stay_ids and stay_id not in target_stay_ids:
                                        continue
                                    
                                    # è¿‡æ»¤å±äºå½“å‰ stay_id çš„è¡Œ
                                    stay_mask = tmp['stay_id'] == stay_id
                                    
                                    if i < len(stays_list) - 1:
                                        # ä¸æ˜¯æœ€åä¸€ä¸ª stayï¼Œæ•°æ®æ—¶é—´å¿…é¡»å°äºä¸‹ä¸€ä¸ª stay çš„ intime
                                        next_intime = intimes_list[i + 1]
                                        time_mask = tmp[time_col] < next_intime
                                        stay_data = tmp[stay_mask & time_mask].copy()
                                        if DEBUG_MODE:
                                            print(f"      ğŸ”„ [Rolling Join] stay_id={stay_id}: time < {next_intime}, ä¿ç•™ {len(stay_data)} è¡Œ")
                                    else:
                                        # æœ€åä¸€ä¸ª stayï¼Œæ²¡æœ‰æ—¶é—´ä¸Šé™
                                        stay_data = tmp[stay_mask].copy()
                                        if DEBUG_MODE:
                                            print(f"      ğŸ”„ [Rolling Join] stay_id={stay_id}: æœ€åä¸€ä¸ªstay, ä¿ç•™ {len(stay_data)} è¡Œ")
                                    
                                    result_frames.append(stay_data)
                                
                                if result_frames:
                                    tmp = pd.concat(result_frames, ignore_index=True)
                                    if DEBUG_MODE:
                                        print(f"      ğŸ”„ [Rolling Join] å¤š stay_id æ—¶é—´è¿‡æ»¤å®Œæˆ: {len(tmp)} è¡Œ")
                        
                        # ç¡®ä¿åªä¿ç•™ç”¨æˆ·è¯·æ±‚çš„ stay_idï¼ˆé˜²æ­¢é—æ¼è¿‡æ»¤ï¼‰
                        if target_stay_ids and 'stay_id' in tmp.columns:
                            before_filter = len(tmp)
                            tmp = tmp[tmp['stay_id'].isin(target_stay_ids)]
                            if DEBUG_MODE and len(tmp) != before_filter:
                                print(f"      ğŸ¯ [æœ€ç»ˆè¿‡æ»¤] åªä¿ç•™ç›®æ ‡ stay_id: {before_filter} â†’ {len(tmp)} è¡Œ")

                        # CRITICAL FIX: Use ICU outtime as upper bound
                        # ricu.R uses ICU discharge (outtime) as the time window, NOT hospital discharge
                        # See ricu/R/data-utils.R: id_win_helper.miiv_env uses icustay's intime/outtime
                        before_filter = len(tmp)

                        # Debug output for ICU window filter
                        if DEBUG_MODE:
                            print(f"      ğŸ¥ [ICUçª—å£] å¼€å§‹å¤„ç†: è¡¨={source.table}, è¡Œæ•°={len(tmp)}")
                            if 'outtime' in tmp.columns:
                                print(f"      ğŸ¥ [ICUçª—å£] tmpåŒ…å«outtime: {tmp['outtime'].notna().sum()}ä¸ªæœ‰æ•ˆå€¼")
                            else:
                                print(f"      ğŸ¥ [ICUçª—å£] âŒ tmpä¸åŒ…å«outtimeåˆ—!")

                        # Use ICU outtime for filtering (ricu.R behavior)
                        # Data points after ICU discharge should be excluded
                        if 'outtime' in tmp.columns:
                            mask_time = tmp['outtime'].isna() | (tmp[time_col] <= tmp['outtime'])
                            tmp = tmp[mask_time].copy()
                            filter_type = "ICUå‡ºé™¢çª—å£"
                        else:
                            filter_type = "æ— æ—¶é—´è¿‡æ»¤(ç¼ºå°‘outtime)"

                        after_filter = len(tmp)
                        # åªåœ¨è°ƒè¯•æ¨¡å¼ä¸‹æ‰“å°æ—¶é—´è¿‡æ»¤ä¿¡æ¯
                        if DEBUG_MODE and before_filter > after_filter:
                            print(f"      â±ï¸  [{concept_name}] ricu.R-styleæ—¶é—´è¿‡æ»¤ ({filter_type}): {source.table} ä» {before_filter} è¡Œ â†’ {after_filter} è¡Œ")
                        
                        # CRITICAL FIX: æ— è®ºtmpæ˜¯å¦ä¸ºç©ºï¼Œéƒ½è¦æ›´æ–°frame
                        # å¦‚æœtmpä¸ºç©ºï¼ˆæ²¡æœ‰åŒ¹é…çš„æ•°æ®æˆ–è¢«æ—¶é—´è¿‡æ»¤ï¼‰ï¼Œframeä¹Ÿåº”è¯¥ä¸ºç©º
                        if not tmp.empty:
                            # å°†è¿‡æ»¤åçš„æ•°æ®ä½œä¸ºæ–°frameï¼Œä»…ä¿ç•™å¿…è¦åˆ—
                            frame = tmp.drop(columns=['intime', 'outtime'])
                            if DEBUG_MODE: print(f"   âœ… [{concept_name}] MIMIC-IV {source.table}: åˆå¹¶+è¿‡æ»¤å {len(frame)} è¡Œ")
                        else:
                            # tmpä¸ºç©ºçš„åŸå› å¯èƒ½æ˜¯ï¼š1) æ²¡æœ‰åŒ¹é…çš„ä½é™¢æ•°æ®ï¼Œ2) æ—¶é—´è¿‡æ»¤åä¸ºç©º
                            # è¿™æ˜¯æ­£å¸¸çš„æ•°æ®è¿‡æ»¤è¡Œä¸ºï¼ˆä¾‹å¦‚å®éªŒå®¤ç»“æœåœ¨ICUå‡ºé™¢åé‡‡é›†ï¼Œæˆ–åœ¨miivä¸­æ˜¯ICUå…¥é™¢å‰çš„æ•°æ®ï¼‰
                            if DEBUG_MODE:
                                reason = "ricu.R-styleæ—¶é—´è¿‡æ»¤" if before_filter > 0 else "ICUä½é™¢åŒ¹é…"
                                print(f"   âš ï¸  [{concept_name}] MIMIC-IV {source.table}: {reason}åä¸ºç©º (åŸå§‹{len(frame)}è¡Œ â†’ åŒ¹é…{before_filter}è¡Œ â†’ è¿‡æ»¤å0è¡Œ)")
                            frame = pd.DataFrame(columns=frame.columns)
                            
                        # ğŸ”— å…³é”®ä¿®å¤ï¼šå¦‚æœç”¨æˆ·æä¾›äº†ç‰¹å®šçš„ stay_idï¼Œåœ¨æ˜ å°„åå†æ¬¡è¿‡æ»¤
                        # ç¡®ä¿åªè¿”å›ç”¨æˆ·æŒ‡å®šçš„ stay_id çš„æ•°æ®
                        if 'stay_id' in frame.columns and patient_ids:
                            # ä½¿ç”¨ä¹‹å‰ä¿å­˜çš„current_expanded_patient_ids
                            if current_expanded_patient_ids and isinstance(current_expanded_patient_ids, dict) and 'stay_id' in current_expanded_patient_ids:
                                specified_stay_ids = current_expanded_patient_ids['stay_id']
                                if specified_stay_ids:
                                    before_stay_filter = len(frame)
                                    frame = frame[frame['stay_id'].isin(specified_stay_ids)].copy()
                                    if DEBUG_MODE and before_stay_filter > len(frame):
                                        print(f"      ğŸ” [{concept_name}] stay_idè¿‡æ»¤: {before_stay_filter}è¡Œ â†’ {len(frame)}è¡Œ (ä¿ç•™{len(specified_stay_ids)}ä¸ªstay_id)")
                        
                        if defaults.id_var == 'subject_id' and 'stay_id' in frame.columns:
                                id_columns = ['stay_id']
                                if DEBUG_MODE: print(f"   ğŸ”„ MIMIC-IVç‰¹æ®Šå¤„ç†: {source.table} IDåˆ—ä» subject_id â†’ stay_id (è¡Œæ•°: {len(frame)})")
                    else:
                        # æ²¡æœ‰æ˜ç¡®æ—¶é—´åˆ—ï¼Œé€€åŒ–ä¸ºsubjectçº§åˆå¹¶ï¼ˆå¯èƒ½äº§ç”Ÿå†—ä½™ï¼‰ï¼Œä½†ä»è¡¥å……stay_id
                        frame = frame.merge(icu_df[['subject_id', 'stay_id']], on='subject_id', how='inner')
                        if defaults.id_var == 'subject_id' and 'stay_id' in frame.columns:
                            id_columns = ['stay_id']
                            if DEBUG_MODE: print(f"   ğŸ”„ MIMIC-IVç‰¹æ®Šå¤„ç†(æ— æ—¶é—´åˆ—): {source.table} IDåˆ—ä» subject_id â†’ stay_id (è¡Œæ•°: {len(frame)})")
                except Exception as ex:
                    print(f"âš ï¸  Warning: Failed to time-map labevents to icu stays: {ex}")
                    if verbose:
                        import traceback
                        traceback.print_exc()
                    # å¤±è´¥æ—¶ä¸åšå¼ºåˆ¶æ˜ å°„ï¼Œä¿æŒåŸé€»è¾‘
            
            # MIMIC-IVç‰¹æ®Šå¤„ç†ï¼šadmissionsè¡¨åªæœ‰subject_idå’Œhadm_idï¼Œéœ€è¦æ˜ å°„åˆ°stay_id
            if source.table == 'admissions' and 'subject_id' in frame.columns and 'stay_id' not in frame.columns:
                if DEBUG_MODE: print(f"   â¡ï¸  è¿›å…¥ MIMIC-IV admissionsç‰¹æ®Šå¤„ç†")
                try:
                    # åŠ è½½icustaysè·å–subject_idâ†’hadm_idâ†’stay_idæ˜ å°„
                    icustay_filters = []
                    current_expanded_patient_ids = None
                    if patient_ids:
                        current_expanded_patient_ids = self._expand_patient_ids(
                            patient_ids, 
                            'subject_id',
                            data_source,
                            verbose=False
                        )
                        subj_vals = current_expanded_patient_ids.get('subject_id') if isinstance(current_expanded_patient_ids, dict) else current_expanded_patient_ids
                        if subj_vals:
                            icustay_filters.append(
                                FilterSpec(column='subject_id', op=FilterOp.IN, value=subj_vals)
                            )
                    
                    icustays = data_source.load_table('icustays', filters=icustay_filters if icustay_filters else None, verbose=verbose)
                    if hasattr(icustays, 'data'):
                        icu_df = icustays.data[['subject_id', 'hadm_id', 'stay_id']].drop_duplicates()
                    else:
                        icu_df = icustays[['subject_id', 'hadm_id', 'stay_id']].drop_duplicates()
                    
                    # é€šè¿‡hadm_idæ˜ å°„åˆ°stay_idï¼ˆadmissionsæ˜¯hospitalçº§åˆ«ï¼Œicustaysæ˜¯ICUçº§åˆ«ï¼‰
                    if 'hadm_id' in frame.columns and 'hadm_id' in icu_df.columns:
                        before_merge = len(frame)
                        frame = frame.merge(icu_df[['hadm_id', 'stay_id']], on='hadm_id', how='inner')
                        if DEBUG_MODE:
                            print(f"      ğŸ¥ [{concept_name}] admissionsâ†’icustaysæ˜ å°„: {before_merge}è¡Œ â†’ {len(frame)}è¡Œ")
                        
                        # æœ€ç»ˆstay_idè¿‡æ»¤
                        if patient_ids and current_expanded_patient_ids and isinstance(current_expanded_patient_ids, dict) and 'stay_id' in current_expanded_patient_ids:
                            specified_stay_ids = current_expanded_patient_ids['stay_id']
                            if specified_stay_ids:
                                before_stay_filter = len(frame)
                                frame = frame[frame['stay_id'].isin(specified_stay_ids)].copy()
                                if DEBUG_MODE and before_stay_filter > len(frame):
                                    print(f"      ğŸ” [{concept_name}] stay_idè¿‡æ»¤: {before_stay_filter}è¡Œ â†’ {len(frame)}è¡Œ")
                        
                        if defaults.id_var == 'subject_id' and 'stay_id' in frame.columns:
                            id_columns = ['stay_id']
                            if DEBUG_MODE: print(f"   ğŸ”„ MIMIC-IVç‰¹æ®Šå¤„ç†: admissions IDåˆ—ä» subject_id â†’ stay_id")
                except Exception as ex:
                    print(f"âš ï¸  Warning: Failed to map admissions to icu stays: {ex}")
                    if verbose:
                        import traceback
                        traceback.print_exc()

            # å¦‚æœé…ç½®ä¸­æ²¡æœ‰IDåˆ—ï¼Œå°è¯•ä»æ•°æ®ä¸­è‡ªåŠ¨æ£€æµ‹
            if not table.id_columns:
                # æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦æœ‰å¸¸è§çš„IDåˆ—
                # ğŸ”§ ä¿®å¤: æ ¹æ®æ•°æ®åº“ç±»å‹ä¼˜å…ˆé€‰æ‹©åˆé€‚çš„IDåˆ—
                db_name = data_source.config.name if hasattr(data_source, 'config') and hasattr(data_source.config, 'name') else ''
                
                # æ•°æ®åº“ç‰¹å®šçš„IDåˆ—ä¼˜å…ˆé¡ºåº
                if db_name in ['aumc']:
                    common_id_cols = ['admissionid', 'patientid']
                elif db_name in ['eicu', 'eicu_demo']:
                    common_id_cols = ['patientunitstayid', 'patientid']
                elif db_name in ['hirid']:
                    common_id_cols = ['patientid']
                else:
                    # MIMIC-IV ç­‰
                    common_id_cols = ['stay_id', 'icustay_id', 'hadm_id', 'subject_id']
                
                found_id_cols = [col for col in common_id_cols if col in frame.columns]
                if found_id_cols:
                    # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„IDåˆ—ï¼ˆå·²æŒ‰æ•°æ®åº“ä¼˜å…ˆé¡ºåºæ’åˆ—ï¼‰
                    preferred_id = found_id_cols[0]
                    id_columns = [preferred_id]
                    if DEBUG_MODE: print(f"   ğŸ” è‡ªåŠ¨æ£€æµ‹åˆ°IDåˆ—: {preferred_id} (db={db_name})")
            else:
                id_columns = id_columns or list(table.id_columns)
            
            # æ¯ä¸ªæºä½¿ç”¨è‡ªå·±çš„ index_column å’Œ unit_column
            # ä¸è¦å¤ç”¨å¾ªç¯å¤–çš„å˜é‡ï¼Œé¿å…å¤šæºæ¦‚å¿µæ—¶ç¬¬ä¸€ä¸ªæºçš„é…ç½®è¦†ç›–åç»­æº
            source_index_column = source.index_var or table.index_column
            source_unit_column = source.unit_var or table.unit_column
            
            # æ›´æ–°å…¨å±€ index_column å’Œ unit_columnï¼ˆç”¨äºåç»­çš„æ—¶é—´å¯¹é½ç­‰æ“ä½œï¼‰
            # ä½†ç¡®ä¿æ¯ä¸ªæºå¤„ç†æ—¶ä½¿ç”¨è‡ªå·±çš„é…ç½®
            if not index_column:
                index_column = source_index_column
            if not unit_column:
                unit_column = source_unit_column

            time_columns = list(
                {
                    *time_columns,
                    *(table.time_columns or []),
                    *( [source_index_column] if source_index_column else []),
                }
            )

            # å¤„ç† dur_varï¼šå¦‚æœæŒ‡å®šäº† dur_var="endtime"ï¼Œè®¡ç®— duration = endtime - starttime
            # å‚è€ƒ R ricu load_win.R ä¸­çš„ dur_is_end é€»è¾‘:
            # if (dur_is_end) {
            #   res <- res[, c(dur_var) := get(dur_var) - get(index_var)]
            # }
            if source.dur_var and source.dur_var in frame.columns:
                if source_index_column and source_index_column in frame.columns:
                    duration_col = concept_name + '_dur'
                    dur_is_end = False  # æ˜¯å¦éœ€è¦è®¡ç®— duration = endtime - starttime
                    
                    # Case 1: datetime ç±»å‹çš„ endtime
                    if pd.api.types.is_datetime64_any_dtype(frame[source.dur_var]):
                        dur_is_end = True
                        # ç¡®ä¿ starttime ä¹Ÿæ˜¯ datetime
                        if not pd.api.types.is_datetime64_any_dtype(frame[source_index_column]):
                            frame[source_index_column] = pd.to_datetime(frame[source_index_column], errors='coerce')
                        
                        # è®¡ç®— duration (timedelta)
                        frame[duration_col] = frame[source.dur_var] - frame[source_index_column]
                    
                    # Case 2: æ•°å€¼ç±»å‹çš„ endtime (å¦‚ AUMC çš„æ¯«ç§’æ—¶é—´)
                    # æ£€æµ‹ï¼šå¦‚æœ dur_var æ˜¯æ•°å€¼ä¸”é€šå¸¸å¤§äº index_varï¼Œè¯´æ˜æ˜¯ endtime
                    elif pd.api.types.is_numeric_dtype(frame[source.dur_var]) and \
                         pd.api.types.is_numeric_dtype(frame[source_index_column]):
                        # æ£€æŸ¥ dur_var æ˜¯å¦å¤§äº index_varï¼ˆè¡¨ç¤ºå®ƒæ˜¯ endtimeï¼‰
                        # ä½¿ç”¨æŠ½æ ·æ£€æŸ¥ä»¥æé«˜æ€§èƒ½
                        sample_size = min(100, len(frame))
                        if sample_size > 0:
                            sample = frame.head(sample_size)
                            dur_vals = pd.to_numeric(sample[source.dur_var], errors='coerce')
                            idx_vals = pd.to_numeric(sample[source_index_column], errors='coerce')
                            valid_mask = dur_vals.notna() & idx_vals.notna()
                            if valid_mask.sum() > 0:
                                # å¦‚æœå¤§éƒ¨åˆ† dur_var > index_varï¼Œåˆ™è®¤ä¸ºæ˜¯ endtime
                                ratio = (dur_vals[valid_mask] > idx_vals[valid_mask]).mean()
                                if ratio > 0.8:  # 80% ä»¥ä¸Šçš„å€¼æ»¡è¶³ dur_var > index_var
                                    dur_is_end = True
                                    # è®¡ç®— duration = endtime - starttime (æ•°å€¼)
                                    # ç»“æœå•ä½ä¸ start/stop ç›¸åŒï¼š
                                    # - AUMC: åˆ†é’Ÿï¼ˆdatasource.py å·²å°† ms è½¬ä¸ºåˆ†é’Ÿï¼‰
                                    # - eICU: åˆ†é’Ÿï¼ˆoffset åˆ—æœ¬èº«å°±æ˜¯åˆ†é’Ÿï¼‰
                                    frame[duration_col] = frame[source.dur_var] - frame[source_index_column]
                                    
                                    # ğŸ”§ FIX: å°† duration ä»åˆ†é’Ÿè½¬æ¢ä¸ºå°æ—¶
                                    # è¿™ä¸ _align_time_to_admission å¯¹ start/stop çš„è½¬æ¢ä¿æŒä¸€è‡´
                                    db_name = data_source.config.name if hasattr(data_source, 'config') and hasattr(data_source.config, 'name') else ''
                                    if db_name in ['eicu', 'eicu_demo', 'aumc']:
                                        frame[duration_col] = frame[duration_col] / 60.0
                                        if DEBUG_MODE:
                                            print(f"   ğŸ”§ {db_name}: å°† {duration_col} ä»åˆ†é’Ÿè½¬æ¢ä¸ºå°æ—¶")
                                    
                                    if DEBUG_MODE:
                                        print(f"   ğŸ”§ AUMC dur_is_end=True: {source.dur_var}={dur_vals.head(3).tolist()}, "
                                              f"{source_index_column}={idx_vals.head(3).tolist()}")
                    
                    if dur_is_end and DEBUG_MODE:
                        print(f"   dur_var '{source.dur_var}' â†’ duration '{duration_col}' (ç¤ºä¾‹: {frame[duration_col].head(1).tolist()})")

            value_column = source.value_var or table.value_column
            if value_column is None:
                raise ValueError(
                    f"Concept '{concept_name}' has no value column in table "
                    f"'{source.table}'. Provide 'value_var' in the dictionary."
                )

            # æ£€æŸ¥æ˜¯å¦æœ‰ apply_map(var='sub_var') å›è°ƒ
            # è¿™ç§æƒ…å†µä¸‹ï¼Œåº”è¯¥ä½¿ç”¨æ˜ å°„åçš„ sub_var ä½œä¸ºæœ€ç»ˆçš„å€¼åˆ—
            uses_sub_var_mapping = False
            if source.callback and 'apply_map' in source.callback and 'var' in source.callback:
                # åŒ¹é… apply_map(..., var='sub_var') æˆ– apply_map(..., var="sub_var")
                match = re.search(r"var\s*=\s*['\"]sub_var['\"]", source.callback)
                if match and source.sub_var:
                    uses_sub_var_mapping = True

            # å¦‚æœvalue_columnä¸åœ¨frameä¸­ï¼Œå¯èƒ½éœ€è¦å…ˆåˆ›å»ºï¼ˆä¾‹å¦‚ä»callbackåˆ›å»ºï¼‰
            # å…ˆæ£€æŸ¥value_columnæ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨ï¼Œå¯èƒ½éœ€è¦é€šè¿‡callbackåˆ›å»º
            if DEBUG_MODE:
                print(f"   ğŸ” é‡å‘½åå‰: value_column={value_column}, åœ¨frameä¸­={value_column in frame.columns}, frameè¡Œæ•°={len(frame)}")
            
            # æ ‡è®°å›è°ƒæ˜¯å¦å·²è¢«åº”ç”¨ï¼Œé¿å…é‡å¤è°ƒç”¨
            callback_applied = False
            
            if value_column not in frame.columns:
                # å¯¹äºæŸäº›æ¦‚å¿µï¼ˆå¦‚lgl_cncptï¼‰ï¼Œvalue_columnå¯èƒ½é€šè¿‡callbackåˆ›å»º
                # å…ˆå°è¯•åº”ç”¨callbackï¼Œç„¶åå†æ£€æŸ¥
                frame = _apply_callback(
                    frame,
                    source,
                    concept_name,
                    unit_column,
                    resolver=self,
                    patient_ids=patient_ids,
                    data_source=data_source,
                )
                callback_applied = True  # æ ‡è®°å›è°ƒå·²åº”ç”¨
                # å¦‚æœcallbackåˆ›å»ºäº†concept_nameï¼Œæ›´æ–°value_column
                if concept_name in frame.columns:
                    value_column = concept_name
                elif value_column not in frame.columns:
                    # å¦‚æœä»ç„¶ä¸å­˜åœ¨ï¼Œè·³è¿‡è¿™ä¸ªæº
                    if DEBUG_MODE:
                        print(f"   âš ï¸  value_column '{value_column}' ä¸å­˜åœ¨ï¼Œè·³è¿‡æ­¤æº")
                    frame = pd.DataFrame()
                    continue

            rename_map = {value_column: concept_name}
            frame = frame.rename(columns=rename_map)
            
            if DEBUG_MODE:
                print(f"   ğŸ”„ é‡å‘½åå: concept_name={concept_name}, åœ¨frameä¸­={concept_name in frame.columns}, frameè¡Œæ•°={len(frame)}")

            # If unit_column is specified but not in frame, set to None
            # This can happen if callbacks don't preserve unit columns
            if unit_column and unit_column not in frame.columns:
                unit_column = None

            if source.regex:
                # ç¡®å®š regex åº”è¯¥åº”ç”¨åœ¨å“ªä¸€åˆ—ï¼š
                # - å¦‚æœåŒæ—¶å­˜åœ¨ ids å’Œ regexï¼Œids ç”¨äºè¿‡æ»¤ sub_varï¼Œregex ç”¨äºè¿‡æ»¤ value_var
                # - å¦‚æœåªæœ‰ regexï¼ˆæ²¡æœ‰ idsï¼‰ï¼Œregex ç”¨äºè¿‡æ»¤ sub_varï¼ˆricu çš„ rgx_itm è¡Œä¸ºï¼‰
                # æ³¨æ„ï¼šæ­¤æ—¶ value_var å¯èƒ½å·²è¢«é‡å‘½åä¸º concept_nameï¼Œéœ€è¦æ£€æŸ¥ä¸¤è€…
                if source.ids is not None and source.value_var:
                    # æ··åˆæ¨¡å¼ï¼šids è¿‡æ»¤ sub_varï¼Œregex è¿‡æ»¤ value_var
                    # ä½† value_var å¯èƒ½å·²è¢«é‡å‘½åä¸º concept_name
                    if source.value_var in frame.columns:
                        regex_column = source.value_var
                    elif concept_name in frame.columns:
                        # value_var å·²è¢«é‡å‘½åä¸º concept_name
                        regex_column = concept_name
                    else:
                        regex_column = source.value_var  # ä¼šåœ¨ä¸‹é¢è§¦å‘è·³è¿‡
                else:
                    # æ ‡å‡† rgx_itm æ¨¡å¼ï¼šregex è¿‡æ»¤ sub_var
                    # ğŸ”§ FIX: å¦‚æœ sub_var == value_varï¼Œåˆ™ sub_var å·²è¢«é‡å‘½åä¸º concept_name
                    # éœ€è¦ä½¿ç”¨ concept_name è€Œä¸æ˜¯ sub_var
                    if source.sub_var == source.value_var:
                        regex_column = concept_name
                    elif source.sub_var in frame.columns:
                        regex_column = source.sub_var
                    else:
                        # sub_var å¯èƒ½è¢«é‡å‘½åäº†ï¼Œå°è¯• concept_name
                        regex_column = concept_name if concept_name in frame.columns else source.sub_var
                
                if not regex_column:
                    raise ValueError(
                        f"Concept '{concept_name}' specifies a regex but no column to match against."
                    )
                if regex_column not in frame.columns:
                    # å¦‚æœç›®æ ‡åˆ—ä¸å­˜åœ¨ï¼Œè·³è¿‡è¿™ä¸ªæº
                    if DEBUG_MODE:
                        print(f"   âš ï¸ regex åˆ— '{regex_column}' ä¸å­˜åœ¨ï¼Œè·³è¿‡æ­¤æº")
                    frame = pd.DataFrame()
                    continue
                # ä½¿ç”¨ regex=True å¹¶æŠ‘åˆ¶ UserWarning
                # str.contains ä¼šè­¦å‘Šå¦‚æœæ­£åˆ™è¡¨è¾¾å¼æœ‰æ•è·ç»„ä½†æ²¡æœ‰ä½¿ç”¨ str.extract
                # è¿™é‡Œæˆ‘ä»¬åªéœ€è¦åŒ¹é…ï¼Œä¸éœ€è¦æå–ï¼Œæ‰€ä»¥æŠ‘åˆ¶è¿™ä¸ªè­¦å‘Š
                pattern = source.regex
                series = frame[regex_column].astype(str)
                before_regex = len(frame)
                # ä½¿ç”¨ regex=True, na=False, å¹¶æŠ‘åˆ¶æ•è·ç»„è­¦å‘Š
                import warnings
                with warnings.catch_warnings():
                    warnings.filterwarnings('ignore', 
                                          message='This pattern is interpreted as a regular expression',
                                          category=UserWarning)
                    frame = frame[series.str.contains(pattern, case=False, na=False, regex=True)]
                if DEBUG_MODE:
                    print(f"   âœ“ regex è¿‡æ»¤ (åˆ—={regex_column}, pattern='{pattern}'): {before_regex} â†’ {len(frame)} è¡Œ")

            # âš ï¸ CRITICAL FIX: å¿…é¡»å…ˆåº”ç”¨å›è°ƒå‡½æ•°ï¼ˆå¦‚ convert_unitï¼‰å†åº”ç”¨å€¼èŒƒå›´è¿‡æ»¤å’Œå•ä½è¿‡æ»¤
            # åŸå› ï¼štemp ç­‰æ¦‚å¿µå¯èƒ½éœ€è¦å…ˆå°†åæ°åº¦è½¬æ¢ä¸ºæ‘„æ°åº¦ï¼Œç„¶åå†è¿‡æ»¤ 32-42Â°C çš„èŒƒå›´å’Œ C/Â°C å•ä½
            # å¦‚æœå…ˆè¿‡æ»¤ï¼Œåæ°åº¦å€¼ï¼ˆ97-100Â°Fï¼‰å’Œå•ä½ï¼ˆÂ°Fï¼‰ä¼šå› ä¸ºä¸ç¬¦åˆè¦æ±‚è€Œè¢«è¯¯åˆ 
            
            # åº”ç”¨å›è°ƒï¼ˆåœ¨å€¼èŒƒå›´è¿‡æ»¤å’Œå•ä½è¿‡æ»¤ä¹‹å‰ï¼‰
            # åªæœ‰å½“å›è°ƒå°šæœªè¢«åº”ç”¨æ—¶æ‰è°ƒç”¨ï¼ˆé¿å…é‡å¤è°ƒç”¨å¯¼è‡´durationå˜ä¸º0çš„é—®é¢˜ï¼‰
            if not callback_applied:
                frame = _apply_callback(
                    frame,
                    source,
                    concept_name,
                    source_unit_column,
                    resolver=self,
                    patient_ids=patient_ids,
                    data_source=data_source,
                )
            
            # å•ä½è¿‡æ»¤ï¼ˆåœ¨å›è°ƒä¹‹åï¼‰
            if definition.units and source_unit_column and source_unit_column in frame.columns:
                allowed_units = {unit.lower() for unit in definition.units}
                
                # å•ä½å½’ä¸€åŒ–ï¼šå¤„ç†ç­‰ä»·å•ä½
                # ä¾‹å¦‚ '10^9/l' ç­‰ä»·äº 'G/l' (Giga = 10^9)
                # ğŸ”§ mcL å’Œ uL æ˜¯ç­‰ä»·å•ä½ï¼šmicro-Liter
                unit_equivalents = {
                    '10^9/l': 'g/l',
                    '10^9/L': 'g/l',
                    '10e9/l': 'g/l',
                    'K/ul': 'k/ul',  # å¤§å°å†™å½’ä¸€åŒ–
                    'K/mcL': 'k/ul',  # eICU uses mcL instead of uL (microliter)
                    'k/mcl': 'k/ul',  # eICU uses mcL instead of uL (microliter)
                    '10^3/mcL': '10(3)/mcl',  # Alternative notation
                    '10^3/uL': '10(3)/mcl',   # Alternative notation
                    # eICU å•ä½å½’ä¸€åŒ–
                    'Units/L': 'iu/l',  # eICU uses 'Units/L' for enzyme activities (ALP, ALT, AST, etc.)
                    'units/l': 'iu/l',  # eICU uses 'Units/L' for enzyme activities
                    'U/L': 'iu/l',      # Common alternative
                    'u/l': 'iu/l',      # Common alternative (lowercase)
                    # AUMC è·å…°è¯­å•ä½
                    'ie': 'units',  # Internationale Eenheden (å›½é™…å•ä½)
                    'ie/uur': 'units/hr',  # å•ä½/å°æ—¶
                    'iu': 'units',  # International Units
                    'iu/hr': 'units/hr',
                }
                
                # ğŸ”§ CRITICAL: å¯¹äº AUMC æ•°æ®åº“ï¼Œæ”¾å®½å•ä½åŒ¹é…
                # AUMC ä½¿ç”¨è·å…°è¯­å•ä½ï¼ˆå¦‚ IE ä»£è¡¨å›½é™…å•ä½ï¼‰
                # å¹¶ä¸”æŸäº›æ¦‚å¿µï¼ˆå¦‚ insï¼‰ä½¿ç”¨ dose åˆ—ä½†æ¦‚å¿µå®šä¹‰æœŸæœ› units/hr
                # ä¸ºä¿æŒä¸ R ricu ä¸€è‡´ï¼Œå¯¹ AUMC ç¦ç”¨ä¸¥æ ¼å•ä½è¿‡æ»¤
                db_name = data_source.config.name if hasattr(data_source, 'config') and hasattr(data_source.config, 'name') else ''
                skip_unit_filter = db_name == 'aumc'
                
                if skip_unit_filter:
                    # AUMC: è·³è¿‡ä¸¥æ ¼å•ä½è¿‡æ»¤ï¼Œä½†ä»è®°å½•è°ƒè¯•ä¿¡æ¯
                    if DEBUG_MODE:
                        series = frame[source_unit_column].astype(str).str.strip()
                        print(f"   âš ï¸ AUMC: è·³è¿‡å•ä½è¿‡æ»¤ (åŸå•ä½: {series.unique()[:5]}, æœŸæœ›: {definition.units})")
                else:
                    # é AUMC æ•°æ®åº“ï¼šåº”ç”¨ä¸¥æ ¼å•ä½è¿‡æ»¤
                    # å½’ä¸€åŒ–æ•°æ®ä¸­çš„å•ä½
                    series = frame[source_unit_column].astype(str).str.strip()
                    normalized_series = series.replace(unit_equivalents).str.lower()
                
                    # ğŸ”§ è¿›ä¸€æ­¥å½’ä¸€åŒ–ï¼šå»é™¤éå­—æ¯æ•°å­—å­—ç¬¦åæ¯”è¾ƒ
                    # è¿™å¤„ç†äº† mmHg çš„å„ç§å˜ä½“ï¼šmm Hg, mm/Hg, mm(hg), mm[Hg] ç­‰
                    # NOTE: re æ¨¡å—å·²åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥ï¼Œä¸è¦åœ¨æ­¤å¤„é‡æ–°å¯¼å…¥ï¼Œå¦åˆ™ä¼šå¯¼è‡´ UnboundLocalError
                    def normalize_unit_for_comparison(unit_str):
                        """å½’ä¸€åŒ–å•ä½å­—ç¬¦ä¸²ï¼Œä»…ä¿ç•™å­—æ¯æ•°å­—å­—ç¬¦"""
                        if not unit_str or pd.isna(unit_str) or unit_str in ['', 'none', 'None', 'nan']:
                            return ''
                        return re.sub(r'[^a-z0-9]', '', str(unit_str).lower())
                    
                    normalized_allowed = {normalize_unit_for_comparison(u) for u in definition.units}
                    normalized_data = normalized_series.apply(normalize_unit_for_comparison)

                    # å¤„ç†None/ç©ºå­—ç¬¦ä¸²å•ä½çš„æƒ…å†µ
                    # å¯¹äºFiO2ç­‰æ•°æ®ï¼Œvalueuom=Noneæ—¶åº”è¯¥ä¿ç•™æ•°æ®ï¼Œè€Œä¸æ˜¯è¿‡æ»¤æ‰
                    # å°†'none'å’Œç©ºå­—ç¬¦ä¸²è§†ä¸ºåŒ¹é…ä»»ä½•å•ä½
                    # ğŸ”§ FIX: æ·»åŠ  'geen' (è·å…°è¯­ "æ— ") å’Œå…¶ä»–æ— å•ä½æ ‡è®°çš„æ”¯æŒ
                    # AUMC æ•°æ®ä½¿ç”¨ 'Geen' è¡¨ç¤ºæ— å•ä½ï¼ˆå¦‚ sao2 çš„ 0.xx æ ¼å¼å€¼ï¼‰
                    no_unit_markers = {'', 'none', 'geen', 'null', 'na', 'n/a', '-'}
                    mask = (
                        normalized_series.isin(allowed_units) |  # åŸå§‹æ¯”è¾ƒ
                        normalized_data.isin(normalized_allowed) |  # å½’ä¸€åŒ–æ¯”è¾ƒ
                        (normalized_series.isin(no_unit_markers))  # æ— å•ä½æ ‡è®°
                    )

                    before_unit = len(frame)
                    frame = frame[mask]
                    if before_unit != len(frame) and DEBUG_MODE:
                        print(f"   âœ“ å•ä½è¿‡æ»¤ (å…è®¸{definition.units}): {before_unit} â†’ {len(frame)} è¡Œ")
            
            # åªæœ‰åœ¨concept_nameåˆ—å­˜åœ¨æ—¶æ‰dropna
            # ä½†ä¸è¦è¿‡æ—©åˆ é™¤ï¼Œå› ä¸ºæŸäº›å›è°ƒå‡½æ•°å¯èƒ½ä¼šå¤„ç†NaNå€¼
            # åªåœ¨æ˜ç¡®éœ€è¦æ—¶æ‰åˆ é™¤NaNï¼ˆä¾‹å¦‚ï¼Œåœ¨åº”ç”¨min/maxè¿‡æ»¤ä¹‹å‰ï¼‰
            if concept_name in frame.columns:
                # å…ˆä¸åˆ é™¤NaNï¼Œå› ä¸ºæŸäº›æ¦‚å¿µï¼ˆå¦‚urine24ï¼‰å¯èƒ½éœ€è¦ä¿ç•™NaN
                # åªåœ¨å€¼èŒƒå›´è¿‡æ»¤ä¹‹å‰åˆ é™¤æ˜æ˜¾æ— æ•ˆçš„NaN
                # ä½†å¦‚æœå€¼èŒƒå›´å·²å®šä¹‰ï¼Œå¯ä»¥åœ¨è¿‡æ»¤ååˆ é™¤NaN
                pass  # æš‚æ—¶ä¸åˆ é™¤NaNï¼Œè®©åç»­å¤„ç†å†³å®š

            # å€¼èŒƒå›´è¿‡æ»¤ï¼ˆåœ¨å›è°ƒä¹‹åï¼‰
            # ç°åœ¨å€¼å·²ç»ç»è¿‡è½¬æ¢ï¼ˆå¦‚åæ°åº¦â†’æ‘„æ°åº¦ï¼‰ï¼Œå¯ä»¥å®‰å…¨è¿‡æ»¤
            if definition.minimum is not None:
                # ç¡®ä¿åˆ—æ˜¯æ•°å€¼ç±»å‹ï¼Œé¿å…å­—ç¬¦ä¸²æ¯”è¾ƒé”™è¯¯
                if concept_name in frame.columns:
                    frame[concept_name] = pd.to_numeric(frame[concept_name], errors='coerce')
                frame = frame[frame[concept_name] >= definition.minimum]
            if definition.maximum is not None:
                # ç¡®ä¿åˆ—æ˜¯æ•°å€¼ç±»å‹
                if concept_name in frame.columns:
                    frame[concept_name] = pd.to_numeric(frame[concept_name], errors='coerce')
                frame = frame[frame[concept_name] <= definition.maximum]
            
            # åœ¨å€¼èŒƒå›´è¿‡æ»¤åï¼Œåˆ é™¤æ— æ•ˆçš„NaNï¼ˆä½†ä¿ç•™æœ‰æ•ˆèŒƒå›´å†…çš„NaNç”¨äºåç»­å¤„ç†ï¼‰
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šåœ¨mergeæ¨¡å¼ä¸‹ä¿ç•™NaNè¡Œï¼Œä»¥åŒ¹é…ricuçš„å®Œæ•´æ—¶é—´ç½‘æ ¼é£æ ¼
            if concept_name in frame.columns:
                # æ£€æŸ¥æ˜¯å¦åœ¨mergeæ¨¡å¼ï¼ˆé€šè¿‡kwargsä¼ é€’ï¼‰
                keep_na_rows = kwargs.get('_keep_na_rows', False)
                if not keep_na_rows:
                    # åªåœ¨émergeæ¨¡å¼ä¸‹åˆ é™¤NaNï¼ˆå•ç‹¬åŠ è½½æ¦‚å¿µæ—¶ï¼‰
                    frame = frame.dropna(subset=[concept_name])
                # åœ¨mergeæ¨¡å¼ä¸‹ï¼Œä¿ç•™NaNè¡Œä»¥ä¾¿åç»­åˆå¹¶æ—¶åˆ›å»ºå®Œæ•´æ—¶é—´ç½‘æ ¼

            # å¦‚æœä½¿ç”¨äº† apply_map(var='sub_var')ï¼Œå°†æ˜ å°„åçš„ sub_var å¤åˆ¶åˆ° concept_name
            if uses_sub_var_mapping and source.sub_var in frame.columns:
                # sub_var åˆ—å·²ç»è¢« apply_map æ˜ å°„ä¸ºç±»åˆ«å€¼ï¼Œå°†å…¶å¤åˆ¶åˆ° concept_name åˆ—
                # ä½†æ˜¯è¦å…ˆä¿å­˜åŸå§‹çš„æ•°å€¼åˆ—ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼Œç”¨äºåç»­æŒç»­æ—¶é—´è®¡ç®—ï¼‰
                # å¯¹äº mech_vent è¿™ç§æ¦‚å¿µï¼ŒåŸå§‹ value åˆ—åŒ…å«æŒç»­æ—¶é—´ï¼Œéœ€è¦ä¿ç•™
                if concept_name in frame.columns and source.value_var:
                    # ä¿å­˜åŸå§‹æ•°å€¼åˆ—ä¸º _duration_val
                    frame['_duration_val'] = frame[concept_name]
                # å°†æ˜ å°„åçš„ç±»åˆ«å€¼å¤åˆ¶åˆ° concept_name
                frame[concept_name] = frame[source.sub_var]

            # DEBUG: åœ¨keep_colsè¿‡æ»¤å‰æ‰“å°
            keep_cols = {
                *(id_columns or []),
                *( [source_index_column] if source_index_column else []),
                concept_name,
            }
            # æ·»åŠ å®é™…å­˜åœ¨çš„time_columnsï¼ˆä¸å¼ºåˆ¶è¦æ±‚æ‰€æœ‰time_columnséƒ½å­˜åœ¨ï¼‰
            for tc in (time_columns or []):
                if tc in frame.columns:
                    keep_cols.add(tc)
            
            if source_unit_column and source_unit_column in frame.columns:
                keep_cols.add(source_unit_column)
            
            # ä¿ç•™ _duration_val åˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œç”¨äºåç»­æŒç»­æ—¶é—´è®¡ç®—
            if '_duration_val' in frame.columns:
                keep_cols.add('_duration_val')
            
            # ä¿ç•™ duration åˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œç”¨äº WinTbl
            # durationåˆ—é€šå¸¸å‘½åä¸º concept_name + '_dur'
            duration_col_name = concept_name + '_dur'
            if duration_col_name in frame.columns:
                keep_cols.add(duration_col_name)
            
            # åªæ£€æŸ¥å¿…éœ€çš„åˆ—ï¼šid_columns, index_column, concept_name
            # æ³¨æ„ï¼šå¯¹äºå¤šæºæ¦‚å¿µï¼Œä¸åŒæºå¯èƒ½ä½¿ç”¨ä¸åŒçš„æ—¶é—´åˆ—åï¼ˆå¦‚starttime vs charttimeï¼‰
            # æ‰€ä»¥å¯¹äºç´¢å¼•åˆ—ï¼Œæˆ‘ä»¬åªæ£€æŸ¥æ˜¯å¦åœ¨æ•°æ®ä¸­æœ‰ä»»ä½•æ—¶é—´åˆ—
            required_cols = {
                *(id_columns or []),
                concept_name,
            }
            missing = required_cols - set(frame.columns)
            
            # å¯¹äºç´¢å¼•åˆ—ï¼Œæ£€æŸ¥æ˜¯å¦åœ¨æ•°æ®ä¸­æœ‰ä»»ä½•æ—¶é—´åˆ—
            if source_index_column:
                # æ£€æŸ¥æ˜¯å¦æœ‰source_index_columnï¼Œæˆ–è€…æœ‰ç±»ä¼¼çš„æ—¶é—´åˆ—
                time_aliases = {"starttime", "endtime", "charttime", "storetime"}
                time_cols = []
                for col in frame.columns:
                    if not isinstance(col, str):
                        continue
                    lowered = col.lower()
                    if "time" in lowered or lowered in time_aliases:
                        time_cols.append(col)
                if source_index_column not in frame.columns and not time_cols:
                    missing.add(source_index_column)
            
            if missing:
                # å¯¹äºlabeventsç­‰è¡¨ï¼Œå¦‚æœç¼ºå°‘stay_idä½†æ˜ å°„è¿‡ç¨‹å·²å¤„ç†ï¼Œåº”è¯¥å·²ç»æœ‰stay_idäº†
                # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œè¯´æ˜æ˜ å°„å¤±è´¥ï¼Œè·³è¿‡è¿™ä¸ªæºå¹¶ç»§ç»­ï¼ˆä¸æŠ¥é”™ï¼‰
                if 'stay_id' in missing and source.table in ['labevents', 'microbiologyevents']:
                    frame = pd.DataFrame()
                    continue
                # å¯¹äºeICUçš„infusiondrugè¡¨ï¼Œpatientunitstayidåœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½è¢«è¿‡æ»¤æ‰
                # è¿™æ˜¯ç”±äºeICUæ•°æ®å¤„ç†ç®¡é“çš„ç‰¹æ®Šæ€§é€ æˆçš„ï¼Œæˆ‘ä»¬åº”è¯¥æ”¾å®½è¦æ±‚
                if (hasattr(data_source, 'config') and
                    hasattr(data_source.config, 'name') and
                    data_source.config.name in ['eicu', 'eicu_demo'] and
                    source.table == 'infusiondrug' and
                    missing.issubset({'patientunitstayid', 'infusiondrugid', 'volumeoffluid'})):
                    logging.debug(f"eICU infusiondrug missing ID columns {missing}, but continuing with available data")
                    missing.discard('patientunitstayid')
                    missing.discard('infusiondrugid')
                    missing.discard('volumeoffluid')

                # å¯¹äºå¤šæºæ¦‚å¿µï¼Œå¦‚æœæŸä¸ªæºç¼ºå°‘index_columnä½†å…¶ä»–æºæœ‰ï¼Œè¿™æ˜¯å¯ä»¥æ¥å—çš„
                if source_index_column in missing and len(sources) > 1:
                    missing.discard(source_index_column)

                if missing:
                    raise KeyError(
                        f"Missing expected columns {sorted(missing)} in concept "
                        f"data for '{concept_name}' (table '{source.table}')"
                    )
            # ç¡®ä¿IDåˆ—åœ¨æ•°æ®ä¸­
            available_id_cols = [col for col in id_columns if col in frame.columns]
            if not available_id_cols and id_columns:
                logging.debug(f"é…ç½®çš„IDåˆ— {id_columns} ä¸åœ¨æ•°æ®ä¸­ï¼Œå¯ç”¨åˆ—: {list(frame.columns)[:10]}")
            
            ordered_cols: List[str] = []
            # ä¿ç•™æ‰€æœ‰å¯ç”¨çš„IDåˆ—ï¼ˆä¸åªæ˜¯ç¬¬ä¸€ä¸ªï¼‰
            ordered_cols.extend(available_id_cols)
            if source_index_column and source_index_column not in ordered_cols:
                ordered_cols.append(source_index_column)
            extra_time = [
                col for col in time_columns if col and col not in ordered_cols
            ]
            ordered_cols.extend(extra_time)
            ordered_cols.append(concept_name)
            if source_unit_column and source_unit_column not in ordered_cols:
                ordered_cols.append(source_unit_column)
            
            # æ·»åŠ  duration åˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            duration_col_name = concept_name + '_dur'
            if duration_col_name in frame.columns and duration_col_name not in ordered_cols:
                ordered_cols.append(duration_col_name)
            
            # ğŸ”§ FIX: ä¿ç•™ endtime åˆ—ç”¨äºçª—å£æ¦‚å¿µå±•å¼€
            # mech_vent ç­‰çª—å£æ¦‚å¿µéœ€è¦ endtime æ¥è¿›è¡Œæ—¶é—´å±•å¼€
            # å¦‚æœæœ‰ dur_var="endtime" çš„å®šä¹‰ï¼Œendtime åˆ—å¿…é¡»ä¿ç•™
            for endtime_candidate in ['endtime', 'end_time', 'stop']:
                if endtime_candidate in frame.columns and endtime_candidate not in ordered_cols:
                    ordered_cols.append(endtime_candidate)
                    break
            
            ordered_cols = [col for col in ordered_cols if col in frame.columns]
            
            # Check and remove duplicate columns before appending
            frame_subset = frame.loc[:, ordered_cols]
            if frame_subset.columns.duplicated().any():
                frame_subset = frame_subset.loc[:, ~frame_subset.columns.duplicated()]
            
            frames.append(frame_subset)

        if not frames:
            # è¿”å›ç©º DataFrame è€Œä¸æ˜¯æŠ¥é”™ï¼ˆæŸäº›æ¦‚å¿µå¯èƒ½åœ¨æµ‹è¯•æ•°æ®ä¸­æ²¡æœ‰æ•°æ®ï¼‰
            # æ£€æŸ¥æ˜¯å¦æ˜¯å› ä¸ºç¼ºå°‘å¿…è¦çš„è¡¨æ–‡ä»¶
            missing_tables = []
            db_name = data_source.config.name if hasattr(data_source, 'config') and hasattr(data_source.config, 'name') else 'unknown'

            for source in sources:
                if hasattr(source, 'table'):
                    import os
                    if hasattr(data_source, 'base_path') and data_source.base_path is not None:
                        table_file = data_source.base_path / f"{source.table}.parquet"
                        csv_file = data_source.base_path / f"{source.table}.csv"
                        csv_gz_file = data_source.base_path / f"{source.table}.csv.gz"

                        if not (table_file.exists() or csv_file.exists() or csv_gz_file.exists()):
                            missing_tables.append(source.table)

            if missing_tables and db_name in ['eicu', 'eicu_demo']:
                logging.debug(f"eICUæµ‹è¯•æ•°æ®ç¼ºå°‘è¡¨ {missing_tables}ï¼Œæ¦‚å¿µ '{concept_name}' æš‚æ—¶ä¸å¯ç”¨")
            else:
                # åªå¯¹æŸäº›é«˜çº§æ²»ç–—æ¦‚å¿µæ˜¾ç¤ºINFOçº§åˆ«ä¿¡æ¯
                advanced_concepts = ['ecmo', 'ecmo_indication', 'mech_circ_support', 'rrt']
                if concept_name in advanced_concepts:
                    logging.info(f"æ¦‚å¿µ '{concept_name}' åœ¨æµ‹è¯•æ•°æ®ä¸­ä¸å¯ç”¨ï¼ˆé«˜çº§æ²»ç–—ï¼‰")
                else:
                    logging.debug(f"æ¦‚å¿µ '{concept_name}' çš„æ‰€æœ‰ {len(sources)} ä¸ªæ•°æ®æºéƒ½è¿”å›ç©ºæ•°æ®")
            # åˆ›å»ºä¸€ä¸ªç©ºçš„ DataFrameï¼ŒåŒ…å«å¿…è¦çš„åˆ—
            # ç¡®ä¿æœ‰ ID åˆ—ï¼šä½¿ç”¨é…ç½®çš„ id_columnsï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨æ•°æ®åº“çš„é»˜è®¤IDåˆ—
            if not id_columns:
                # ä»æ•°æ®æºåç§°æ¨æ–­é»˜è®¤IDåˆ—
                db_name = data_source.config.name if hasattr(data_source, 'config') and hasattr(data_source.config, 'name') else 'unknown'
                id_columns = _default_id_columns_for_db(db_name)
            empty_cols = list(id_columns) + ([index_column] if index_column else []) + [concept_name]
            combined = pd.DataFrame(columns=empty_cols)
        else:
            # Check for duplicate column names before concat
            for i, frame in enumerate(frames):
                if frame.columns.duplicated().any():
                    # Keep only first occurrence of duplicate columns
                    frames[i] = frame.loc[:, ~frame.columns.duplicated()]
            
            # ğŸ” DEBUG: æ£€æŸ¥æ¯ä¸ª frame çš„æ‚£è€…æ•°ï¼ˆåªåœ¨è°ƒè¯•æ¨¡å¼ä¸‹æ˜¾ç¤ºï¼‰
            if DEBUG_MODE and concept_name == 'plt':
                print(f"\\nğŸ” [pltåˆå¹¶] å‡†å¤‡åˆå¹¶ {len(frames)} ä¸ª sources:")
                for i, frame in enumerate(frames):
                    if 'stay_id' in frame.columns:
                        print(f"  Source {i+1}: {len(frame)}è¡Œ, {frame['stay_id'].nunique()}ä¸ªæ‚£è€…, IDs={sorted(frame['stay_id'].unique())[:5]}")
                    else:
                        print(f"  Source {i+1}: {len(frame)}è¡Œ, æ— stay_idåˆ—")
            
            combined = pd.concat(frames, ignore_index=True)
            
        # DEBUG
        # Standardize time column name for eICU BEFORE any processing
        # eICU uses different time column names (labresultoffset, observationoffset, etc.)
        # For multi-source concepts (like abx), different sources may use different offset columns
        # Rename all offset columns to 'charttime' to enable unified processing
        db_name = data_source.config.name if hasattr(data_source, 'config') and hasattr(data_source.config, 'name') else ''
        if db_name in ['eicu', 'eicu_demo'] and index_column:
            # All possible eICU time offset columns
            eicu_time_cols = [
                'labresultoffset', 'observationoffset', 'nursecharting_offset', 
                'respiratorycharting_offset', 'intakeoutput_offset', 'respchartoffset',
                'infusionoffset', 'drugstartoffset', 'drugstopoffset', 'drugorderoffset',
                'culturetakenoffset', 'cultureoffset',
                # ğŸ”¥ æ·»åŠ  respiratorycare è¡¨çš„æ—¶é—´åˆ—
                'respcarestatusoffset', 'ventstartoffset', 'ventendoffset',
                'priorventstartoffset', 'priorventendoffset',
            ]
            
            offset_cols_in_data = [col for col in combined.columns if col in eicu_time_cols]
            
            if offset_cols_in_data:
                # é‡å‘½åç¬¬ä¸€ä¸ªoffsetåˆ—ä¸ºcharttime
                first_offset = offset_cols_in_data[0]
                combined = combined.rename(columns={first_offset: 'charttime'})
                
                # åˆå¹¶å…¶ä»–offsetåˆ—åˆ°charttimeï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªéNaNå€¼ï¼‰
                for offset_col in offset_cols_in_data[1:]:
                    if offset_col in combined.columns:
                        combined['charttime'] = combined['charttime'].fillna(combined[offset_col])
                        combined = combined.drop(columns=[offset_col])
                
                index_column = 'charttime'  # Update index_column for subsequent processing
        
        sort_keys = [col for col in id_columns if col]
        if index_column:
            sort_keys.append(index_column)
        if sort_keys:
            # ä¿®å¤ï¼šç¡®ä¿sort_keysä¸­çš„åˆ—éƒ½å­˜åœ¨äºcombinedä¸­
            sort_keys = [k for k in sort_keys if k in combined.columns]
            
            if not sort_keys:
                # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„æ’åºé”®ï¼Œè·³è¿‡æ’åº
                pass
            else:
                # ä¿®å¤ï¼šå¦‚æœåˆ—åé‡å¤ï¼Œå…ˆå»é‡
                if combined.columns.duplicated().any():
                    # ä¿ç•™ç¬¬ä¸€ä¸ªå‡ºç°çš„åˆ—ï¼Œåˆ é™¤é‡å¤çš„
                    combined = combined.loc[:, ~combined.columns.duplicated()]

                # ä¿®å¤ï¼šç¡®ä¿æ’åºé”®ä¸­çš„åˆ—å…·æœ‰ä¸€è‡´çš„ç±»å‹ï¼Œé¿å…æ··åˆç±»å‹æ’åºé—®é¢˜
                try:
                    combined = combined.sort_values(by=sort_keys)
                except TypeError as e:
                    if 'ordered' in str(e) or 'not supported between instances' in str(e):
                        # å¤„ç†æ··åˆç±»å‹æ’åºé—®é¢˜
                        if DEBUG_MODE:
                            print(f"      [æ’åºä¿®å¤] æ£€æµ‹åˆ°æ··åˆç±»å‹æ’åºé—®é¢˜: {e}")

                        # å°è¯•é€ä¸ªæ£€æŸ¥å’Œä¿®å¤æ’åºé”®çš„ç±»å‹
                        cleaned_combined = combined.copy()
                        for key in sort_keys:
                            if key in cleaned_combined.columns:
                                # å¦‚æœæ˜¯æ—¶é—´åˆ—ï¼Œç¡®ä¿éƒ½æ˜¯datetimeç±»å‹
                                if 'time' in key.lower() or key == 'charttime':
                                    try:
                                        cleaned_combined[key] = pd.to_datetime(cleaned_combined[key], errors='coerce')
                                    except:
                                        pass
                                # å¦‚æœæœ‰æ··åˆç±»å‹ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿›è¡Œæ’åº
                                else:
                                    try:
                                        # å°è¯•æ’åºä»¥æ£€æµ‹é—®é¢˜
                                        cleaned_combined.sort_values(by=[key])
                                    except TypeError:
                                        if DEBUG_MODE:
                                            print(f"      [æ’åºä¿®å¤] åˆ—{key}å­˜åœ¨æ··åˆç±»å‹ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²")
                                        cleaned_combined[key] = cleaned_combined[key].astype(str)

                        # é‡æ–°æ’åº
                        combined = cleaned_combined.sort_values(by=sort_keys)
                    else:
                        # å…¶ä»–ç±»å‹çš„é”™è¯¯ï¼Œé‡æ–°æŠ›å‡º
                        raise
        combined = combined.reset_index(drop=True)
        agg_value = self._coerce_final_aggregator(aggregator)
        if agg_value in (None, "auto"):
            fallback_agg = definition.aggregate
            if fallback_agg is not None:
                agg_value = self._coerce_final_aggregator(fallback_agg)

        # CRITICAL FIX: Avoid double aggregation issue
        # Strategy: Only use change_interval's aggregation (on relative time after floor)
        # Do NOT use _apply_aggregation before time alignment
        should_aggregate_in_change_interval = agg_value is not False
        
        # ğŸ”§ FIX: ç¡®ä¿ index_column å®é™…å­˜åœ¨äº combined ä¸­
        # å¯¹äº id_tbl ç±»å‹çš„æ¦‚å¿µï¼ˆå¦‚ los_icuï¼‰ï¼Œå¯èƒ½ä»è¡¨é…ç½®ç»§æ‰¿äº† index_columnï¼Œä½†æ•°æ®ä¸­ä¸åŒ…å«è¯¥åˆ—
        if index_column and index_column not in combined.columns:
            index_column = None
        
        # å¦‚æœæ•°æ®ä¸ºç©ºï¼Œè¿”å›ç©º ICUTable
        if combined.empty:
            return ICUTable(
                data=combined,
                id_columns=id_columns,
                index_column=index_column,  # æ­¤æ—¶å·²éªŒè¯å­˜åœ¨æˆ–ä¸º None
                value_column=concept_name,
                unit_column=None,
                time_columns=[col for col in time_columns if col and col in combined.columns],
            )
        
        # Only set unit_column if it actually exists in the combined data
        final_unit_column = unit_column if unit_column and unit_column in combined.columns else None
        
        # Apply interval alignment and aggregation if interval is specified
        if interval is not None and index_column and index_column in combined.columns:
            # DEBUG
            from .ts_utils import change_interval
            
            # Align time to ICU admission if requested (BEFORE any aggregation)
            if align_to_admission:
                # DEBUG
                combined = self._align_time_to_admission(
                    combined,
                    data_source,
                    id_columns,
                    index_column
                )
                
                # ğŸ”§ FIX: _align_time_to_admission å¯èƒ½ä¼šåˆ é™¤ intime/outtime åˆ—
                # éœ€è¦é‡æ–°æ£€æŸ¥ index_column æ˜¯å¦ä»åœ¨ combined ä¸­
                if index_column and index_column not in combined.columns:
                    # å°è¯•æŸ¥æ‰¾å¯èƒ½çš„æ—¶é—´åˆ—
                    time_cols = [c for c in combined.columns if c in ['start', 'charttime', 'measuredat']]
                    if time_cols:
                        index_column = time_cols[0]
                    else:
                        # æ²¡æœ‰æœ‰æ•ˆçš„æ—¶é—´åˆ—ï¼Œè·³è¿‡ interval å¤„ç†
                        index_column = None
                
                # å¦‚æœ index_column å˜æˆ Noneï¼Œè·³è¿‡åç»­çš„ interval å¤„ç†
                if index_column is None:
                    # è¿”å›ä¸å¸¦ interval å¤„ç†çš„æ•°æ®
                    return ICUTable(
                        data=combined.reset_index(drop=True),
                        id_columns=id_columns,
                        index_column=None,
                        value_column=concept_name,
                        unit_column=final_unit_column,
                        time_columns=[col for col in time_columns if col and col in combined.columns],
                    )
                
                # DEBUG
            # Determine aggregation method for change_interval
            # This is the ONLY aggregation we should do (on relative time)
            agg_method = agg_value if agg_value not in (None, False, "auto") else None
            if agg_method in (None, "auto"):
                agg_method = None
            # Default aggregation based on value type (matches R ricu)
            if agg_method is None:
                # Check value column type
                if concept_name in combined.columns:
                    col_dtype = combined[concept_name].dtype
                    if pd.api.types.is_bool_dtype(col_dtype):
                        agg_method = 'any'  # R ricu: logical -> "any"
                    elif pd.api.types.is_numeric_dtype(col_dtype):
                        agg_method = 'median'  # R ricu: numeric -> "median"
                    else:
                        agg_method = 'first'  # R ricu: character/other -> "first"
            
            # Create ICUTable temporarily to use change_interval
            temp_table = ICUTable(
                data=combined,
                id_columns=id_columns,
                index_column=index_column,
                value_column=concept_name,
                unit_column=final_unit_column,
                time_columns=[col for col in time_columns if col],
            )

            fill_missing = self._should_fill_gaps(concept_name, definition)
            fill_method = self._get_fill_method(concept_name, definition)
            
            # Apply interval change with aggregation (SINGLE aggregation on relative time)
            combined_result = change_interval(
                temp_table,
                interval=interval,
                aggregation=agg_method,
                fill_gaps=fill_missing,
                fill_method=fill_method,
                copy=False
            )
            
            # Extract data if ICUTable is returned
            if hasattr(combined_result, 'data'):
                combined = combined_result.data
                # æ›´æ–°index_columnï¼šchange_intervalå¯èƒ½æ”¹å˜äº†æ—¶é—´åˆ—å(å¦‚å˜ä¸º'start')
                if hasattr(combined_result, 'index_column') and combined_result.index_column:
                    index_column = combined_result.index_column
            else:
                combined = combined_result
        elif align_to_admission:
            # Just alignment, no interval/aggregation
            combined = self._align_time_to_admission(
                combined,
                data_source,
                id_columns,
                index_column
            )
        
        # ğŸ”§ NOTE: ä¸è¿‡æ»¤è´Ÿæ—¶é—´ï¼ˆå…¥ICUå‰çš„æ•°æ®ï¼‰ï¼Œricu ä¿ç•™è¿™äº›æ•°æ®
        # ä¾‹å¦‚ï¼šAUMC esr measuredat=-2 è¡¨ç¤ºå…¥é™¢å‰2å°æ—¶çš„æ•°æ®ï¼Œricu ä¹Ÿä¿ç•™
        
        # æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿index_columnå­˜åœ¨äºcombinedä¸­
        if index_column and index_column not in combined.columns:
            # å°è¯•æŸ¥æ‰¾å¯èƒ½çš„æ—¶é—´åˆ—
            time_cols = [c for c in combined.columns if c in ['start', 'charttime', 'measuredat', index_column]]
            if time_cols:
                index_column = time_cols[0]
            else:
                # æ²¡æœ‰æœ‰æ•ˆçš„æ—¶é—´åˆ—ï¼Œè®¾ä¸ºNone
                index_column = None
        
        # CRITICAL: Check if target is 'win_tbl', and convert to WinTbl if needed
        # For concepts like mech_vent that have target='win_tbl' but no concept-level callback
        # DISABLED for now - WinTbl conversion has issues with endtime handling
        # Return raw ICUTable and let expansion happen in _ensure_concept_loaded
        if False and definition.target == 'win_tbl' and interval is not None:
            from .table import WinTbl
            # WinTbl needs: index_var (time), dur_var (duration), id_vars (IDs)
            # Check if we have endtime or duration columns
            has_endtime = any(col in combined.columns for col in ['endtime', 'end_time', 'stop'])
            has_duration = any(col in combined.columns for col in ['duration', 'dur', concept_name + '_dur'])
            
            if has_endtime or has_duration:
                # Find the appropriate columns
                endtime_col = next((col for col in ['endtime', 'end_time', 'stop'] if col in combined.columns), None)
                duration_col = next((col for col in ['duration', 'dur', concept_name + '_dur'] if col in combined.columns), None)
                
                # If we have endtime, calculate duration
                if endtime_col and index_column:
                    # Ensure both are numeric (hours)
                    # endtime might still be datetime if it wasn't properly aligned
                    if pd.api.types.is_datetime64_any_dtype(combined[endtime_col]):
                        # Skip endtime conversion for now - has issues
                        # TODO: Fix endtime handling for procedureevents
                        pass
                    
                    # Now both should be numeric
                    if pd.api.types.is_numeric_dtype(combined[endtime_col]) and pd.api.types.is_numeric_dtype(combined[index_column]):
                        # Calculate duration as endtime - starttime
                        combined[concept_name + '_dur'] = combined[endtime_col] - combined[index_column]
                        duration_col = concept_name + '_dur'
                        # Remove endtime column (WinTbl uses duration, not endtime)
                        combined = combined.drop(columns=[endtime_col], errors='ignore')
                    else:
                        # Can't calculate duration, skip WinTbl conversion
                        duration_col = None
                
                if duration_col and index_column:
                    # Create WinTbl
                    return WinTbl(
                        data=combined,
                        id_vars=id_columns,
                        index_var=index_column,
                        dur_var=duration_col,
                    )
        
        if concept_name == "infusionoffset" and index_column and index_column in combined.columns:
            combined[concept_name] = combined[index_column]
            combined = combined.drop(columns=["drugrate"], errors="ignore")
        try:
            return ICUTable(
                data=combined,
                id_columns=id_columns,
                index_column=index_column,  # Already updated for eICU if needed
                value_column=concept_name,
                unit_column=final_unit_column,
                time_columns=[col for col in time_columns if col],
            )
        except KeyError as exc:
            if concept_name == "infusionoffset" and index_column and index_column in combined.columns:
                combined[concept_name] = combined[index_column]
                combined = combined.drop(columns=["drugrate"], errors="ignore")
                return ICUTable(
                    data=combined,
                    id_columns=id_columns,
                    index_column=index_column,
                    value_column=concept_name,
                    unit_column=final_unit_column,
                    time_columns=[col for col in time_columns if col],
                )
            raise exc
    
    def _align_time_to_admission(
        self,
        data: pd.DataFrame,
        data_source: ICUDataSource,
        id_columns: List[str],
        index_column: str,
        time_columns: Optional[List[str]] = None,
    ) -> pd.DataFrame:
        """Align time column to ICU admission time as anchor (R ricu as_dt_min).
        
        Converts absolute time to relative time (hours or minutes since ICU admission).
        This replicates R ricu's behavior where time is relative to admission.
        
        Args:
            data: Input DataFrame with time column
            data_source: Data source instance
            id_columns: ID columns (e.g., ['stay_id'])
            index_column: Time column name (e.g., 'charttime')
            time_columns: Additional time columns to convert (e.g., ['stop', 'mech_vent_dur'])
            
        Returns:
            DataFrame with time converted to hours since ICU admission
        """
        # eICUå’ŒAUMCæ—¶é—´åˆ—éœ€è¦ç‰¹æ®Šå¤„ç†
        # eICU uses offset columns (labresultoffset, observationoffset, etc.) which are
        # already in MINUTES from ICU admission. Convert to HOURS for consistency.
        # AUMC times are ABSOLUTE timestamps in MINUTES (converted from ms in datasource.py).
        # For AUMC, we need to subtract admittedat to get relative time since ICU admission.
        db_name = data_source.config.name if hasattr(data_source, 'config') and hasattr(data_source.config, 'name') else ''
        if db_name in ['eicu', 'eicu_demo']:
            # eICUæ—¶é—´åˆ—æ˜¯ç›¸å¯¹äºå…¥é™¢æ—¶é—´çš„offset,å•ä½æ˜¯åˆ†é’Ÿ
            # è½¬æ¢ä¸ºå°æ—¶ä»¥ä¸å…¶ä»–æ•°æ®åº“ä¿æŒä¸€è‡´
            
            # æ”¶é›†æ‰€æœ‰éœ€è¦è½¬æ¢çš„æ—¶é—´åˆ—
            cols_to_convert = set()
            if index_column and index_column in data.columns:
                cols_to_convert.add(index_column)
            
            # æ·»åŠ é¢å¤–çš„æ—¶é—´åˆ— (å¦‚ stop ç­‰)
            if time_columns:
                for col in time_columns:
                    if col and col in data.columns:
                        if not col.endswith('_dur'):
                            cols_to_convert.add(col)
            
            # è‡ªåŠ¨æ£€æµ‹å…¶ä»–å¯èƒ½çš„æ—¶é—´åˆ— (start, stop)
            for col in data.columns:
                if col in ['start', 'stop']:
                    if pd.api.types.is_numeric_dtype(data[col]):
                        cols_to_convert.add(col)
            
            # è½¬æ¢æ‰€æœ‰æ—¶é—´åˆ—ï¼ˆä»åˆ†é’Ÿåˆ°å°æ—¶ï¼‰
            for col in cols_to_convert:
                if col in data.columns and pd.api.types.is_numeric_dtype(data[col]):
                    data[col] = data[col] / 60.0
            return data
        
        if db_name == 'aumc':
            # AUMCæ—¶é—´åˆ—æ˜¯ç»å¯¹æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼Œå·²åœ¨datasource.pyä¸­è½¬æ¢ä¸ºåˆ†é’Ÿï¼‰
            # éœ€è¦å‡å» admittedat å¾—åˆ°ç›¸å¯¹äº ICU å…¥ä½çš„æ—¶é—´
            # è¿™å¯¹äºå¤šæ¬¡å…¥ä½çš„æ‚£è€…ï¼ˆå¦‚patient 14ï¼Œadmittedat=208661820000msï¼‰å¾ˆé‡è¦
            
            # æ”¶é›†æ‰€æœ‰éœ€è¦è½¬æ¢çš„æ—¶é—´åˆ—
            cols_to_convert = set()
            if index_column and index_column in data.columns:
                cols_to_convert.add(index_column)
            
            if time_columns:
                for col in time_columns:
                    if col and col in data.columns:
                        if not col.endswith('_dur'):
                            cols_to_convert.add(col)
            
            for col in data.columns:
                if col in ['start', 'stop']:
                    if pd.api.types.is_numeric_dtype(data[col]):
                        cols_to_convert.add(col)
            
            if not cols_to_convert:
                return data
            
            # è·å– admittedat ä»¥è®¡ç®—ç›¸å¯¹æ—¶é—´
            # å¯¹äº AUMCï¼ŒID åˆ—æ˜¯ admissionid
            id_col = 'admissionid' if 'admissionid' in data.columns else (id_columns[0] if id_columns else None)
            
            if id_col and id_col in data.columns:
                try:
                    # åŠ è½½ admissions è¡¨è·å– admittedat
                    admissions = data_source.load_table('admissions', 
                                                         columns=['admissionid', 'admittedat'], 
                                                         verbose=False)
                    if hasattr(admissions, 'data'):
                        admissions_df = admissions.data
                    else:
                        admissions_df = admissions
                    
                    # admittedat ä¹Ÿæ˜¯æ¯«ç§’ï¼Œéœ€è¦è½¬æ¢ä¸ºåˆ†é’Ÿ
                    if 'admittedat' in admissions_df.columns:
                        admissions_df['admittedat_min'] = (admissions_df['admittedat'] / 60000.0).apply(
                            lambda x: int(x) if pd.notna(x) else x).astype('float64')
                        
                        # åˆå¹¶ admittedat åˆ°æ•°æ®ä¸­
                        data = data.merge(admissions_df[['admissionid', 'admittedat_min']], 
                                         on='admissionid', how='left')
                        
                        # ä»æ—¶é—´åˆ—ä¸­å‡å» admittedat_min å¾—åˆ°ç›¸å¯¹æ—¶é—´
                        for col in cols_to_convert:
                            if col in data.columns and pd.api.types.is_numeric_dtype(data[col]):
                                if DEBUG_MODE:
                                    try:
                                        print(f"   ğŸ [AUMC _align_time] {col} before subtract: min/max = {data[col].min()} / {data[col].max()}")
                                    except Exception:
                                        pass
                                # å‡å» admittedat_min å¾—åˆ°ç›¸å¯¹åˆ†é’Ÿ
                                data[col] = data[col] - data['admittedat_min']
                                # è½¬æ¢ä¸ºå°æ—¶
                                data[col] = data[col] / 60.0
                                if DEBUG_MODE:
                                    try:
                                        print(f"   ğŸ [AUMC _align_time] {col} after subtract & hours: min/max = {data[col].min()} / {data[col].max()}")
                                    except Exception:
                                        pass
                        
                        # åˆ é™¤è¾…åŠ©åˆ—
                        if 'admittedat_min' in data.columns:
                            data = data.drop(columns=['admittedat_min'])
                        
                        return data
                except Exception as e:
                    if DEBUG_MODE:
                        print(f"   âš ï¸ [AUMC _align_time] Failed to load admittedat: {e}")
            
            # å›é€€ï¼šå¦‚æœæ— æ³•è·å– admittedatï¼Œåªåšå•ä½è½¬æ¢
            for col in cols_to_convert:
                if col in data.columns and pd.api.types.is_numeric_dtype(data[col]):
                    data[col] = data[col] / 60.0
            return data
        
        # Early return checks (no verbose output for performance)
        if data.empty or not index_column or index_column not in data.columns:
            return data
        
        # Get the primary ID column (usually stay_id for MIMIC-IV)
        if not id_columns:
            return data
        
        primary_id = id_columns[0]
        if primary_id not in data.columns:
            return data
        
        # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœprimary_idä¸æ˜¯stay_idï¼Œéœ€è¦å…ˆjoin icustaysè·å–stay_id
        # è¿™å¯¹äºlabeventsï¼ˆä½¿ç”¨subject_idï¼‰å¾ˆé‡è¦
        if primary_id != 'stay_id' and 'stay_id' not in data.columns:
            try:
                # Use cached icustays table if available
                cache_key = f"{primary_id}_stay_id_intime"
                if self._icustays_cache is None or cache_key not in self._icustays_cache.columns:
                    icustays_temp = data_source.load_table('icustays', columns=[primary_id, 'stay_id', 'intime'], verbose=False)
                if hasattr(icustays_temp, 'data'):
                    icustays_temp_df = icustays_temp.data
                else:
                    icustays_temp_df = icustays_temp
                
                # ç¡®ä¿intimeæ˜¯tz-naive datetime
                if pd.api.types.is_datetime64_any_dtype(icustays_temp_df['intime']):
                    if hasattr(icustays_temp_df['intime'].dt, 'tz') and icustays_temp_df['intime'].dt.tz is not None:
                        icustays_temp_df['intime'] = icustays_temp_df['intime'].dt.tz_localize(None)
                    
                    # Cache the table
                    self._icustays_cache = icustays_temp_df
                else:
                    icustays_temp_df = self._icustays_cache
                
                # Joinè·å–stay_idå’Œintime
                data = data.merge(icustays_temp_df[[primary_id, 'stay_id', 'intime']], 
                                 on=primary_id, how='left')
                
                # æ›´æ–°primary_idä¸ºstay_id
                primary_id = 'stay_id'
                # å·²ç»æœ‰intimeäº†ï¼Œåé¢ä¸éœ€è¦å†åŠ è½½
            except Exception as e:
                return data
        
        # è‹¥æ—¶é—´åˆ—å·²æ˜¯numericï¼ˆç›¸å¯¹å°æ—¶ï¼‰ï¼Œä»å°è¯•æŒ‰ICUçª—å£è£å‰ªèŒƒå›´
        if pd.api.types.is_numeric_dtype(data[index_column]):
            try:
                # ç¡®ä¿å­˜åœ¨intime/outtimeä»¥è®¡ç®—çª—å£é•¿åº¦ï¼ˆå°æ—¶ï¼‰
                if 'intime' not in data.columns or 'outtime' not in data.columns:
                    # Use cached icustays if available, otherwise load
                    if self._icustays_cache is not None and all(c in self._icustays_cache.columns for c in [primary_id, 'intime', 'outtime', 'los']):
                        icu_df = self._icustays_cache.copy()
                    else:
                        icu_cols = [primary_id, 'intime', 'outtime', 'los']
                        icustays_table = data_source.load_table('icustays', columns=icu_cols, verbose=False)
                        icu_df = icustays_table.data if hasattr(icustays_table, 'data') else icustays_table
                        # Cache it
                        self._icustays_cache = icu_df.copy()
                    icu_df['intime'] = pd.to_datetime(icu_df['intime'], errors='coerce', utc=True).dt.tz_localize(None)
                    if 'outtime' in icu_df.columns:
                        icu_df['outtime'] = pd.to_datetime(icu_df['outtime'], errors='coerce', utc=True).dt.tz_localize(None)
                    # è‹¥outtimeç¼ºå¤±ï¼Œå°è¯•ç”¨losæ¨æ–­
                    if 'los' in icu_df.columns:
                        los_hours = pd.to_numeric(icu_df['los'], errors='coerce') * 24.0
                        icu_df['outtime_fallback'] = icu_df['intime'] + pd.to_timedelta(los_hours, unit='h')
                        if 'outtime' in icu_df.columns:
                            icu_df['outtime'] = icu_df['outtime'].fillna(icu_df['outtime_fallback'])
                        else:
                            icu_df['outtime'] = icu_df['outtime_fallback']
                    data = data.merge(icu_df[[primary_id] + [c for c in ['intime', 'outtime'] if c in icu_df.columns]], on=primary_id, how='left')

                # è®¡ç®—ICUçª—å£é•¿åº¦ï¼ˆå°æ—¶ï¼‰
                icu_len_hours = None
                if 'outtime' in data.columns and data['outtime'].notna().any():
                    icu_len = (pd.to_datetime(data['outtime']) - pd.to_datetime(data['intime']))
                    icu_len_hours = icu_len.dt.total_seconds() / 3600.0

                # ä¿®å¤ï¼šR ricuä¿ç•™æ‰€æœ‰æ•°æ®ï¼ŒåŒ…æ‹¬ï¼š
                # 1. å…¥ICUå‰çš„æ•°æ®ï¼ˆè´Ÿæ—¶é—´ï¼‰
                # 2. ICUä½é™¢æœŸé—´çš„æ•°æ®ï¼ˆ0åˆ°icu_len_hoursï¼‰
                # 3. å‡ºICUåçš„æ•°æ®ï¼ˆè¶…è¿‡icu_len_hoursï¼‰
                # ä¸è¿‡æ»¤ä»»ä½•æ—¶é—´æ•°æ®ï¼Œå®Œå…¨åŒ¹é…R ricuçš„è¡Œä¸º
                # æ³¨é‡Šæ‰æ—¶é—´è¿‡æ»¤ï¼Œä¿ç•™æ‰€æœ‰åŸå§‹æ•°æ®ç‚¹
                # if icu_len_hours is not None:
                #     mask = data[index_column] <= icu_len_hours
                #     data = data[mask].copy()
                # æ¸…ç†ä¸´æ—¶åˆ—
                drop_cols = [c for c in ['intime', 'outtime'] if c in data.columns]
                if drop_cols:
                    data = data.drop(columns=drop_cols)
            except Exception as _:
                # è¿‡æ»¤å¤±è´¥åˆ™åŸæ ·è¿”å›
                pass
            return data
        
        # æ£€æŸ¥æ—¶é—´åˆ—æ˜¯å¦æ˜¯æœ‰æ•ˆçš„datetimeç±»å‹
        if not pd.api.types.is_datetime64_any_dtype(data[index_column]):
            # å¦‚æœä¸æ˜¯datetimeä¹Ÿä¸æ˜¯numericï¼Œå°è¯•è½¬æ¢ä¸ºdatetime
            try:
                data[index_column] = pd.to_datetime(data[index_column], errors='coerce', utc=True).dt.tz_localize(None)
            except Exception as e:
                print(f"  âš ï¸  è­¦å‘Š: æ— æ³•å°†æ—¶é—´åˆ— {index_column} è½¬æ¢ä¸ºdatetime: {e}")
                return data
        
        try:
            # å¦‚æœå·²ç»æœ‰intimeåˆ—ï¼ˆä»å‰é¢çš„joinå¾—åˆ°ï¼‰ï¼Œç›´æ¥ä½¿ç”¨ï¼Œä¸éœ€è¦å†æ¬¡åŠ è½½
            if 'intime' not in data.columns:
                # Use cached icustays if available
                if self._icustays_cache is not None and all(c in self._icustays_cache.columns for c in [primary_id, 'intime', 'outtime', 'los']):
                    icustays_df = self._icustays_cache.copy()
                else:
                # Load icustays table to get admission times
                    icustays_table = data_source.load_table('icustays', columns=[primary_id, 'intime', 'outtime', 'los'], verbose=False)
                if hasattr(icustays_table, 'data'):
                    icustays_df = icustays_table.data
                else:
                    icustays_df = icustays_table
                    # Cache it
                    self._icustays_cache = icustays_df.copy()
                
                if 'intime' not in icustays_df.columns:
                    # No admission time available, return as-is
                    return data
                
                # Merge with admission times
                admission_times = icustays_df[[primary_id, 'intime', 'outtime', 'los'] if 'los' in icustays_df.columns else [primary_id, 'intime', 'outtime']].copy()
                # ç§»é™¤æ—¶åŒºä¿¡æ¯ä»¥é¿å…æ—¶åŒºä¸ä¸€è‡´é”™è¯¯
                admission_times['intime'] = pd.to_datetime(admission_times['intime'], errors='coerce', utc=True).dt.tz_localize(None)
                if 'outtime' in admission_times.columns:
                    admission_times['outtime'] = pd.to_datetime(admission_times['outtime'], errors='coerce', utc=True).dt.tz_localize(None)
                # å¦‚æœouttimeç¼ºå¤±ï¼Œä½¿ç”¨losæ¨æ–­
                if 'los' in admission_times.columns:
                    los_hours = pd.to_numeric(admission_times['los'], errors='coerce') * 24.0
                    admission_times['outtime_fallback'] = admission_times['intime'] + pd.to_timedelta(los_hours, unit='h')
                    if 'outtime' in admission_times.columns:
                        admission_times['outtime'] = admission_times['outtime'].fillna(admission_times['outtime_fallback'])
                    else:
                        admission_times['outtime'] = admission_times['outtime_fallback']
                    admission_times = admission_times.drop(columns=[c for c in ['los','outtime_fallback'] if c in admission_times.columns])
                
                # Merge with data
                data = data.merge(admission_times, on=primary_id, how='left')
            else:
                # ç¡®ä¿intimeæ˜¯tz-naive datetime
                if pd.api.types.is_datetime64_any_dtype(data['intime']):
                    if hasattr(data['intime'].dt, 'tz') and data['intime'].dt.tz is not None:
                        data['intime'] = data['intime'].dt.tz_localize(None)
            # è‹¥å­˜åœ¨outtimeï¼Œäº¦è§„èŒƒåŒ–
            if 'outtime' in data.columns and pd.api.types.is_datetime64_any_dtype(data['outtime']):
                if hasattr(data['outtime'].dt, 'tz') and data['outtime'].dt.tz is not None:
                    data['outtime'] = data['outtime'].dt.tz_localize(None)
            
            # ç¡®ä¿æ—¶é—´åˆ—æ˜¯datetimeç±»å‹ï¼ˆå¦‚æœä¸æ˜¯ï¼Œç§»é™¤æ—¶åŒºä¿¡æ¯ï¼‰
            if pd.api.types.is_datetime64_any_dtype(data[index_column]):
                if hasattr(data[index_column].dt, 'tz') and data[index_column].dt.tz is not None:
                    data[index_column] = data[index_column].dt.tz_localize(None)
            else:
                # å¦‚æœä»ç„¶ä¸æ˜¯datetimeï¼Œå°è¯•è½¬æ¢
                data[index_column] = pd.to_datetime(data[index_column], errors='coerce', utc=True).dt.tz_localize(None)
            
            # å…³é”®ä¿®å¤ï¼šR ricu ä¸è¿‡æ»¤è¶…å‡º ICU æ—¶é—´çª—å£çš„æ•°æ®
            # R ricu ä¿ç•™æ‰€æœ‰æ•°æ®ç‚¹ï¼ŒåŒ…æ‹¬ï¼š
            # 1. å…¥ ICU å‰çš„æ•°æ®ï¼ˆè´Ÿæ—¶é—´ï¼‰
            # 2. å‡º ICU åçš„æ•°æ®ï¼ˆè¶…è¿‡ outtimeï¼‰
            # è¿™æ˜¯å› ä¸ºä¸´åºŠæ•°æ®å¯èƒ½åœ¨ ICU å…¥ä½å‰åæµ‹é‡ï¼Œä½†ä»ç„¶ä¸ ICU ä½é™¢ç›¸å…³
            # ä¾‹å¦‚ï¼šå®éªŒå®¤æ£€éªŒã€ç”Ÿå‘½ä½“å¾ç­‰å¯èƒ½åœ¨å…¥ICUå‰æˆ–è½¬å‡ºåè®°å½•
            
            # Calculate hours since admission (ä¸è¿›è¡Œä»»ä½•æ—¶é—´çª—å£è¿‡æ»¤)
            time_diff = data[index_column] - data['intime']
            # Convert to hours (as float, matching ricu's behavior)
            hours = time_diff.dt.total_seconds() / 3600.0
            
            data[index_column] = hours
            
            # ğŸ”§ CRITICAL FIX: Also convert ALL other datetime columns to relative hours
            # This fixes the norepi_rate issue where starttime was float but endtime was datetime,
            # causing expand() to generate 30 million invalid rows
            # Common time columns that need conversion: endtime, stop_var, stoptime, etc.
            time_related_cols = [col for col in data.columns 
                                if col not in [index_column, 'intime', 'outtime', primary_id] 
                                and pd.api.types.is_datetime64_any_dtype(data[col])]
            
            for time_col in time_related_cols:
                # Remove timezone if present
                if hasattr(data[time_col].dt, 'tz') and data[time_col].dt.tz is not None:
                    data[time_col] = data[time_col].dt.tz_localize(None)
                # Convert to hours since admission
                time_diff_col = data[time_col] - data['intime']
                data[time_col] = time_diff_col.dt.total_seconds() / 3600.0
            
            # æ³¨æ„ï¼šä¸è¿‡æ»¤è´Ÿæ—¶é—´ï¼ˆå…¥ICUå‰ï¼‰æˆ–è¶…è¿‡outtimeçš„æ•°æ®ï¼ŒåŒ¹é… R ricu è¡Œä¸º
            
            # Drop the temporary alignment columns
            drop_cols = ['intime']
            if 'outtime' in data.columns:
                drop_cols.append('outtime')
            data = data.drop(columns=drop_cols)
            
        except Exception as e:
            # If alignment fails, return original data silently
            pass
        
        return data

    def _load_recursive_concept(
        self,
        concept_name: str,
        definition: ConceptDefinition,
        data_source: ICUDataSource,
        *,
        aggregator: object,
        patient_ids: Optional[Iterable[object]],
        verbose: bool = True,
        interval: Optional[pd.Timedelta] = None,
        align_to_admission: bool = True,
        **kwargs,  # Additional parameters for callbacks
    ) -> ICUTable:
        if not definition.callback:
            raise NotImplementedError(
                f"Recursive concept '{concept_name}' requires a callback."
            )

        # Check for database-specific sub_concepts override
        db_name = data_source.config.name if hasattr(data_source, 'config') and hasattr(data_source.config, 'name') else ''
        sub_names = list(definition.sub_concepts)

        # DEBUG: Print database detection info
        if verbose and logger.isEnabledFor(logging.DEBUG):
            logger.debug(f"ğŸ” Database-specific config check for concept '{concept_name}':")
            logger.debug(f"   db_name: '{db_name}'")
            logger.debug(f"   original sub_concepts: {sub_names}")
            logger.debug(f"   definition has sources: {hasattr(definition, 'sources')}")
            if hasattr(definition, 'sources'):
                logger.debug(f"   definition.sources: {definition.sources}")

        # Check if there's a database-specific configuration that overrides sub_concepts
        if db_name and hasattr(definition, 'sources') and db_name in definition.sources:
            db_sources = definition.sources[db_name]
            # db_sources is a list of ConceptSource objects, but concept-dict.
            if isinstance(db_sources, list):
                for db_source in db_sources:
                    if hasattr(db_source, '__dict__'):
                        # This is a ConceptSource object
                        db_source_dict = db_source.__dict__
                    else:
                        # This is already a dict
                        db_source_dict = db_source

                    if 'concepts' in db_source_dict:
                        # Use database-specific sub_concepts
                        sub_names = list(db_source_dict['concepts'])
                        if verbose and logger.isEnabledFor(logging.DEBUG):
                            logger.debug(f"ğŸ”„ Using {db_name}-specific sub_concepts for '{concept_name}': {sub_names}")
                        break
                    elif 'params' in db_source_dict and isinstance(db_source_dict['params'], dict) and 'concepts' in db_source_dict['params']:
                        # Use database-specific sub_concepts from params
                        sub_names = list(db_source_dict['params']['concepts'])
                        if verbose and logger.isEnabledFor(logging.DEBUG):
                            logger.debug(f"ğŸ”„ Using {db_name}-specific sub_concepts from params for '{concept_name}': {sub_names}")
                        break
            else:
                # db_sources is a dict (loaded from JSON)
                if 'concepts' in db_sources:
                    sub_names = list(db_sources['concepts'])
                    if verbose and logger.isEnabledFor(logging.DEBUG):
                        logger.debug(f"ğŸ”„ Using {db_name}-specific sub_concepts for '{concept_name}': {sub_names}")

        if not sub_names:
            raise ValueError(
                f"Recursive concept '{concept_name}' specifies no sub concepts."
            )

        agg_value = self._coerce_final_aggregator(aggregator)
        if agg_value in (None, "auto"):
            fallback_agg = definition.aggregate
            if fallback_agg is not None:
                agg_value = self._coerce_final_aggregator(fallback_agg)

        aggregate_mapping = self._build_sub_aggregate(definition.aggregate, sub_names)

        # Prepare kwargs for sub-concepts, allowing them to be optional
        sub_kwargs = {**kwargs, '_allow_missing_concept': True}
        
        # ğŸ”¥ CRITICAL: å†…éƒ¨é€’å½’è°ƒç”¨å¿…é¡»ä½¿ç”¨ ricu_compatible=False
        # å¦åˆ™ä¼šè¿”å› DataFrame è€Œä¸æ˜¯ Dict[str, ICUTable]ï¼Œå¯¼è‡´åç»­å¤„ç†å¤±è´¥
        sub_tables = self.load_concepts(
            sub_names,
            data_source,
            merge=False,
            aggregate=aggregate_mapping,
            patient_ids=patient_ids,
            verbose=verbose,
            interval=interval,  # Pass interval to recursive calls
            align_to_admission=align_to_admission,  # Pass align flag
            ricu_compatible=False,  # ğŸ”¥ å†…éƒ¨è°ƒç”¨å¿…é¡»è¿”å› Dict[str, ICUTable]
            concept_workers=1,  # ğŸ”§ å­æ¦‚å¿µé¡ºåºåŠ è½½ï¼Œé¿å…è¿‡åº¦å¹¶è¡Œå¯¼è‡´çº¿ç¨‹ç«äº‰
            **sub_kwargs,  # Pass kwargs with allow_missing flag
        )

        if isinstance(sub_tables, ICUTable):
            sub_tables = {sub_names[0]: sub_tables}

        # Standardize time column names for eICU BEFORE passing to callbacks
        # eICU uses different time column names (labresultoffset, observationoffset, etc.)
        # Rename them to a standard name 'charttime' to enable merging across concepts
        db_name = data_source.config.name if hasattr(data_source, 'config') and hasattr(data_source.config, 'name') else ''
        if db_name in ['eicu', 'eicu_demo']:
            # All possible eICU time offset columns
            eicu_time_cols = [
                'labresultoffset', 'observationoffset', 'nursecharting_offset', 
                'respiratorycharting_offset', 'intakeoutput_offset', 'respchartoffset',
                'infusionoffset', 'drugstartoffset', 'drugstopoffset', 'drugorderoffset',
                'culturetakenoffset', 'cultureoffset'  # æ·»åŠ å¾®ç”Ÿç‰©åŸ¹å…»æ—¶é—´åˆ—
            ]
            
            standardized_sub_tables = {}
            for name, table in sub_tables.items():
                if isinstance(table, ICUTable) and table.index_column:
                    # Check if this table uses an eICU-specific time column
                    if table.index_column in eicu_time_cols and table.index_column != 'charttime':
                        # Rename the column in the DataFrame
                        if table.index_column in table.data.columns:
                            renamed_data = table.data.rename(columns={table.index_column: 'charttime'})
                            # Create new ICUTable with updated index_column
                            table = ICUTable(
                                data=renamed_data,
                                id_columns=table.id_columns,
                                index_column='charttime',  # Update metadata
                                value_column=table.value_column,
                                unit_column=table.unit_column,
                                time_columns=table.time_columns,
                            )
                standardized_sub_tables[name] = table
            sub_tables = standardized_sub_tables

        # Align WinTbl time columns BEFORE passing to callbacks
        # This ensures _merge_tables can properly merge WinTbl concepts with numeric time columns
        if align_to_admission:
            from .table import WinTbl
            aligned_sub_tables = {}
            for name, table in sub_tables.items():
                if isinstance(table, WinTbl):
                    # WinTbl needs both index_var and dur_var aligned
                    idx_col = table.index_var
                    dur_col = table.dur_var
                    id_cols = table.id_vars
                    
                    if verbose and logger.isEnabledFor(logging.DEBUG):
                        logger.debug("   å¯¹é½ WinTbl '%s': index_var=%s, dur_var=%s", name, idx_col, dur_col)
                        if idx_col in table.data.columns:
                            logger.debug("      index_var ç±»å‹: %s", table.data[idx_col].dtype)
                        if dur_col and dur_col in table.data.columns:
                            logger.debug("      dur_var ç±»å‹: %s", table.data[dur_col].dtype)
                    
                    # Align index_var (start time) if it's datetime
                    if idx_col and idx_col in table.data.columns and pd.api.types.is_datetime64_any_dtype(table.data[idx_col]):
                        if verbose and logger.isEnabledFor(logging.DEBUG):
                            logger.debug("      âœ… è½¬æ¢ index_var ä» datetime åˆ°å°æ—¶")
                        table.data = self._align_time_to_admission(
                            table.data,
                            data_source,
                            id_cols,
                            idx_col
                        )
                    
                    # Convert dur_var (duration) from timedelta to hours
                    if dur_col and dur_col in table.data.columns:
                        if pd.api.types.is_timedelta64_dtype(table.data[dur_col]):
                            if verbose and logger.isEnabledFor(logging.DEBUG):
                                logger.debug("      âœ… è½¬æ¢ dur_var ä» timedelta åˆ°å°æ—¶")
                            table.data[dur_col] = table.data[dur_col].dt.total_seconds() / 3600.0
                        elif pd.api.types.is_datetime64_any_dtype(table.data[dur_col]):
                            # If dur_var is datetime (shouldn't happen), warn
                            logger.warning("âš ï¸  WinTbl '%s' çš„ dur_var '%s' æ˜¯ datetime ç±»å‹ï¼Œé¢„æœŸæ˜¯ timedelta", name, dur_col)
                
                aligned_sub_tables[name] = table
            sub_tables = aligned_sub_tables

        ctx = ConceptCallbackContext(
            concept_name=concept_name,
            target=definition.target,
            interval=definition.interval,
            resolver=self,
            data_source=data_source,
            patient_ids=patient_ids,
            kwargs=kwargs,  # Pass kwargs to callback context
        )

        # Check for database-specific callback override
        callback_name = definition.callback

        # DEBUG: Print callback detection info
        if verbose and logger.isEnabledFor(logging.DEBUG):
            logger.debug(f"ğŸ” Callback detection for concept '{concept_name}':")
            logger.debug(f"   original callback: '{callback_name}'")
            logger.debug(f"   db_name: '{db_name}'")
            logger.debug(f"   has sources: {hasattr(definition, 'sources')}")

        if db_name and hasattr(definition, 'sources') and db_name in definition.sources:
            db_sources = definition.sources[db_name]
            # db_sources is a list of ConceptSource objects, but concept-dict.
            if isinstance(db_sources, list):
                for db_source in db_sources:
                    if hasattr(db_source, '__dict__'):
                        # This is a ConceptSource object
                        db_source_dict = db_source.__dict__
                    else:
                        # This is already a dict
                        db_source_dict = db_source

                    if 'callback' in db_source_dict and db_source_dict['callback'] is not None:
                        # Use database-specific callback only if explicitly specified
                        callback_name = db_source_dict['callback']
                        if verbose and logger.isEnabledFor(logging.DEBUG):
                            logger.debug(f"ğŸ”„ Using {db_name}-specific callback '{callback_name}' for '{concept_name}'")
                        break
            else:
                # db_sources is a dict (loaded from JSON)
                if 'callback' in db_sources and db_sources['callback'] is not None:
                    callback_name = db_sources['callback']
                    if verbose and logger.isEnabledFor(logging.DEBUG):
                        logger.debug(f"ğŸ”„ Using {db_name}-specific callback '{callback_name}' for '{concept_name}'")

        # Validate callback_name before execution
        if callback_name is None:
            raise ValueError(f"Concept '{concept_name}' has no callback specified. Both original and database-specific callbacks are None.")

        if verbose and logger.isEnabledFor(logging.DEBUG):
            logger.debug(f"ğŸ¯ Executing callback '{callback_name}' for concept '{concept_name}' with {len(sub_tables)} sub-tables")

        result = execute_concept_callback(callback_name, sub_tables, ctx)

        # CRITICAL: Align WinTbl result time columns immediately after callback
        # This ensures that when this concept is used as a sub-concept in parent recursion,
        # it already has numeric time columns (not datetime)
        from .table import WinTbl
        if isinstance(result, WinTbl) and align_to_admission and not result.data.empty:
            idx_col = result.index_var
            dur_col = result.dur_var
            id_cols = result.id_vars
            
            # Align index_var if it's still datetime
            if idx_col and idx_col in result.data.columns and pd.api.types.is_datetime64_any_dtype(result.data[idx_col]):
                if verbose and logger.isEnabledFor(logging.DEBUG):
                    logger.debug(
                        "   å¯¹é½ WinTbl ç»“æœ '%s': index_var=%s (datetime â†’ å°æ—¶)",
                        concept_name,
                        idx_col,
                    )
                result.data = self._align_time_to_admission(
                    result.data,
                    data_source,
                    id_cols,
                    idx_col
                )
            
            # Convert dur_var from timedelta to hours
            if dur_col and dur_col in result.data.columns and pd.api.types.is_timedelta64_dtype(result.data[dur_col]):
                if verbose and logger.isEnabledFor(logging.DEBUG):
                    logger.debug(
                        "   è½¬æ¢ WinTbl ç»“æœ '%s': dur_var=%s (timedelta â†’ å°æ—¶)",
                        concept_name,
                        dur_col,
                    )
                result.data[dur_col] = result.data[dur_col].dt.total_seconds() / 3600.0

        # Rä»£ç ä¸­ï¼Œé€’å½’æ¦‚å¿µçš„å›è°ƒè¿”å›ç»“æœå°±æ˜¯æœ€ç»ˆç»“æœï¼Œä¸éœ€è¦å†æ¬¡èšåˆ
        # aggregateå‚æ•°å·²ç»åœ¨åŠ è½½å­æ¦‚å¿µæ—¶åº”ç”¨äº†
        # æˆ‘ä»¬åªéœ€è¦åº”ç”¨æ—¶é—´å¯¹é½å’Œintervalå¤„ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
        
        # Apply interval alignment and aggregation for recursive concepts
        # Handle both ICUTable and WinTbl
        if isinstance(result, WinTbl):
            idx_col = result.index_var
            dur_col = result.dur_var  # WinTbl è¿˜æœ‰ duration åˆ—
        else:
            idx_col = result.index_column
            dur_col = None
        
        if interval is not None and idx_col and idx_col in result.data.columns:
            from .ts_utils import change_interval
            
            # å…³é”®ä¿®å¤ï¼šå¦‚æœæ—¶é—´åˆ—æ˜¯datetimeç±»å‹ä½†åº”è¯¥æ˜¯numericï¼ˆalign_to_admission=Trueï¼‰ï¼Œ
            # å¼ºåˆ¶è½¬æ¢ä¸ºç›¸å¯¹å°æ—¶æ•°
            # å¯¹äº WinTblï¼Œéœ€è¦åŒæ—¶è½¬æ¢ index_var å’Œ dur_var
            if align_to_admission and not result.data.empty and idx_col in result.data.columns:
                if pd.api.types.is_datetime64_any_dtype(result.data[idx_col]):
                    # æ—¶é—´åˆ—æ˜¯datetimeï¼Œä½†åº”è¯¥æ˜¯numericï¼ˆç›¸å¯¹ICUå…¥é™¢æ—¶é—´çš„å°æ—¶æ•°ï¼‰
                    # è¿™å¯èƒ½æ˜¯å› ä¸ºcallbackå¤åˆ¶äº†æ•°æ®ä½†æ²¡æœ‰ä¿æŒç±»å‹è½¬æ¢
                    # å¼ºåˆ¶é‡æ–°å¯¹é½
                    if isinstance(result, WinTbl):
                        id_cols = result.id_vars
                    else:
                        id_cols = result.id_columns
                    
                    # å¯¹é½ index_varï¼ˆå¼€å§‹æ—¶é—´ï¼‰
                    result.data = self._align_time_to_admission(
                        result.data,
                        data_source,
                        id_cols,
                        idx_col
                    )
                    
                    # WinTbl ç‰¹æ®Šå¤„ç†ï¼šdur_varï¼ˆæŒç»­æ—¶é—´ï¼‰ä¹Ÿéœ€è¦è½¬æ¢
                    # æ³¨æ„ï¼šdur_var æ˜¯æ—¶é—´é—´éš”ï¼ˆå¦‚ timedeltaï¼‰ï¼Œéœ€è¦è½¬æ¢ä¸ºå°æ—¶æ•°
                    if dur_col and dur_col in result.data.columns:
                        if pd.api.types.is_timedelta64_dtype(result.data[dur_col]):
                            # timedelta è½¬æ¢ä¸ºå°æ—¶æ•°
                            result.data[dur_col] = result.data[dur_col].dt.total_seconds() / 3600.0
                        elif pd.api.types.is_datetime64_any_dtype(result.data[dur_col]):
                            # å¦‚æœæ˜¯ datetimeï¼ˆä¸åº”è¯¥ï¼Œä½†ä¿é™©èµ·è§ï¼‰ï¼Œè®°å½•è­¦å‘Š
                            print(f"   âš ï¸  è­¦å‘Š: WinTbl çš„ dur_var '{dur_col}' æ˜¯ datetime ç±»å‹ï¼Œé¢„æœŸæ˜¯ timedelta")
            
            # Align time to ICU admission if requested
            if align_to_admission and not result.data.empty:
                # Get id_columns based on result type
                if isinstance(result, WinTbl):
                    id_cols = result.id_vars
                else:
                    id_cols = result.id_columns
                
                # åªæœ‰åœ¨æ—¶é—´åˆ—ä¸æ˜¯numericç±»å‹æ—¶æ‰å¯¹é½ï¼ˆé¿å…é‡å¤å¯¹é½ï¼‰
                if not pd.api.types.is_numeric_dtype(result.data[idx_col]):
                    result.data = self._align_time_to_admission(
                        result.data,
                        data_source,
                        id_cols,
                        idx_col
                    )
                    
                    # WinTbl: åŒæ—¶è½¬æ¢ dur_var
                    if dur_col and dur_col in result.data.columns:
                        if pd.api.types.is_timedelta64_dtype(result.data[dur_col]):
                            result.data[dur_col] = result.data[dur_col].dt.total_seconds() / 3600.0
            
            # CRITICAL: Expand WinTbl to time series before applying interval aggregation
            # WinTbl represents time windows (start_time, duration) and must be expanded
            # to individual time points when interval is specified
            if isinstance(result, WinTbl) and not result.data.empty:
                idx_col = result.index_var
                dur_col = result.dur_var
                id_cols = result.id_vars
                
                if idx_col and dur_col and idx_col in result.data.columns and dur_col in result.data.columns:
                    if verbose:
                        if logger.isEnabledFor(logging.DEBUG):
                            logger.debug("   æ‰©å±• WinTbl '%s' åˆ°æ—¶é—´åºåˆ— (interval=%s)", concept_name, interval)
                    
                    # æ‰©å±•çª—å£åˆ°æ—¶é—´åºåˆ—
                    interval_hours = interval.total_seconds() / 3600.0
                    expanded_rows = []
                    for _, row in result.data.iterrows():
                        start_time = row[idx_col]
                        duration = row[dur_col]
                        
                        # FIX: å¯¹äº duration=0 çš„è¡Œï¼Œåªæ·»åŠ ä¸€ä¸ªæ—¶é—´ç‚¹ï¼ˆå¯¹é½åˆ° intervalï¼‰
                        if duration <= 0:
                            aligned_time = np.floor(start_time / interval_hours) * interval_hours
                            new_row = {idx_col: aligned_time}
                            # å¤åˆ¶ ID åˆ—
                            for col in id_cols:
                                if col in row.index:
                                    new_row[col] = row[col]
                            # å¤åˆ¶å€¼åˆ—ï¼ˆé™¤äº† dur_colï¼‰
                            for col in result.data.columns:
                                if col not in [idx_col, dur_col] and col not in id_cols:
                                    new_row[col] = row[col]
                            expanded_rows.append(new_row)
                            continue
                        
                        # è®¡ç®—ç»“æŸæ—¶é—´ï¼ˆå°æ—¶ï¼‰
                        # R ricu ä½¿ç”¨ seq(min, max, step) åŒ…å«ç»ˆç‚¹ï¼Œæ‰€ä»¥è¿™é‡Œç”¨ <=
                        end_time = start_time + duration
                        
                        # ç”Ÿæˆæ—¶é—´åºåˆ—ï¼ˆæ¯ä¸ª intervalï¼‰
                        current_time = np.floor(start_time / interval_hours) * interval_hours
                        
                        while current_time <= end_time:
                            new_row = {idx_col: current_time}
                            # å¤åˆ¶ ID åˆ—
                            for col in id_cols:
                                if col in row.index:
                                    new_row[col] = row[col]
                            # å¤åˆ¶å€¼åˆ—ï¼ˆé™¤äº† dur_colï¼‰
                            for col in result.data.columns:
                                if col not in [idx_col, dur_col] and col not in id_cols:
                                    new_row[col] = row[col]
                            expanded_rows.append(new_row)
                            current_time += interval_hours
                    
                    # è½¬æ¢ä¸º DataFrame
                    if expanded_rows:
                        expanded_df = pd.DataFrame(expanded_rows)
                        # è½¬æ¢ä¸º ICUTable
                        value_col = [c for c in expanded_df.columns if c not in id_cols and c != idx_col]
                        value_col = value_col[0] if value_col else None
                        result = ICUTable(
                            data=expanded_df,
                            id_columns=id_cols,
                            index_column=idx_col,
                            value_column=value_col,
                            unit_column=None,
                            time_columns=[],
                        )
                        if logger.isEnabledFor(logging.DEBUG):
                            logger.debug("   âœ… æ‰©å±•å®Œæˆ: %d è¡Œ", len(expanded_df))
                        elif verbose:
                            print(f"   âœ… æ‰©å±•å®Œæˆ: {len(expanded_df)} è¡Œ")
                    else:
                        # æ²¡æœ‰æ•°æ®ï¼Œè¿”å›ç©ºçš„ ICUTable
                        result = ICUTable(
                            data=pd.DataFrame(columns=[*id_cols, idx_col]),
                            id_columns=id_cols,
                            index_column=idx_col,
                            value_column=None,
                            unit_column=None,
                            time_columns=[],
                        )
            
            # Apply change_interval: round to interval and aggregate same-hour records
            # CRITICAL: For sofa_single type callbacks (sofa_coag, sofa_liver, sofa_cns),
            # the sub-concept already has the correct time points after interval alignment.
            # The callback just calculates a new column and removes the input column,
            # so time points should remain unchanged. However, ricu_code still applies
            # change_interval after callback, so we do the same for consistency.
            # But we should NOT re-aggregate if the result already has the correct interval.
            
            # ç¡®å®šèšåˆæ–¹æ³•ï¼šä½¿ç”¨ä¼ å…¥çš„aggregatoræˆ–definition.aggregate
            agg_method = agg_value if agg_value not in (None, False, "auto") else None
            if agg_method in (None, "auto"):
                agg_method = None
            # GCS total score should use 'min' aggregation (for recursive concepts)
            # But GCS sub-components should use default aggregation (median)
            if concept_name == 'gcs':
                if agg_method is None or (isinstance(agg_method, str) and agg_method != 'min'):
                    agg_method = 'min'
            if agg_method in (None, "auto") and concept_name in VASO_RATE_CONCEPTS:
                agg_method = "max"
            # SOFA cardiovascular components must retain the highest severity within the window.
            # Using the default 'median' aggregation diluted vasopressor-driven spikes (e.g. 2 and 4
            # becoming 3, or 1 and 2 becoming 1.5). ricu keeps the window maximum, so align here.
            sofa_max_concepts = {'sofa_cardio', 'sofa2_cardio'}
            if agg_method is None and concept_name in sofa_max_concepts:
                agg_method = 'max'
            # å¦‚æœä»ç„¶æ²¡æœ‰æŒ‡å®šï¼Œæ ¹æ®å€¼åˆ—ç±»å‹è‡ªåŠ¨é€‰æ‹©
            if agg_method is None:
                # Get value column based on result type
                if isinstance(result, WinTbl):
                    value_col = None  # WinTbl doesn't have a single value column
                else:
                    value_col = getattr(result, 'value_column', None)
                
                if value_col and value_col in result.data.columns:
                    if pd.api.types.is_numeric_dtype(result.data[value_col]):
                        agg_method = 'median'  # Changed from 'mean' to 'median' to match R ricu default
                    else:
                        agg_method = 'first'
                else:
                    # Default to 'first' if no value column found
                    agg_method = 'first'
            
            # åªæœ‰æŒ‡å®šäº†èšåˆæ–¹æ³•æ—¶æ‰åº”ç”¨change_interval
            # For sofa_single type, the time points should already be correct,
            # but we still apply change_interval to match ricu_code's behavior
            # Skip if result is still WinTbl (not expanded)
            has_time_column = getattr(result, 'index_column', None)
            if agg_method and has_time_column and has_time_column in result.data.columns and not result.data.empty and not isinstance(result, WinTbl):
                try:
                    fill_missing = self._should_fill_gaps(concept_name, definition)
                    fill_method = self._get_fill_method(concept_name, definition)
                    combined_result = change_interval(
                        result,
                        interval=interval,
                        aggregation=agg_method,
                        fill_gaps=fill_missing,
                        fill_method=fill_method,
                        copy=False
                    )
                    
                    # Extract data if ICUTable is returned
                    if hasattr(combined_result, 'data'):
                        result.data = combined_result.data
                    else:
                        result.data = combined_result
                except Exception as e:
                    # If change_interval fails, log but continue
                    if verbose:
                        print(f"  âš ï¸ è­¦å‘Š: {concept_name} çš„intervalå¤„ç†å¤±è´¥: {e}")

        # ğŸ”§ NOTE: ä¸è¿‡æ»¤è´Ÿæ—¶é—´ï¼ˆå…¥ICUå‰çš„æ•°æ®ï¼‰ï¼Œricu ä¿ç•™è¿™äº›æ•°æ®

        return result

    @staticmethod
    def _build_sub_aggregate(
        aggregate_spec: object,
        sub_names: List[str],
    ) -> Optional[Mapping[str, object]]:
        def normalise(value: object) -> object:
            if isinstance(value, (list, tuple)):
                if len(value) == 1:
                    return normalise(value[0])
                return [normalise(item) for item in value]
            return value

        if aggregate_spec is None:
            return None

        if isinstance(aggregate_spec, Mapping):
            return {name: normalise(aggregate_spec.get(name)) for name in sub_names}

        if isinstance(aggregate_spec, (list, tuple)):
            return {
                name: normalise(aggregate_spec[i])
                for i, name in enumerate(sub_names)
                if i < len(aggregate_spec)
            }

        return {name: normalise(aggregate_spec) for name in sub_names}

    @staticmethod
    def _coerce_final_aggregator(aggregator: object) -> object:
        if isinstance(aggregator, (list, tuple, dict)):
            return "auto"
        return aggregator

    def _load_fun_item(
        self,
        concept_name: str,
        definition: ConceptDefinition,
        source: ConceptSource,
        data_source: ICUDataSource,
        *,
        aggregator: object,
        patient_ids: Optional[Iterable[object]],
        **kwargs,  # Additional parameters (not used in fun_item but accepted for consistency)
    ) -> ICUTable:
        callback = (source.callback or "").strip()

        if callback == "los_callback":
            raw = self._load_fun_item_los(concept_name, source, data_source, patient_ids)
        elif "fwd_concept" in callback:
            raw = self._load_fun_item_forward(
                concept_name,
                source,
                data_source,
                patient_ids,
            )
        else:
            raise NotImplementedError(
                f"Function item callback '{callback}' is not yet supported."
            )

        agg_value = aggregator
        if agg_value in (None, "auto") and definition.aggregate is not None:
            agg_value = definition.aggregate

        agg_value = self._coerce_final_aggregator(agg_value)

        # WinTbl ä¸éœ€è¦èšåˆï¼Œç›´æ¥è¿”å›
        from .table import WinTbl
        if isinstance(raw, WinTbl):
            return raw

        if agg_value is not False:
            frame = self._apply_aggregation(
                raw.data,
                raw.value_column or concept_name,
                list(raw.id_columns),
                raw.index_column,
                raw.unit_column,
                agg_value,
            )
            raw = ICUTable(
                data=frame,
                id_columns=list(raw.id_columns),
                index_column=raw.index_column,
                value_column=raw.value_column or concept_name,
                unit_column=raw.unit_column,
                time_columns=list(raw.time_columns),
            )

        return raw

    def _load_fun_item_los(
        self,
        concept_name: str,
        source: ConceptSource,
        data_source: ICUDataSource,
        patient_ids: Optional[Iterable[object]],
    ) -> ICUTable:
        win_type = source.params.get("win_type")
        if not win_type:
            raise ValueError("los_callback requires 'win_type' parameter.")

        id_cfg = data_source.config.id_configs.get(win_type)
        if id_cfg is None or not id_cfg.table or not id_cfg.start or not id_cfg.end:
            raise ValueError(f"Identifier configuration for '{win_type}' is incomplete.")

        required_cols = [id_cfg.id, id_cfg.start, id_cfg.end]
        table = data_source.load_table(id_cfg.table, columns=required_cols)

        base_frame = table.data.copy()
        missing_required = [col for col in required_cols if col not in base_frame.columns]
        if missing_required:
            for column in missing_required:
                fallback = self._synthesise_los_column(column, data_source, base_frame)
                if fallback is None:
                    raise KeyError(
                        f"Required column '{column}' missing for LOS calculation in table '{id_cfg.table}'"
                    )
                base_frame[column] = fallback

        frame = base_frame[required_cols].copy()
        frame = frame.dropna(subset=[id_cfg.start, id_cfg.end])

        # Detect time format and database type
        start_col = frame[id_cfg.start]
        end_col = frame[id_cfg.end]
        is_numeric_time = pd.api.types.is_numeric_dtype(start_col)
        ds_name = (data_source.config.name or "").lower()
        
        # Determine time unit: eICU uses minutes, AUMC uses milliseconds
        is_eicu = ds_name.startswith("eicu")
        
        if is_numeric_time:
            start_val = pd.to_numeric(start_col, errors="coerce")
            end_val = pd.to_numeric(end_col, errors="coerce")
            valid_mask = start_val.notna() & end_val.notna() & (end_val >= start_val)
            frame = frame.loc[valid_mask].copy()
            if frame.empty:
                return ICUTable(
                    data=pd.DataFrame(columns=[id_cfg.id, concept_name]),
                    id_columns=[id_cfg.id],
                    index_column=None,
                    value_column=concept_name,
                )
            
            if is_eicu:
                # eICU: times are relative MINUTES from ICU admission
                los_days = (end_val.loc[valid_mask] - start_val.loc[valid_mask]) / (60 * 24)
                duration_hours = (end_val.loc[valid_mask] - start_val.loc[valid_mask]) / 60
                start_hours = start_val.loc[valid_mask] / 60
            else:
                # AUMC/HiRID: times are relative MILLISECONDS from admission
                los_days = (end_val.loc[valid_mask] - start_val.loc[valid_mask]) / (1000 * 60 * 60 * 24)
                duration_hours = (end_val.loc[valid_mask] - start_val.loc[valid_mask]) / (1000 * 60 * 60)
                start_hours = start_val.loc[valid_mask] / (1000 * 60 * 60)
            
            frame[concept_name] = los_days
        else:
            # MIIV/eICU: times are datetime objects
            start_time = pd.to_datetime(start_col, errors="coerce")
            end_time = pd.to_datetime(end_col, errors="coerce")
            valid_mask = start_time.notna() & end_time.notna() & (end_time >= start_time)
            frame = frame.loc[valid_mask].copy()
            if frame.empty:
                return ICUTable(
                    data=pd.DataFrame(columns=[id_cfg.id, concept_name]),
                    id_columns=[id_cfg.id],
                    index_column=None,
                    value_column=concept_name,
                )
            frame[concept_name] = (end_time.loc[valid_mask] - start_time.loc[valid_mask]).dt.total_seconds() / 86400.0
            duration_hours = (end_time.loc[valid_mask] - start_time.loc[valid_mask]).dt.total_seconds() / 3600.0
            start_hours = None  # Will use datetime-based approach

        frame = frame[frame[concept_name] >= 0]
        if frame.empty:
            return ICUTable(
                data=pd.DataFrame(columns=[id_cfg.id, concept_name]),
                id_columns=[id_cfg.id],
                index_column=None,
                value_column=concept_name,
            )

        if patient_ids is not None:
            if isinstance(patient_ids, dict):
                candidates = patient_ids.get(id_cfg.id) or patient_ids.get(str(id_cfg.id)) or []
            else:
                candidates = patient_ids
            if candidates:
                mask = frame[id_cfg.id].isin(set(candidates))
                frame = frame[mask]
                if is_numeric_time:
                    duration_hours = duration_hours.loc[frame.index]
                    start_hours = start_hours.loc[frame.index]
                else:
                    duration_hours = duration_hours.loc[frame.index]

        if frame.empty:
            return ICUTable(
                data=pd.DataFrame(columns=[id_cfg.id, concept_name]),
                id_columns=[id_cfg.id],
                index_column=None,
                value_column=concept_name,
            )

        # Generate hourly time grid
        rows: List[dict] = []
        for idx, row in frame.iterrows():
            stay_id = row[id_cfg.id]
            los_val = row[concept_name]
            dur_h = duration_hours.loc[idx] if hasattr(duration_hours, 'loc') else duration_hours[idx]
            
            if pd.isna(dur_h) or dur_h < 0:
                continue
            
            if is_numeric_time:
                # AUMC: use relative hours directly
                st_h = start_hours.loc[idx] if hasattr(start_hours, 'loc') else start_hours[idx]
                # Generate hourly grid from (start - 1) to end
                start_hour = int(st_h) - 1
                end_hour = int(st_h + dur_h) + 1
                for hour in range(start_hour, end_hour):
                    rows.append({
                        id_cfg.id: stay_id,
                        "index_var": float(hour),
                        concept_name: los_val,
                    })
            else:
                # MIIV/eICU: use datetime and convert later
                start_dt = start_time.loc[idx]
                end_dt = end_time.loc[idx]
                current_time = start_dt - pd.Timedelta(hours=1)
                while current_time < end_dt:
                    rows.append({
                        id_cfg.id: stay_id,
                        "index_var": current_time,
                        concept_name: los_val,
                    })
                    current_time += pd.Timedelta(hours=1)

        if not rows:
            return ICUTable(
                data=pd.DataFrame(columns=[id_cfg.id, concept_name]),
                id_columns=[id_cfg.id],
                index_column=None,
                value_column=concept_name,
            )

        ts_df = pd.DataFrame(rows)
        # Note: For los_icu, index_var is already in hours relative to ICU admission,
        # so we skip _align_time_to_admission which would incorrectly divide by 60 again for eICU
        ts_df["index_var"] = pd.to_numeric(ts_df["index_var"], errors="coerce")
        ts_df = ts_df.dropna(subset=["index_var"]).reset_index(drop=True)
        return ICUTable(
            data=ts_df,
            id_columns=[id_cfg.id],
            index_column="index_var",
            value_column=concept_name,
        )

    def _synthesise_los_column(
        self,
        column_name: str,
        data_source: ICUDataSource,
        frame: pd.DataFrame,
    ) -> Optional[pd.Series]:
        ds_name = (data_source.config.name or "").lower()
        if column_name == "unitadmitoffset" and ds_name.startswith("eicu"):
            logger.warning(
                "Column '%s' missing for %s; assuming zero-minute ICU admission offsets.",
                column_name,
                data_source.config.name,
            )
            return pd.Series(0, index=frame.index, dtype="float64")
        return None

    def _load_fun_item_forward(
        self,
        concept_name: str,
        source: ConceptSource,
        data_source: ICUDataSource,
        patient_ids: Optional[Iterable[object]],
    ) -> ICUTable:
        callback = source.callback or ""
        match = re.search(r"fwd_concept\('(.+?)'\)", callback)
        if not match:
            raise ValueError("fwd_concept callback is missing concept name.")

        base_name = match.group(1)
        # ğŸ”§ FIX: ç¦ç”¨ ricu_compatible æ¨¡å¼ï¼Œç¡®ä¿è¿”å› dict[str, ICUTable]
        base_tables = self.load_concepts(
            [base_name],
            data_source,
            merge=False,
            aggregate=None,
            patient_ids=patient_ids,
            ricu_compatible=False,  # ç¡®ä¿è¿”å›åŸå§‹ ICUTable æ ¼å¼
        )
        if isinstance(base_tables, ICUTable):
            base_table = base_tables
        elif isinstance(base_tables, dict):
            base_table = base_tables[base_name]
        else:
            # å¦‚æœè¿”å›çš„æ˜¯ DataFrameï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼Œä½†åšé˜²å¾¡æ€§å¤„ç†ï¼‰
            raise TypeError(
                f"Expected ICUTable or dict, got {type(base_tables).__name__} "
                f"when loading '{base_name}' for fwd_concept in '{concept_name}'"
            )

        data = base_table.data.copy()
        value_col = base_table.value_column or base_name

        comp_match = re.search(r"comp_na\(`(.+?)`,\s*(.+?)\)", callback, flags=re.DOTALL)
        if comp_match:
            op_symbol = comp_match.group(1)
            literal = _parse_literal(comp_match.group(2))
            series = data[value_col]
            if op_symbol == "<=":
                # comp_na: NA -> False, å¦åˆ™æ ¹æ®æ¯”è¾ƒç»“æœ
                numeric_series = pd.to_numeric(series, errors="coerce")
                mask = (~numeric_series.isna()) & (numeric_series <= literal)
            elif op_symbol == "==":
                mask = (~series.isna()) & (series.astype(str) == str(literal))
            elif op_symbol == "!=":
                mask = (~series.isna()) & (series.astype(str) != str(literal))
            else:
                raise NotImplementedError(f"Unsupported comparison operator '{op_symbol}'")
        else:
            mask = pd.Series(True, index=data.index)

        if "ts_to_win_tbl" in callback:
            # å¦‚æœ base_table ä¸ºç©ºæˆ–æ²¡æœ‰ index_columnï¼Œè¿”å›ç©ºçš„ WinTbl
            if base_table.index_column is None or base_table.data.empty:
                # ä½¿ç”¨ base_table çš„ ID åˆ—ï¼ˆä¼˜å…ˆï¼‰ï¼Œå¦åˆ™ä½¿ç”¨æ•°æ®åº“ç‰¹å®šçš„é»˜è®¤å€¼
                # WinTbl å·²åœ¨æ¨¡å—é¡¶éƒ¨å¯¼å…¥ï¼Œä¸éœ€è¦é‡å¤å¯¼å…¥
                
                # ç¡®å®šæ•°æ®åº“ç‰¹å®šçš„é»˜è®¤ ID åˆ—
                db_name = data_source.config.name if hasattr(data_source, 'config') and hasattr(data_source.config, 'name') else 'unknown'
                default_id_cols = _default_id_columns_for_db(db_name)
                
                if isinstance(base_table, WinTbl):
                    id_cols = list(base_table.id_vars) if base_table.id_vars else default_id_cols
                else:
                    id_cols = list(base_table.id_columns) if base_table.id_columns else default_id_cols
                idx_col = base_table.index_column if base_table.index_column else 'charttime'  # é»˜è®¤æ—¶é—´åˆ—
                # åˆ›å»ºç©º DataFrame å¹¶è®¾ç½®æ­£ç¡®çš„ dtype
                empty_win_df = pd.DataFrame(columns=id_cols + [idx_col, concept_name + "_dur", concept_name])
                # è®¾ç½® index åˆ—ä¸º datetime ç±»å‹ï¼ˆå³ä½¿ä¸ºç©ºï¼‰
                empty_win_df[idx_col] = pd.to_datetime(empty_win_df[idx_col])
                # dur_var åº”è¯¥æ˜¯ floatï¼ˆå°æ—¶ï¼‰ï¼Œè€Œä¸æ˜¯ timedelta
                empty_win_df[concept_name + "_dur"] = empty_win_df[concept_name + "_dur"].astype(float)
                empty_win_df[concept_name] = empty_win_df[concept_name].astype(bool)
                return WinTbl(
                    data=empty_win_df,
                    id_vars=id_cols,
                    index_var=idx_col,
                    dur_var=concept_name + "_dur",
                )
            # ä½¿ç”¨éè´ªå©ªåŒ¹é…ï¼Œå¹¶æ”¯æŒåµŒå¥—æ‹¬å·ï¼ˆå¦‚ mins(360L)ï¼‰
            dur_match = re.search(r"ts_to_win_tbl\(([^)]+\))\)", callback, flags=re.DOTALL)
            if not dur_match:
                # å¤‡ç”¨ï¼šç®€å•åŒ¹é…
                dur_match = re.search(r"ts_to_win_tbl\((.+?)\)", callback, flags=re.DOTALL)
            duration = self._parse_interval_expression(dur_match.group(1).strip() if dur_match else "mins(60)")
            # å°† timedelta è½¬æ¢ä¸ºå°æ—¶ï¼ˆfloatï¼‰
            if isinstance(duration, pd.Timedelta):
                duration_hours = duration.total_seconds() / 3600.0
            else:
                duration_hours = float(duration)
            
            # FIX: ä¸ºæ‰€æœ‰è¡Œåˆ›å»º WinTblï¼ŒTrue è¡Œæœ‰çª—å£æŒç»­æ—¶é—´ï¼ŒFalse è¡ŒæŒç»­æ—¶é—´ä¸º 0
            # è¿™æ ·åœ¨ downsampling æ—¶ï¼ŒTrue çš„çª—å£ä¼šæ‰©å±•ï¼ŒFalse çš„åªä¿ç•™åŸå§‹æ—¶é—´ç‚¹
            win_df = data[list(base_table.id_columns) + [base_table.index_column]].copy()
            # True è¡Œä½¿ç”¨å®Œæ•´çª—å£æŒç»­æ—¶é—´ï¼ŒFalse è¡Œä½¿ç”¨ 0ï¼ˆåªè¡¨ç¤ºè¯¥æ—¶é—´ç‚¹å­˜åœ¨ï¼‰
            win_df["duration"] = np.where(mask.values, duration_hours, 0.0)
            win_df[concept_name] = mask.values
            return WinTbl(
                data=win_df.rename(columns={"duration": concept_name + "_dur"}),
                id_vars=list(base_table.id_columns),
                index_var=base_table.index_column,
                dur_var=concept_name + "_dur",
            )

        cols = list(base_table.id_columns)
        if base_table.index_column:
            cols.append(base_table.index_column)
        cols.append(value_col)
        result = data[cols].rename(columns={value_col: concept_name})

        return ICUTable(
            data=result.reset_index(drop=True),
            id_columns=list(base_table.id_columns),
            index_column=base_table.index_column,
            value_column=concept_name,
        )

    @staticmethod
    def _parse_interval_expression(expression: str) -> pd.Timedelta:
        expr = expression.strip()
        match = re.fullmatch(r"([a-zA-Z]+)\((.+)\)", expr)
        if not match:
            raise ValueError(f"Unsupported interval expression '{expression}'")

        unit = match.group(1).lower()
        raw_value = match.group(2).strip()
        
        # ç§»é™¤ R è¯­è¨€çš„æ•´æ•°åç¼€ 'L'ï¼ˆä¾‹å¦‚ 360L -> 360ï¼‰
        # æ³¨æ„ï¼šR çš„ 'L' åªæ˜¯è¡¨ç¤ºæ•´æ•°ï¼Œä¸æ˜¯æ—¶é—´å•ä½
        if raw_value.endswith('L'):
            raw_value = raw_value[:-1]
        
        value = _parse_literal(raw_value)
        if isinstance(value, pd.Timedelta):
            return value
        # å¦‚æœvalueæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æä¸ºæ•°å€¼
        if isinstance(value, str):
            # ç§»é™¤å¯èƒ½çš„å°¾éšå­—ç¬¦ï¼ˆå¦‚æ‹¬å·ï¼‰
            value = value.strip().rstrip(')')
            # å†æ¬¡æ£€æŸ¥ L åç¼€
            if value.endswith('L'):
                value = value[:-1]
            try:
                value = float(value)
            except ValueError:
                raise ValueError(f"Cannot parse interval value '{value}' in expression '{expression}'")
        # valueæ˜¯æ•°å€¼ï¼Œéœ€è¦åŠ ä¸Šunit
        if unit in {"min", "mins", "minute", "minutes"}:
            return pd.to_timedelta(value, unit="m")
        if unit in {"hour", "hours"}:
            return pd.to_timedelta(value, unit="h")
        if unit in {"sec", "secs", "second", "seconds"}:
            return pd.to_timedelta(value, unit="s")
        if unit in {"day", "days"}:
            return pd.to_timedelta(value, unit="d")
        raise ValueError(f"Unsupported interval unit '{unit}' in expression '{expression}'")

    def _merge_tables(self, tables: Mapping[str, ICUTable]) -> pd.DataFrame:
        from .table import WinTbl
        merged: Optional[pd.DataFrame] = None
        index_column: Optional[str] = None
        id_columns: Optional[List[str]] = None

        for name, table in tables.items():
            frame = table.data.copy()

            # Handle both ICUTable and WinTbl
            if isinstance(table, WinTbl):
                id_columns = id_columns or list(table.id_vars)
                index_column = index_column or table.index_var
                expected_id = id_columns or []
                if list(table.id_vars) != expected_id:
                    raise ValueError(
                        "All concepts must share identical identifier columns to merge"
                    )
                if table.index_var != index_column:
                    raise ValueError(
                        "All concepts must share identical index column to merge"
                    )
            else:
                id_columns = id_columns or list(table.id_columns)
                index_column = index_column or table.index_column
                expected_id = id_columns or []
                if list(table.id_columns) != expected_id:
                    raise ValueError(
                        "All concepts must share identical identifier columns to merge"
                    )
                if table.index_column != index_column:
                    raise ValueError(
                        "All concepts must share identical index column to merge"
                    )

            key_cols = expected_id + ([index_column] if index_column else [])
            
            # ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„åˆ—éƒ½å­˜åœ¨
            missing_key_cols = [col for col in key_cols if col not in frame.columns]
            if missing_key_cols:
                # å¦‚æœç¼ºå°‘å…³é”®åˆ—ï¼Œè·³è¿‡è¿™ä¸ªè¡¨
                print(f"âš ï¸  è­¦å‘Š: è¡¨ '{name}' ç¼ºå°‘å…³é”®åˆ— {missing_key_cols}ï¼Œè·³è¿‡åˆå¹¶")
                continue
            
            if name not in frame.columns:
                # å¦‚æœæ¦‚å¿µå€¼åˆ—ä¸å­˜åœ¨ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–å€¼åˆ—
                # è¿™ç§æƒ…å†µå¯èƒ½å‘ç”Ÿåœ¨keep_components=Trueæ—¶ï¼Œå›è°ƒè¿”å›äº†ç»„ä»¶åˆ—è€Œä¸æ˜¯æ¦‚å¿µåç§°åˆ—
                value_cols = [col for col in frame.columns if col not in key_cols]
                if not value_cols:
                    print(f"âš ï¸  è­¦å‘Š: è¡¨ '{name}' æ²¡æœ‰å€¼åˆ—ï¼Œè·³è¿‡åˆå¹¶")
                    continue
            
            # é€‰æ‹©è¦ä¿ç•™çš„åˆ—ï¼šIDåˆ— + æ—¶é—´åˆ— + æ‰€æœ‰éå…³é”®åˆ—ï¼ˆåŒ…æ‹¬æ¦‚å¿µå€¼åˆ—å’Œç»„ä»¶åˆ—ï¼‰
            # ä¿ç•™æ‰€æœ‰å€¼åˆ—ï¼Œä¸ä»…ä»…æ˜¯æ¦‚å¿µåç§°åˆ—
            # è¿™å¯¹äº keep_components=True çš„æƒ…å†µå¾ˆé‡è¦ï¼ˆå¦‚ SOFA ç»„ä»¶ï¼‰
            # ä½†æ˜¯è¦æ’é™¤å•ä½åˆ—ï¼ˆvalueuomï¼‰ï¼Œå› ä¸ºå®ƒä¼šå¯¼è‡´åˆå¹¶æ—¶çš„åˆ—å†²çª
            # å•ä½åˆ—é€šå¸¸ä¸éœ€è¦ä¿ç•™ï¼Œå› ä¸ºå€¼å·²ç»æ ‡å‡†åŒ–äº†
            excluded_cols = ['valueuom', 'unit']  # æ’é™¤è¿™äº›åˆ—ä»¥é¿å…åˆå¹¶å†²çª
            
            # å¤„ç† MultiIndex åˆ—ï¼ˆå½“ aggregate=['min', 'max'] æ—¶äº§ç”Ÿï¼‰
            if isinstance(frame.columns, pd.MultiIndex):
                # MultiIndex åˆ—ï¼šä¿ç•™æ‰€æœ‰éå…³é”®åˆ—
                # key_cols æ˜¯ç®€å•å­—ç¬¦ä¸²ï¼Œéœ€è¦åŒ¹é… MultiIndex çš„ç¬¬ä¸€å±‚
                key_cols_set = set(key_cols)
                excluded_set = set(excluded_cols)
                
                # é€‰æ‹©åˆ—ï¼šä¿ç•™ç¬¬ä¸€å±‚ä¸åœ¨ key_cols å’Œ excluded_cols ä¸­çš„åˆ—
                cols_to_keep = [col for col in frame.columns 
                               if col[0] not in key_cols_set and col[0] not in excluded_set]
                # æ·»åŠ  key_colsï¼ˆå®ƒä»¬æ˜¯ç®€å•åˆ—ï¼Œä¸æ˜¯ MultiIndexï¼‰
                # å…ˆå±•å¹³ MultiIndexï¼Œç„¶åé€‰æ‹©
                frame = frame.copy()
                # é‡ç½®åˆ—ï¼šå°† MultiIndex å±•å¹³ä¸ºå•å±‚ï¼ˆå¦‚ ('pafi', 'min') -> 'pafi_min'ï¼‰
                if frame.columns.nlevels == 2:
                    frame.columns = ['_'.join(col).strip('_') if col[1] else col[0] 
                                    for col in frame.columns.values]
                # ç°åœ¨ key_cols å’Œ value_cols éƒ½æ˜¯ç®€å•å­—ç¬¦ä¸²
                excluded_cols_flat = excluded_cols
                value_cols = [col for col in frame.columns 
                             if col not in key_cols and col not in excluded_cols_flat]
                cols_to_keep = key_cols + value_cols
                frame = frame[cols_to_keep].copy()
            else:
                # æ™®é€šåˆ—ï¼šåŸæœ‰é€»è¾‘
                value_cols = [col for col in frame.columns 
                             if col not in key_cols and col not in excluded_cols]
                cols_to_keep = key_cols + value_cols
                frame = frame[cols_to_keep].copy()
            
            # åœ¨è®¾ç½®ç´¢å¼•å‰æ’åºï¼Œé¿å…"left keys must be sorted"é”™è¯¯
            frame = frame.sort_values(key_cols)
            
            # è®¾ç½®ç´¢å¼•ç”¨äºåˆå¹¶
            frame = frame.set_index(key_cols)

            if merged is None:
                merged = frame
            else:
                # ç¡®ä¿ç´¢å¼•å±‚çº§ä¸€è‡´
                if merged.index.nlevels != frame.index.nlevels:
                    print(f"âš ï¸  è­¦å‘Š: ç´¢å¼•å±‚çº§ä¸ä¸€è‡´ ({merged.index.nlevels} vs {frame.index.nlevels})ï¼Œé‡ç½®ç´¢å¼•åé‡æ–°åˆå¹¶")
                    # é‡ç½®ä¸ºå…±åŒçš„ç´¢å¼•åˆ—
                    common_keys = [col for col in merged.index.names if col in frame.index.names]
                    merged = merged.reset_index()
                    frame = frame.reset_index()
                    # æ£€æµ‹åˆ—é‡å ï¼Œä½¿ç”¨suffixesé¿å…å†²çª
                    overlapping_cols = set(merged.columns) & set(frame.columns) - set(common_keys)
                    if overlapping_cols:
                        merged = merged.merge(frame, on=common_keys, how='outer', suffixes=('', '_dup'))
                        # åˆ é™¤é‡å¤åˆ—
                        merged = merged[[c for c in merged.columns if not c.endswith('_dup')]]
                    else:
                        merged = merged.merge(frame, on=common_keys, how='outer')
                    merged = merged.sort_values(common_keys)
                    merged = merged.set_index(common_keys)
                else:
                    merged = merged.join(frame, how="outer", rsuffix='_dup')
                    # åˆ é™¤joinäº§ç”Ÿçš„é‡å¤åˆ—
                    merged = merged[[c for c in merged.columns if not c.endswith('_dup')]]

        if merged is None:
            return pd.DataFrame()

        merged = merged.reset_index()
        return merged

    def _build_cache_key(
        self,
        concept_name: str,
        data_source: ICUDataSource,
        patient_ids: Optional[Iterable[object]],
        interval: Optional[pd.Timedelta],
        align_to_admission: bool,
        aggregator: object,
        kwargs: Dict[str, object],
    ) -> str:
        """Build a cache key for a concept based on all relevant parameters."""
        import hashlib
        import json
        
        # Create a dictionary of all parameters that affect the result
        cache_params = {
            "concept_name": concept_name,
            "database": data_source.config.name if hasattr(data_source.config, 'name') else str(data_source.config),
            "patient_ids": sorted(list(patient_ids)) if patient_ids else None,
            "interval": str(interval) if interval else None,
            "align_to_admission": align_to_admission,
            "aggregator": str(aggregator),
            "kwargs": {k: str(v) for k, v in kwargs.items()},
            "dictionary_signature": self.dictionary_signature,
            "schema_version": self.cache_schema_version,
        }
        
        # Serialize and hash the parameters
        serialized = json.dumps(cache_params, sort_keys=True, default=str)
        return hashlib.sha1(serialized.encode("utf-8")).hexdigest()

    def _load_from_disk_cache(
        self,
        concept_name: str,
        data_source: ICUDataSource,
        cache_key: str,
    ) -> Optional[ICUTable]:
        """Load a concept from disk cache if available."""
        if self.cache_dir is None:
            return None
            
        try:
            import pickle
            from pathlib import Path
            
            # Create cache file path
            cache_file = Path(self.cache_dir) / f"{cache_key}.pkl"
            if not cache_file.exists():
                return None
                
            # Load from cache
            with open(cache_file, "rb") as f:
                cached_data = pickle.load(f)
                
            # Verify the cached data is an ICUTable
            if isinstance(cached_data, ICUTable):
                return cached_data
                
        except Exception:
            # If anything goes wrong, silently return None to force recomputation
            pass
            
        return None

    def _store_in_disk_cache(
        self,
        concept_name: str,
        data_source: ICUDataSource,
        cache_key: str,
        result: ICUTable,
    ) -> None:
        """Store a concept result in disk cache."""
        if self.cache_dir is None:
            return
            
        try:
            import pickle
            from pathlib import Path
            
            # Ensure cache directory exists
            Path(self.cache_dir).mkdir(parents=True, exist_ok=True)
            
            # Create cache file path
            cache_file = Path(self.cache_dir) / f"{cache_key}.pkl"
            
            # Store in cache
            with open(cache_file, "wb") as f:
                pickle.dump(result, f)
                
        except Exception:
            # If anything goes wrong, silently continue without caching
            pass

    def _expand_dependencies(self, requested: List[str]) -> List[str]:
        """Return dependency-closed list of concept names."""
        ordered: List[str] = []
        seen: set[str] = set()

        def visit(name: str) -> None:
            if name in seen:
                return
            if name not in self.dictionary:
                raise KeyError(f"Concept '{name}' not present in dictionary")
            seen.add(name)
            definition = self.dictionary[name]
            for dep in definition.depends_on:
                visit(dep)
            ordered.append(name)

        for concept in requested:
            visit(concept)
        return ordered

    def _ensure_concept_loaded(
        self,
        concept_name: str,
        data_source: ICUDataSource,
        aggregators: Dict[str, object],
        patient_ids: Optional[Iterable[object]],
        verbose: bool,
        interval: pd.Timedelta,
        align_to_admission: bool,
        kwargs: Dict[str, object],
        _skip_concept_cache: bool = False,  # ğŸ”§ è·³è¿‡æ¦‚å¿µç¼“å­˜
    ) -> ICUTable:
        # ğŸš€ ä¼˜åŒ–ï¼šå¢å¼ºæ¦‚å¿µæ•°æ®ç¼“å­˜ï¼ˆé¿å…é‡å¤åŠ è½½ç›¸åŒæ¦‚å¿µï¼Œå¦‚urineã€vaso_indã€pafiï¼‰
        patient_ids_hash = hash(frozenset(patient_ids)) if patient_ids else None
        agg_value = aggregators.get(concept_name, "auto")
        if agg_value in (None, "auto"):
            definition = self.dictionary.get(concept_name)
            if definition and definition.aggregate is not None:
                agg_value = definition.aggregate
        
        # ğŸ”¥ å…³é”®ä¼˜åŒ–: æ‰©å±•ç¼“å­˜é”®åŒ…å«kwargsä¸­çš„å…³é”®å‚æ•°ï¼Œç¡®ä¿ä¸åŒé…ç½®ä¸ä¼šæ··æ·†
        # ä½†å¯¹äºå­æ¦‚å¿µï¼ˆå¦‚vaso_indï¼‰ï¼Œkwargsé€šå¸¸ç›¸åŒï¼Œæ‰€ä»¥å¯ä»¥å®‰å…¨ç¼“å­˜
        # ğŸ”§ ä¿®å¤: å¯¹ä¸å¯å“ˆå¸Œçš„å€¼ï¼ˆå¦‚listï¼‰è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        def _hashable_kwargs_items(kw):
            for k, v in sorted(kw.items()):
                try:
                    hash(v)
                    yield (k, v)
                except TypeError:
                    yield (k, str(v))
        kwargs_hash = hash(frozenset(_hashable_kwargs_items(kwargs))) if kwargs else 0
        concept_cache_key = (concept_name, patient_ids_hash, str(interval), str(agg_value), kwargs_hash)
        
        # ğŸ”§ å¦‚æœ _skip_concept_cache=Trueï¼Œè·³è¿‡æ‰€æœ‰ç¼“å­˜æ£€æŸ¥å’Œç¼“å­˜å†™å…¥
        # è¿™ç”¨äºå›è°ƒå†…éƒ¨åŠ è½½æ¦‚å¿µï¼Œé¿å…æ±¡æŸ“ä¸»ç¼“å­˜
        if not _skip_concept_cache:
            with self._cache_lock:
                # æ£€æŸ¥å¢å¼ºçš„æ¦‚å¿µæ•°æ®ç¼“å­˜
                if concept_cache_key in self._concept_data_cache:
                    if verbose and logger.isEnabledFor(logging.DEBUG):
                        logger.debug("âœ¨ ä»å†…å­˜ç¼“å­˜åŠ è½½æ¦‚å¿µ '%s' (å‘½ä¸­å¢å¼ºç¼“å­˜)", concept_name)
                    return self._concept_data_cache[concept_cache_key]
                
                # å›é€€æ£€æŸ¥æ—§çš„ç®€å•ç¼“å­˜ï¼ˆç”¨äºå‘åå…¼å®¹ï¼‰
                simple_key = (concept_name, patient_ids_hash, str(interval), str(agg_value))
                if simple_key in self._concept_data_cache:
                    if verbose and logger.isEnabledFor(logging.DEBUG):
                        logger.debug("âœ¨ ä»å†…å­˜ç¼“å­˜åŠ è½½æ¦‚å¿µ '%s' (å‘½ä¸­ç®€å•ç¼“å­˜)", concept_name)
                    result = self._concept_data_cache[simple_key]
                    # åŒæ­¥åˆ°æ–°çš„ç¼“å­˜é”®
                    self._concept_data_cache[concept_cache_key] = result
                    return result
                
                # æ£€æŸ¥æ—§çš„æ¦‚å¿µç¼“å­˜
                cached = self._concept_cache.get(concept_name)
                if cached is not None:
                    # åŒæ—¶æ›´æ–°åˆ°æ–°ç¼“å­˜
                    self._concept_data_cache[concept_cache_key] = cached
                    return cached
                # çº¿ç¨‹å®‰å…¨çš„å¾ªç¯ä¾èµ–æ£€æµ‹
                inflight = self._get_inflight()
                if concept_name in inflight:
                    raise RuntimeError(f"Circular dependency detected for concept '{concept_name}'")
                inflight.add(concept_name)
        else:
            # è·³è¿‡ç¼“å­˜æ¨¡å¼ä¹Ÿéœ€è¦è®¾ç½® inflight ä»¥æ£€æµ‹å¾ªç¯ä¾èµ–
            with self._cache_lock:
                inflight = self._get_inflight()
                if concept_name in inflight:
                    raise RuntimeError(f"Circular dependency detected for concept '{concept_name}'")
                inflight.add(concept_name)

        definition = self.dictionary[concept_name]
        for dependency in definition.depends_on:
            self._ensure_concept_loaded(
                dependency,
                data_source,
                aggregators,
                patient_ids,
                verbose,
                interval,
                align_to_admission,
                kwargs,
                _skip_concept_cache=_skip_concept_cache,  # ä¼ é€’è·³è¿‡ç¼“å­˜æ ‡å¿—
            )

        cache_key = self._build_cache_key(
            concept_name,
            data_source,
            patient_ids,
            interval,
            align_to_admission,
            agg_value,
            kwargs,
        )

        # ğŸ”§ å¦‚æœ _skip_concept_cache=Trueï¼Œè·³è¿‡ç£ç›˜ç¼“å­˜
        if not _skip_concept_cache:
            disk_hit = self._load_from_disk_cache(concept_name, data_source, cache_key)
            if disk_hit is not None:
                with self._cache_lock:
                    self._concept_cache[concept_name] = disk_hit
                    self._concept_data_cache[concept_cache_key] = disk_hit  # ğŸš€ ä¹Ÿå­˜å…¥æ–°ç¼“å­˜
                    self._get_inflight().discard(concept_name)
                return disk_hit

        try:
            result = self._load_single_concept(
                concept_name,
                data_source,
                aggregator=agg_value,
                patient_ids=patient_ids,
                verbose=verbose,
                interval=interval,
                align_to_admission=align_to_admission,
                **kwargs,
            )
            
            # CRITICAL: Expand WinTbl to time series if interval is specified
            # This must happen after loading but before caching, so all concepts
            # (including those without sub_concepts) get expanded
            from .table import WinTbl
            if isinstance(result, WinTbl) and interval is not None and not result.data.empty:
                idx_col = result.index_var
                dur_col = result.dur_var
                id_cols = result.id_vars
                
                if idx_col and dur_col and idx_col in result.data.columns and dur_col in result.data.columns:
                    if verbose:
                        logger.info("   æ‰©å±• WinTbl '%s' åˆ°æ—¶é—´åºåˆ— (interval=%s)", concept_name, interval)
                    
                    # æ‰©å±•çª—å£åˆ°æ—¶é—´åºåˆ—
                    expanded_rows = []
                    for _, row in result.data.iterrows():
                        start_time = row[idx_col]
                        duration = row[dur_col]
                        
                        # è®¡ç®—ç»“æŸæ—¶é—´ï¼ˆå°æ—¶ï¼‰
                        # R ricu ä½¿ç”¨ seq(min, max, step) åŒ…å«ç»ˆç‚¹ï¼Œæ‰€ä»¥è¿™é‡Œç”¨ <=
                        end_time = start_time + duration
                        
                        # ç”Ÿæˆæ—¶é—´åºåˆ—ï¼ˆæ¯ä¸ª intervalï¼‰
                        interval_hours = interval.total_seconds() / 3600.0
                        current_time = np.floor(start_time / interval_hours) * interval_hours
                        
                        while current_time <= end_time:
                            new_row = {idx_col: current_time}
                            # å¤åˆ¶ ID åˆ—
                            for col in id_cols:
                                if col in row.index:
                                    new_row[col] = row[col]
                            # å¤åˆ¶å€¼åˆ—ï¼ˆé™¤äº† dur_colï¼‰
                            for col in result.data.columns:
                                if col not in [idx_col, dur_col] and col not in id_cols:
                                    new_row[col] = row[col]
                            expanded_rows.append(new_row)
                            current_time += interval_hours
                    
                    # è½¬æ¢ä¸º DataFrame
                    if expanded_rows:
                        expanded_df = pd.DataFrame(expanded_rows)
                        # è½¬æ¢ä¸º ICUTable
                        value_col = [c for c in expanded_df.columns if c not in id_cols and c != idx_col]
                        value_col = value_col[0] if value_col else None
                        from .table import ICUTable
                        result = ICUTable(
                            data=expanded_df,
                            id_columns=id_cols,
                            index_column=idx_col,
                            value_column=value_col,
                            unit_column=None,
                            time_columns=[],
                        )
                        if verbose:
                            logger.info("   âœ… æ‰©å±•å®Œæˆ: %d è¡Œ", len(expanded_df))
                        
        except Exception:
            with self._cache_lock:
                self._get_inflight().discard(concept_name)
            raise

        # ğŸ”§ å¦‚æœ _skip_concept_cache=Trueï¼Œè·³è¿‡ç¼“å­˜å†™å…¥
        if not _skip_concept_cache:
            self._store_in_disk_cache(concept_name, data_source, cache_key, result)

            with self._cache_lock:
                self._concept_cache[concept_name] = result
                self._concept_data_cache[concept_cache_key] = result  # ğŸš€ å­˜å…¥æ–°ç¼“å­˜
                self._get_inflight().discard(concept_name)
        else:
            # ä»…æ¸…é™¤ inflight æ ‡è®°
            with self._cache_lock:
                self._get_inflight().discard(concept_name)
        return result

    def _apply_aggregation(
        self,
        frame: pd.DataFrame,
        concept_name: str,
        id_columns: List[str],
        index_column: Optional[str],
        unit_column: Optional[str],
        aggregator: object,
    ) -> pd.DataFrame:
        # ğŸš€ CRITICAL FIX for norepi_rate: WinTbl expand before aggregation
        # R ricu does: aggregate(expand(...)), but pyricu was skipping expand
        # 
        # Check if this is WinTbl-style data (has endtime/duration)
        # and needs to be expanded before aggregation
        has_endtime = 'endtime' in frame.columns
        has_duration = 'duration' in frame.columns
        
        if (has_endtime or has_duration) and index_column and aggregator not in (None, False):
            # This is WinTbl data that needs expansion to time series
            from .ts_utils import expand
            
            # Determine end column
            end_col = 'duration' if has_duration else 'endtime'
            
            # Determine step size (default 1 hour for ICU data)
            step_size = pd.Timedelta(hours=1)
            
            # Determine columns to keep (value columns + unit if present)
            keep_vars = [concept_name] if concept_name in frame.columns else []
            if unit_column and unit_column in frame.columns:
                keep_vars.append(unit_column)
            
            # Additional value columns (not ID, not time, not end, not unit)
            excluded = set(id_columns + [index_column, end_col])
            if unit_column:
                excluded.add(unit_column)
            value_cols = [col for col in frame.columns 
                         if col not in excluded and col != concept_name]
            keep_vars.extend(value_cols)
            
            # Expand windows to hourly time series
            try:
                frame = expand(
                    frame,
                    start_var=index_column,
                    end_var=end_col,
                    step_size=step_size,
                    id_cols=id_columns,
                    keep_vars=keep_vars,
                )
                # After expand, index_column becomes the time column (no more endtime/duration)
            except Exception as e:
                import logging
                logger = logging.getLogger(__name__)
                logger.warning(f"Failed to expand WinTbl data for {concept_name}: {e}")
                # Continue without expansion
        
        key_cols = [col for col in id_columns if col]
        if index_column:
            key_cols.append(index_column)

        if not key_cols:
            return frame

        # Check if concept_name column exists, if not try to find it
        if concept_name not in frame.columns:
            # Try to find the value column - could be from callback result
            value_cols = [col for col in frame.columns if col not in key_cols and col != unit_column]
            if value_cols:
                concept_name = value_cols[0]  # Use first non-key column as concept value
        
        if concept_name not in frame.columns:
            # Still not found, return frame as-is
            return frame
        
        agg_value = self._resolve_aggregator(frame[concept_name], aggregator)
        agg_spec: MutableMapping[str, object] = {concept_name: agg_value}

        if unit_column and unit_column in frame.columns:
            agg_spec[unit_column] = "first"

        grouped = frame.groupby(key_cols, dropna=False, as_index=False)
        aggregated = grouped.agg(agg_spec)
        
        # Flatten MultiIndex columns if any (from multiple aggregation functions)
        if isinstance(aggregated.columns, pd.MultiIndex):
            # Flatten: keep last level if it's meaningful, otherwise join
            new_columns = []
            for col in aggregated.columns:
                if isinstance(col, tuple):
                    # Join tuple elements, skipping empty strings
                    parts = [str(c) for c in col if c and str(c).strip()]
                    new_col = '_'.join(parts) if parts else concept_name
                    new_columns.append(new_col)
                else:
                    new_columns.append(str(col))
            aggregated.columns = new_columns
            # If concept_name is not in columns, try to find it
            if concept_name not in aggregated.columns:
                # Look for column that contains concept_name
                for col in aggregated.columns:
                    if concept_name.lower() in col.lower():
                        aggregated = aggregated.rename(columns={col: concept_name})
                        break

        ordered_cols = key_cols + [concept_name]
        if unit_column and unit_column in aggregated.columns:
            ordered_cols.append(unit_column)
        
        # Ensure all ordered_cols exist
        ordered_cols = [col for col in ordered_cols if col in aggregated.columns]

        return aggregated.loc[:, ordered_cols]

    @staticmethod
    def _resolve_aggregator(series: pd.Series, aggregator: object) -> object:
        if aggregator in (None, "auto"):
            return _default_aggregator_for_dtype(series)
        return aggregator

    @staticmethod
    def _normalise_aggregators(
        aggregate: Optional[Union[str, bool, Mapping[str, object]]],
        names: List[str],
    ) -> Dict[str, object]:
        if aggregate is None:
            return {name: "auto" for name in names}

        if not isinstance(aggregate, Mapping):
            return {name: aggregate for name in names}

        result: Dict[str, object] = {}
        for name in names:
            result[name] = aggregate.get(name, aggregate.get("*", "auto"))
        return result

    def _to_ricu_format(self, icu_table: ICUTable, concept_name: str, interval_hours: float = 1.0) -> pd.DataFrame:
        """
        å°†ICUTableè½¬æ¢ä¸ºricu.Rå…¼å®¹çš„æ ¼å¼

        Args:
            icu_table: ICUTableå¯¹è±¡
            concept_name: æ¦‚å¿µåç§°
            interval_hours: æ—¶é—´é—´éš”ï¼ˆå°æ—¶ï¼‰ï¼Œç”¨äºçª—å£å±•å¼€

        Returns:
            ricu.Ræ ¼å¼çš„DataFrameï¼ˆåªåŒ…å«IDåˆ—ã€charttimeå’Œæ¦‚å¿µå€¼åˆ—ï¼Œé™æ€æ•°æ®åªåŒ…å«IDåˆ—å’Œæ¦‚å¿µå€¼åˆ—ï¼‰
        """
        frame = icu_table.data.copy()

        # è¯†åˆ«é™æ€æ•°æ®ï¼ˆæ— æ—¶é—´åˆ—çš„æ¦‚å¿µï¼‰
        is_static_data = (
            icu_table.index_column is None or
            icu_table.index_column not in frame.columns or
            concept_name in ['age', 'sex', 'height', 'weight', 'bmi']  # å¼ºåˆ¶å°†è¿™äº›è¯†åˆ«ä¸ºé™æ€æ•°æ®
        )

        if is_static_data:
            # é™æ€æ•°æ®ï¼ˆå¦‚age, sexï¼‰: è¿”å›IDåˆ—å’Œæ¦‚å¿µå€¼åˆ—
            if len(frame) == 0:
                return pd.DataFrame(columns=[concept_name])

            # æ„å»ºç»“æœåˆ—ï¼šIDåˆ— + æ¦‚å¿µå€¼åˆ—
            result_cols = []
            
            # æ·»åŠ IDåˆ—
            for id_col in icu_table.id_columns:
                if id_col in frame.columns:
                    result_cols.append(id_col)
            
            # æ·»åŠ æ¦‚å¿µå€¼åˆ—
            if concept_name in frame.columns:
                result_cols.append(concept_name)
            elif icu_table.value_column and icu_table.value_column in frame.columns:
                # é‡å‘½åå€¼åˆ—ä¸ºæ¦‚å¿µå
                frame = frame.rename(columns={icu_table.value_column: concept_name})
                result_cols.append(concept_name)
            
            if not result_cols:
                return pd.DataFrame(columns=[concept_name])
            
            # è¿”å›æ‰€æœ‰è¡Œï¼ˆricuæ ¼å¼ä¿ç•™æ‰€æœ‰åŒ¹é…çš„è¡Œï¼‰
            return frame[result_cols].copy()
        else:
            # æ—¶é—´åºåˆ—æ•°æ®: åªè¿”å›charttimeå’Œæ¦‚å¿µå€¼åˆ—
            time_col = icu_table.index_column

            # å¦‚æœæ²¡æœ‰index_columnï¼Œå°è¯•è¯†åˆ«æ—¶é—´åˆ—
            if time_col is None:
                possible_time_cols = [col for col in frame.columns if any(time_key in col.lower() for time_key in ['charttime', 'time', 'timestamp', 'measuredat', 'observationoffset'])]
                if possible_time_cols:
                    time_col = possible_time_cols[0]

            # å¦‚æœä»ç„¶æ²¡æœ‰æ—¶é—´åˆ—ï¼Œä½†æœ‰æ•°æ®ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤æ—¶é—´åˆ—
            if time_col is None and len(frame) > 0:
                frame = frame.copy()
                frame['charttime'] = range(len(frame))
                time_col = 'charttime'

            if time_col is None:
                # å¦‚æœçœŸçš„æ²¡æœ‰æ—¶é—´åˆ—ï¼Œè¿”å›åªæœ‰æ¦‚å¿µå€¼çš„æ•°æ®æ¡†
                value_cols = [col for col in frame.columns if col not in icu_table.id_columns]
                if concept_name in value_cols:
                    return frame[[concept_name]]
                elif value_cols:
                    return frame[value_cols[0]].to_frame()
                else:
                    return pd.DataFrame(columns=[concept_name])

            value_cols = [col for col in frame.columns if col not in icu_table.id_columns + [time_col]]

            # æ„å»ºricu.Ræ ¼å¼ - ğŸ”§ FIX: ä¹Ÿéœ€è¦åŒ…å«IDåˆ—ï¼Œå¦åˆ™æ— æ³•åˆå¹¶
            # ricu.R çš„æ—¶é—´åºåˆ—æ ¼å¼: IDåˆ— + æ—¶é—´åˆ— + å€¼åˆ—
            result_cols = []
            
            # æ·»åŠ IDåˆ—
            id_col_name = None
            for id_col in icu_table.id_columns:
                if id_col in frame.columns:
                    result_cols.append(id_col)
                    id_col_name = id_col
            
            # æ·»åŠ æ—¶é—´åˆ—
            result_cols.append(time_col)

            # æ·»åŠ æ¦‚å¿µå€¼åˆ—ï¼ˆä¼˜å…ˆä½¿ç”¨concept_nameï¼Œå¦åˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªå€¼åˆ—ï¼‰
            value_col_name = None
            if concept_name in value_cols:
                result_cols.append(concept_name)
                value_col_name = concept_name
            elif value_cols:
                result_cols.append(value_cols[0])
                value_col_name = value_cols[0]

            # ç¡®ä¿åªè¿”å›éœ€è¦çš„åˆ—
            available_cols = [col for col in result_cols if col in frame.columns]
            result = frame[available_cols].copy()

            # ğŸ”§ FIX: çª—å£æ¦‚å¿µå±•å¼€
            # æ£€æŸ¥æ˜¯å¦æ˜¯çª—å£æ¦‚å¿µï¼ˆå¦‚ mech_vent, vent_ind, supp_o2 ç­‰ï¼‰
            if concept_name in ricu_compat.WINDOW_CONCEPTS or concept_name.endswith('_rate'):
                # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æŸæ—¶é—´æˆ–æŒç»­æ—¶é—´åˆ—
                endtime_col = None
                duration_col = None
                
                # æŸ¥æ‰¾ç»“æŸæ—¶é—´åˆ—
                for candidate in ['endtime', 'stop', 'end_time', 'end']:
                    if candidate in frame.columns:
                        endtime_col = candidate
                        break
                
                # æŸ¥æ‰¾æŒç»­æ—¶é—´åˆ—
                for candidate in ['duration', 'dur', 'durationhours']:
                    if candidate in frame.columns:
                        duration_col = candidate
                        break
                
                # å¦‚æœæœ‰ç»“æŸæ—¶é—´æˆ–æŒç»­æ—¶é—´ï¼Œè¿›è¡Œå±•å¼€
                if endtime_col is not None or duration_col is not None:
                    # å‡†å¤‡å±•å¼€æ•°æ®
                    expand_df = result.copy()
                    
                    # æ·»åŠ ç»“æŸæ—¶é—´åˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    if endtime_col is not None and endtime_col not in expand_df.columns:
                        expand_df[endtime_col] = frame[endtime_col]
                    
                    # æ·»åŠ æŒç»­æ—¶é—´åˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    if duration_col is not None and duration_col not in expand_df.columns:
                        expand_df[duration_col] = frame[duration_col]
                    
                    # æ ‡å‡†åŒ–åˆ—åç”¨äºå±•å¼€å‡½æ•°
                    rename_map = {}
                    if id_col_name and id_col_name != 'id':
                        rename_map[id_col_name] = 'id'
                    if time_col != 'time':
                        rename_map[time_col] = 'time'
                    if value_col_name and value_col_name != concept_name:
                        rename_map[value_col_name] = concept_name
                    
                    if rename_map:
                        expand_df = expand_df.rename(columns=rename_map)
                    
                    # è°ƒç”¨çª—å£å±•å¼€å‡½æ•°
                    expanded = ricu_compat.expand_interval_rows(
                        expand_df,
                        concept_name,
                        id_col='id',
                        time_col='time',
                        value_col=concept_name,
                        endtime_col=endtime_col if endtime_col else 'endtime',
                        duration_col=duration_col if duration_col else 'duration',
                        interval_hours=interval_hours,
                    )
                    
                    # æ¢å¤åŸå§‹åˆ—å
                    reverse_map = {v: k for k, v in rename_map.items()}
                    if reverse_map:
                        expanded = expanded.rename(columns=reverse_map)
                    
                    return expanded

            # å¯¹äºAUMCç­‰æ•°æ®åº“ï¼Œä¿æŒåŸå§‹æ—¶é—´åˆ—åç§°ä»¥æ”¯æŒricu.Rå…¼å®¹æ€§
            # ä¸å¼ºåˆ¶é‡å‘½åä¸ºcharttimeï¼Œè®©éªŒè¯å·¥å…·è¯†åˆ«åŸå§‹åˆ—å

            return result

    def _to_ricu_format_merged(self, merged_df: pd.DataFrame, concept_names: List[str]) -> pd.DataFrame:
        """
        å°†åˆå¹¶åçš„DataFrameè½¬æ¢ä¸ºricu.Rå…¼å®¹çš„æ ¼å¼

        Args:
            merged_df: åˆå¹¶åçš„DataFrame
            concept_names: æ¦‚å¿µåç§°åˆ—è¡¨

        Returns:
            ricu.Ræ ¼å¼çš„DataFrame
        """
        frame = merged_df.reset_index()

        # è¯†åˆ«æ—¶é—´åˆ—å’ŒIDåˆ— - åŒ…å«æ‰€æœ‰å¯èƒ½çš„æ—¶é—´åˆ—åç§°
        time_cols = [col for col in frame.columns if any(time_key in col.lower() for time_key in ['charttime', 'time', 'timestamp', 'measuredat', 'observationoffset', 'labresultoffset'])]
        id_cols = [col for col in frame.columns if any(id_key in col.lower() for id_key in ['id', 'stay_id', 'subject_id', 'patient'])]

        # é€‰æ‹©ricu.Réœ€è¦çš„åˆ—
        result_cols = []

        # æ·»åŠ æ—¶é—´åˆ—
        if time_cols:
            result_cols.append(time_cols[0])  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ—¶é—´åˆ—

        # æ·»åŠ æ¦‚å¿µå€¼åˆ—
        for concept_name in concept_names:
            if concept_name in frame.columns:
                result_cols.append(concept_name)

        # è¿‡æ»¤å¹¶é‡å‘½å
        if result_cols:
            result = frame[result_cols].copy()
            # é‡å‘½åæ—¶é—´åˆ—ä¸ºcharttime
            if time_cols:
                result = result.rename(columns={time_cols[0]: 'charttime'})
            return result
        else:
            return frame

    def _to_ricu_format_merged_enhanced(
        self, 
        tables: Mapping[str, ICUTable], 
        concept_names: List[str],
        interval: Optional[pd.Timedelta] = None,
    ) -> pd.DataFrame:
        """
        å°†å¤šä¸ªæ¦‚å¿µè¡¨ä»¥ricué£æ ¼åˆå¹¶ï¼Œå®ç°å®Œæ•´çš„æ—¶é—´ç½‘æ ¼å¯¹é½å’Œçª—å£å±•å¼€
        
        è¿™æ˜¯å¢å¼ºç‰ˆæœ¬ï¼Œç›´æ¥åœ¨åŸå§‹tablesä¸Šæ“ä½œï¼Œå®ç°ï¼š
        1. çª—å£å‹æ¦‚å¿µçš„æ—¶é—´å±•å¼€ï¼ˆmech_vent, *_rateç­‰ï¼‰
        2. ç»Ÿä¸€æ—¶é—´ç½‘æ ¼æ„å»º
        3. æ‰€æœ‰æ¦‚å¿µå¯¹é½åˆ°ç½‘æ ¼
        4. é™æ€æ¦‚å¿µå¡«å……
        
        Args:
            tables: æ¦‚å¿µåç§°åˆ°ICUTableçš„æ˜ å°„
            concept_names: æ¦‚å¿µåç§°åˆ—è¡¨ï¼ˆä¿æŒé¡ºåºï¼‰
            interval: æ—¶é—´é—´éš”ï¼Œé»˜è®¤1å°æ—¶
            
        Returns:
            ricué£æ ¼çš„å®½æ ¼å¼DataFrame
        """
        interval_hours = 1.0
        if interval is not None:
            if hasattr(interval, 'total_seconds'):
                interval_hours = interval.total_seconds() / 3600.0
            elif isinstance(interval, (int, float)):
                interval_hours = float(interval)
            else:
                interval_hours = 1.0
        
        # å°†ICUTableè½¬æ¢ä¸ºDataFrameå­—å…¸
        concept_data: Dict[str, pd.DataFrame] = {}
        for name, table in tables.items():
            if isinstance(table, ICUTable):
                df = table.data.copy()
                # é‡å‘½åå€¼åˆ—ä¸ºæ¦‚å¿µå
                if name not in df.columns:
                    # æŸ¥æ‰¾å¯èƒ½çš„å€¼åˆ—
                    value_candidates = ['value', 'valuenum', table.index_column] if hasattr(table, 'index_column') else ['value', 'valuenum']
                    for cand in value_candidates:
                        if cand in df.columns and cand != name:
                            df = df.rename(columns={cand: name})
                            break
                concept_data[name] = df
            elif isinstance(table, pd.DataFrame):
                df = table.copy()
                if name not in df.columns:
                    for cand in ['value', 'valuenum']:
                        if cand in df.columns and cand != name:
                            df = df.rename(columns={cand: name})
                            break
                concept_data[name] = df
        
        if not concept_data:
            return pd.DataFrame()
        
        # æ£€æµ‹IDåˆ—å’Œæ—¶é—´åˆ—
        id_col = None
        time_col = None
        for df in concept_data.values():
            if df is None or df.empty:
                continue
            # æ£€æµ‹IDåˆ—
            for cand in ['stay_id', 'subject_id', 'patientunitstayid', 'admissionid', 'patientid']:
                if cand in df.columns:
                    id_col = cand
                    break
            # æ£€æµ‹æ—¶é—´åˆ— - FIX: æ·»åŠ  eICU çš„æ—¶é—´åˆ—å’ŒåŒºé—´æ ¼å¼çš„ start åˆ—
            for cand in ['charttime', 'time', 'starttime', 'start', 'index_var', 'measuredat',
                         'nursingchartoffset', 'labresultoffset', 'observationoffset',
                         'respchartoffset', 'intakeoutputoffset', 'infusionoffset']:
                if cand in df.columns:
                    time_col = cand
                    break
            if id_col and time_col:
                break
        
        if not id_col:
            id_col = 'stay_id'  # é»˜è®¤å€¼
        if not time_col:
            time_col = 'charttime'  # é»˜è®¤å€¼
        
        # ä½¿ç”¨ricu_compatæ¨¡å—è¿›è¡Œåˆå¹¶
        result = ricu_compat.merge_concepts_ricu_style(
            concept_data,
            id_col=id_col,
            time_col=time_col,
            interval_hours=interval_hours,
        )
        
        # ç¡®ä¿æ¦‚å¿µåˆ—æŒ‰è¯·æ±‚çš„é¡ºåºæ’åˆ—
        final_cols = [id_col, time_col]
        for name in concept_names:
            if name in result.columns:
                final_cols.append(name)
        
        # æ·»åŠ ä»»ä½•å…¶ä»–å€¼åˆ—ï¼ˆå¯èƒ½æ˜¯å­ç»„ä»¶ï¼‰
        for col in result.columns:
            if col not in final_cols:
                final_cols.append(col)
        
        final_cols = [c for c in final_cols if c in result.columns]
        result = result[final_cols]
        
        return result

def _apply_callback(
    frame: pd.DataFrame,
    source: ConceptSource,
    concept_name: str,
    unit_column: Optional[str] = None,
    resolver: Optional['ConceptResolver'] = None,
    patient_ids: Optional[List] = None,
    data_source: Optional['ICUDataSource'] = None,
) -> pd.DataFrame:
    callback = source.callback
    if not callback:
        return frame

    expr = callback.strip()

    if expr == "identity_callback":
        return frame

    if expr == "aumc_death":
        # R ricu logic: is_true(index_var - val_var < hours(72L))
        def _pick(col: Optional[str], fallbacks: List[str]) -> Optional[str]:
            ordered = [col] if col else []
            ordered.extend(fallbacks)
            for candidate in ordered:
                if candidate and candidate in frame.columns:
                    return candidate
            return None

        index_col = _pick(source.index_var, ["dateofdeath", "deathdate", "dod", "death_time"])
        value_col = _pick(source.value_var, [concept_name, "dischargedat", "dischargetime", "dischargeat"])

        if index_col is None or value_col is None:
            return frame

        df = frame.copy()
        death_ts = pd.to_datetime(df[index_col], errors="coerce")
        discharge_ts = pd.to_datetime(df[value_col], errors="coerce")
        delta = death_ts - discharge_ts
        within_window = delta < pd.Timedelta(hours=72)
        within_window = within_window & death_ts.notna() & discharge_ts.notna()
        df[value_col] = within_window.astype(int)
        return df

    # Handle eicu_age - process eICU age data (convert '> 89' to 90)
    if re.fullmatch(r"transform_fun\(eicu_age\)", expr):
        from .callback_utils import eicu_age
        return eicu_age(frame, val_col=concept_name)

    # Handle percent_as_numeric - remove '%' and convert to numeric
    if re.fullmatch(r"transform_fun\(percent_as_numeric\)", expr):
        series = frame[concept_name].copy()

        def parse_percent(val):
            if pd.isna(val):
                return np.nan
            if isinstance(val, str):
                val_clean = val.strip().rstrip('%')
                try:
                    return float(val_clean)
                except (ValueError, AttributeError):
                    return np.nan
            try:
                return float(val)
            except (TypeError, ValueError):
                return np.nan

        def missing_mask(values: pd.Series) -> pd.Series:
            mask = values.isna()
            as_str = values.astype(str).str.strip().str.lower()
            mask |= as_str.eq("") | as_str.eq("nan") | as_str.eq("none")
            return mask

        mask = missing_mask(series)
        for fallback_col in ("value", "valuetext"):
            if fallback_col in frame.columns and fallback_col != concept_name:
                fallback_series = frame[fallback_col]
                series = series.where(~mask, fallback_series)
                mask = missing_mask(series)

        frame.loc[:, concept_name] = series.apply(parse_percent)
        return frame

    match = re.fullmatch(r"transform_fun\(set_val\((.+)\)\)", expr, flags=re.DOTALL)
    if match:
        value = _parse_literal(match.group(1))
        frame = frame.copy()
        if concept_name in frame.columns:
            frame.drop(columns=[concept_name], inplace=True)
        dtype = "boolean" if isinstance(value, bool) else None
        result_series = pd.Series([value] * len(frame), index=frame.index, dtype=dtype)
        frame[concept_name] = result_series
        return frame

    # Handle comp_na() without arguments - check if value is not NA
    if re.fullmatch(r"transform_fun\(comp_na\(\)\)", expr):
        series = frame[concept_name]
        # Convert to boolean: True if not NA, False if NA
        frame.loc[:, concept_name] = series.notna().astype(float)
        return frame

    match = re.fullmatch(r"transform_fun\(comp_na\(`(.+?)`,\s*(.+)\)\)", expr, flags=re.DOTALL)
    if match:
        op_token = match.group(1)
        value = _parse_literal(match.group(2))
        op_map = {
            "==": operator.eq,
            "!=": operator.ne,
            "<": operator.lt,
            "<=": operator.le,
            ">": operator.gt,
            ">=": operator.ge,
        }
        if op_token not in op_map:
            raise NotImplementedError(
                f"Unsupported comparison operator '{op_token}' in callback '{expr}'."
            )
        series = frame[concept_name]
        if isinstance(value, (int, float)) and not pd.api.types.is_numeric_dtype(series):
            series = pd.to_numeric(series, errors="coerce")
        comparator = op_map[op_token]
        comparison = series.apply(
            lambda item: False if pd.isna(item) else comparator(item, value)
        ).astype("boolean")
        frame = frame.copy()
        frame.drop(columns=[concept_name], inplace=True)
        frame[concept_name] = comparison
        return frame

    match = re.fullmatch(r"transform_fun\(binary_op\(`(.+?)`,\s*(.+)\)\)", expr, flags=re.DOTALL)
    if match:
        symbol = match.group(1)
        value = _parse_literal(match.group(2))
        frame = frame.copy()
        series = pd.to_numeric(frame[concept_name], errors="coerce")
        result = _apply_binary_op(symbol, series, value)
        frame.loc[:, concept_name] = result
        return frame

    # åŒ¹é… mimic_sampling (R ricu callback-itm.R)
    # mimic_sampling(x, val_var, aux_time, ...)
    # åŠŸèƒ½ï¼š1) combine_date_time(x, aux_time, hours(12L))
    #      2) set(x, j = val_var, value = !is.na(x[[val_var]]))
    if expr == "mimic_sampling":
        frame = frame.copy()
        val_var = source.value_var or concept_name
        aux_time = source.params.get("aux_time") if source.params else None
        
        # 1. combine_date_time: å¦‚æœaux_timeæ˜¯NAï¼Œä½¿ç”¨index_column + 12å°æ—¶
        if aux_time and aux_time in frame.columns:
            # æ‰¾åˆ°å®é™…çš„indexåˆ—ï¼ˆé€šå¸¸æ˜¯charttime, starttimeç­‰ï¼‰
            # æ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®çš„index_var
            index_col = source.index_var
            if not index_col:
                # å°è¯•ä»è¡¨é…ç½®ä¸­è·å–
                time_cols = [col for col in frame.columns if pd.api.types.is_datetime64_any_dtype(frame[col])]
                if time_cols:
                    # ä¼˜å…ˆä½¿ç”¨éaux_timeçš„datetimeåˆ—
                    index_col = next((col for col in time_cols if col != aux_time), time_cols[0])
            
            if index_col and index_col in frame.columns:
                # å¦‚æœaux_timeæ˜¯NAï¼Œä½¿ç”¨index_col + 12å°æ—¶
                mask = frame[aux_time].isna()
                if mask.any():
                    frame.loc[mask, aux_time] = pd.to_datetime(frame.loc[mask, index_col], errors='coerce') + pd.Timedelta(hours=12)
                # æ›´æ–°index_columnä¸ºaux_timeï¼ˆä½¿ç”¨aux_timeä½œä¸ºæ—¶é—´ç´¢å¼•ï¼‰
                if index_col != aux_time:
                    # å°†aux_timeçš„å€¼å¤åˆ¶åˆ°index_colï¼Œç„¶ååˆ é™¤aux_time
                    frame[index_col] = pd.to_datetime(frame[aux_time], errors='coerce')
                    frame = frame.drop(columns=[aux_time])
        
        # 2. å°†val_varè½¬æ¢ä¸ºå¸ƒå°”å€¼ï¼ˆéNAä¸ºTrueï¼‰
        if val_var in frame.columns:
            frame[concept_name] = frame[val_var].notna().astype(bool)
            if val_var != concept_name:
                frame = frame.drop(columns=[val_var])
        else:
            # å¦‚æœval_varä¸å­˜åœ¨ï¼Œåˆ›å»ºconcept_nameåˆ—ï¼ˆå…¨Falseï¼‰
            frame[concept_name] = False
        
        return frame
    
    # åŒ¹é… apply_map(c(...), var = 'sub_var') æˆ– apply_map(c(...))
    match = re.fullmatch(r"apply_map\(\s*c\((.+?)\)\s*(?:,\s*var\s*=\s*['\"](.+?)['\"])?\s*\)", expr, flags=re.DOTALL)
    if match:
        mapping = _parse_mapping(match.group(1))
        var_param = match.group(2) if match.group(2) else None
        
        frame = frame.copy()
        
        # è§£æ var_paramï¼Œå¦‚æœæ˜¯ 'sub_var'ï¼Œä½¿ç”¨ source.sub_var çš„å®é™…å€¼
        target_col = None
        if var_param:
            if var_param == 'sub_var' and source.sub_var:
                # var='sub_var' è¡¨ç¤ºæ˜ å°„ sub_var åˆ—ï¼ˆå¦‚ itemidï¼‰
                target_col = source.sub_var
            elif var_param == 'val_col' and concept_name in frame.columns:
                # var='val_col' è¡¨ç¤ºæ˜ å°„å€¼åˆ—ï¼ˆconcept_nameï¼‰
                target_col = concept_name
            elif var_param in frame.columns:
                # ç›´æ¥ä½¿ç”¨ var_param ä½œä¸ºåˆ—å
                target_col = var_param
        
        # å¦‚æœæŒ‡å®šäº†ç›®æ ‡åˆ—ä¸”å­˜åœ¨ï¼Œæ˜ å°„è¯¥åˆ—ï¼›å¦åˆ™æ˜ å°„concept_nameåˆ—
        if target_col and target_col in frame.columns:
            # æ˜ å°„æŒ‡å®šçš„åˆ—
            series = frame[target_col]
            def mapper(val):
                if pd.isna(val):
                    return val
                # å°è¯•ç›´æ¥åŒ¹é…ï¼Œç„¶åå°è¯•å­—ç¬¦ä¸²åŒ¹é…
                result = mapping.get(val, mapping.get(str(val), val))
                return result
            
            # æ˜¾å¼è½¬æ¢ä¸º object ç±»å‹ä»¥é¿å… FutureWarning
            # å½“æ˜ å°„å€¼çš„ç±»å‹ä¸åŸåˆ—ç±»å‹ä¸å…¼å®¹æ—¶ï¼ˆå¦‚å­—ç¬¦ä¸²æ˜ å°„åˆ° int32ï¼‰ï¼Œéœ€è¦å…ˆè½¬æ¢ç±»å‹
            mapped_series = series.map(mapper)
            if frame[target_col].dtype != mapped_series.dtype:
                frame[target_col] = frame[target_col].astype(object)
            frame.loc[:, target_col] = mapped_series
        elif concept_name in frame.columns:
            # é»˜è®¤æ˜ å°„concept_nameåˆ—
            series = frame[concept_name]
            def mapper(val):
                if pd.isna(val):
                    return val
                return mapping.get(val, mapping.get(str(val), val))
            
            # åŒæ ·å¤„ç†ç±»å‹ä¸å…¼å®¹é—®é¢˜
            mapped_series = series.map(mapper)
            if frame[concept_name].dtype != mapped_series.dtype:
                frame[concept_name] = frame[concept_name].astype(object)
            frame.loc[:, concept_name] = mapped_series
        
        return frame

    match = re.fullmatch(r"convert_unit\((.+)\)", expr, flags=re.DOTALL)
    if match:
        arguments = _split_arguments(match.group(1))
        if not arguments:
            raise NotImplementedError(f"Callback '{callback}' is empty.")

        symbol, value = _parse_binary_op(arguments[0])
        new_unit = _strip_quotes(arguments[1]) if len(arguments) > 1 else None
        old_unit = _strip_quotes(arguments[2]) if len(arguments) > 2 else None

        frame = frame.copy()
        
        # å¦‚æœ source.unit_var æœªæŒ‡å®šï¼Œå°è¯•è‡ªåŠ¨æ£€æµ‹å•ä½åˆ—
        actual_unit_var = source.unit_var or unit_column
        
        # å¦‚æœä»ç„¶æ²¡æœ‰ï¼Œå°è¯•å¸¸è§çš„å•ä½åˆ—å
        if not actual_unit_var and 'valueuom' in frame.columns:
            actual_unit_var = 'valueuom'
        elif not actual_unit_var and 'unit' in frame.columns:
            actual_unit_var = 'unit'
        
        if actual_unit_var and actual_unit_var in frame.columns:
            unit_series = frame[actual_unit_var].fillna('').astype(str)
            if old_unit:
                case_flag = False
                try:
                    mask = unit_series.str.contains(old_unit, case=case_flag, na=False, regex=True)
                except re.error:
                    mask = unit_series.str.contains(re.escape(old_unit), case=case_flag, na=False, regex=True)
                # âš ï¸ ä¸åŒ¹é…ç©ºå•ä½è¡Œ: MIMIC-IVä¸­å•ä½ä¸ºç©ºæ—¶å€¼å·²ç»æ­£ç¡®
            else:
                # å¦‚æœold_unitä¸ºNoneï¼Œè½¬æ¢æ‰€æœ‰è¡Œï¼ˆR ricuè¡Œä¸ºï¼‰
                mask = pd.Series(True, index=frame.index)
        else:
            mask = pd.Series(True, index=frame.index)

        numeric = pd.to_numeric(frame.loc[mask, concept_name], errors="coerce")
        transformed = _apply_binary_op(symbol, numeric, value)
        
        # æ˜ç¡®è½¬æ¢ç±»å‹ä»¥é¿å… dtype ä¸å…¼å®¹è­¦å‘Š
        frame.loc[mask, concept_name] = transformed.astype('float64')

        # æ›´æ–°å•ä½åˆ—
        if new_unit and actual_unit_var and actual_unit_var in frame.columns:
            frame.loc[mask, actual_unit_var] = new_unit

        return frame

    match = re.fullmatch(r"combine_callbacks\((.+)\)", expr, flags=re.DOTALL)
    if match:
        frame_result = frame
        for arg in _split_arguments(match.group(1)):
            nested = arg.strip()
            if not nested:
                continue
            nested_source = replace(source, callback=nested)
            frame_result = _apply_callback(frame_result, nested_source, concept_name, unit_column)
        return frame_result
    
    # Handle dex_to_10 callback (convert different dextrose concentrations to D10 equivalent)
    # Format: dex_to_10(ids, factors) or dex_to_10(c(...), c(...))
    match = re.fullmatch(r"dex_to_10\((.+)\)", expr, flags=re.DOTALL)
    if match:
        args = _split_arguments(match.group(1))
        if len(args) >= 2:
            # Parse itemids and factors
            id_arg = args[0].strip()
            factor_arg = args[1].strip()
            
            # Parse list/vector syntax: c(228140L, 220952L) or list(...)
            def parse_vector(s):
                # Handle c(...) or list(...)
                vec_match = re.search(r'(?:c|list)\(([^)]+)\)', s)
                if vec_match:
                    items_str = vec_match.group(1)
                    items = [int(re.sub(r'L$', '', x.strip())) for x in items_str.split(',')]
                    return items
                # Handle single value
                else:
                    return [int(re.sub(r'L$', '', s.strip()))]
            
            try:
                itemids = parse_vector(id_arg)
                factors = parse_vector(factor_arg)
                
                # Apply conversion factors
                sub_var = source.sub_var if hasattr(source, 'sub_var') else 'itemid'
                # Try to find the value column: concept_name, or unit_column (which is the value column before renaming)
                val_col = None
                if concept_name in frame.columns:
                    val_col = concept_name
                elif unit_column and unit_column in frame.columns:
                    val_col = unit_column
                # Fallback: try common value column names
                elif 'rate' in frame.columns:
                    val_col = 'rate'
                elif 'amount' in frame.columns:
                    val_col = 'amount'
                elif 'valuenum' in frame.columns:
                    val_col = 'valuenum'
                
                if sub_var in frame.columns and val_col:
                    frame = frame.copy()
                    for itemid, factor in zip(itemids, factors):
                        mask = frame[sub_var] == itemid
                        if mask.any():
                            frame.loc[mask, val_col] = frame.loc[mask, val_col] * factor
            except Exception:
                # Silently skip if parsing fails
                pass
        return frame
    
    # Handle ts_to_win_tbl callback
    match = re.fullmatch(r"ts_to_win_tbl\((.+)\)", expr, flags=re.DOTALL)
    if match:
        # Parse the duration expression (e.g., "mins(1L)")
        dur_expr = match.group(1).strip()
        # Simple parsing for common duration patterns
        if 'mins(' in dur_expr:
            mins_match = re.search(r'mins\((\d+)', dur_expr)
            if mins_match:
                duration = pd.Timedelta(minutes=int(mins_match.group(1)))
            else:
                duration = pd.Timedelta(minutes=1)  # default
        elif 'hours(' in dur_expr:
            hours_match = re.search(r'hours\((\d+)', dur_expr)
            if hours_match:
                duration = pd.Timedelta(hours=int(hours_match.group(1)))
            else:
                duration = pd.Timedelta(hours=1)  # default
        else:
            duration = pd.Timedelta(minutes=1)  # default fallback
        
        # Add duration column
        frame = frame.copy()
        frame['dur_var'] = duration
        return frame
    
    # Handle mimic_rate_mv callback (for infusion rates)
    if expr.strip() == "mimic_rate_mv":
        from .callback_utils import mimic_rate_mv
        # Call the callback with appropriate parameters
        id_cols = [col for col in frame.columns if 'id' in col.lower()]
        # stop_var is stored in params dict
        stop_var = source.params.get('stop_var', None) if source.params else None
        unit_col = source.unit_var if hasattr(source, 'unit_var') else None
        val_col = concept_name
        
        return mimic_rate_mv(
            frame,
            val_col=val_col,
            unit_col=unit_col,
            stop_var=stop_var,
            id_cols=id_cols
        )
    
    # Handle mimic_dur_inmv callback (for infusion durations)
    if expr.strip() == "mimic_dur_inmv":
        from .callback_utils import mimic_dur_inmv
        # Call the callback with appropriate parameters
        id_cols = [col for col in frame.columns if 'id' in col.lower()]
        # stop_var and grp_var are stored in params dict
        stop_var = source.params.get('stop_var', None) if source.params else None
        grp_var = source.params.get('grp_var', None) if source.params else None
        # Use unit_column from parent context or source.unit_var
        unit_col = unit_column or (source.unit_var if hasattr(source, 'unit_var') else None)
        val_col = concept_name
        
        return mimic_dur_inmv(
            frame,
            val_col=val_col,
            grp_var=grp_var,
            stop_var=stop_var,
            id_cols=id_cols,
            unit_col=unit_col
        )
    
    # Handle mimic_dur_incv callback (for CareVue durations)
    if expr.strip() == "mimic_dur_incv":
        from .callback_utils import mimic_dur_incv
        # Call the callback with appropriate parameters
        id_cols = [col for col in frame.columns if 'id' in col.lower()]
        # grp_var is stored in params dict
        grp_var = source.params.get('grp_var', None) if source.params else None
        # Use unit_column from parent context or source.unit_var
        unit_col = unit_column or (source.unit_var if hasattr(source, 'unit_var') else None)
        val_col = concept_name
        
        return mimic_dur_incv(
            frame,
            val_col=val_col,
            grp_var=grp_var,
            id_cols=id_cols,
            unit_col=unit_col
        )
    
    # Handle mimic_rate_cv callback (for CareVue infusion rates)
    if expr.strip() == "mimic_rate_cv":
        from .callback_utils import mimic_rate_cv
        # Call the callback with appropriate parameters
        id_cols = [col for col in frame.columns if 'id' in col.lower()]
        # grp_var is stored in params dict
        grp_var = source.params.get('grp_var', None) if source.params else None
        unit_col = source.unit_var if hasattr(source, 'unit_var') else None
        val_col = concept_name
        
        return mimic_rate_cv(
            frame,
            val_col=val_col,
            grp_var=grp_var,
            unit_col=unit_col,
            id_cols=id_cols
        )

    if expr.strip() == "vent_flag":
        from .callback_utils import vent_flag

        id_cols = [col for col in frame.columns if 'id' in col.lower()]
        index_var = source.index_var
        
        # ğŸ”¥ FIX: å¦‚æœ source.index_var æ˜¯ Noneï¼Œå°è¯•ä»è¡¨é…ç½®è·å–é»˜è®¤ index_var
        # eICU vent_start æºæœ‰ index_var: Noneï¼Œä½†è¡¨ respiratorycare çš„é»˜è®¤æ˜¯ respcarestatusoffset
        if index_var is None and data_source is not None:
            try:
                table_cfg = data_source.config.get_table(source.table)
                if table_cfg and table_cfg.defaults:
                    index_var = table_cfg.defaults.index_var
                    if DEBUG_MODE:
                        print(f"   ğŸ”§ vent_flag: source.index_var=Noneï¼Œä½¿ç”¨è¡¨é»˜è®¤ index_var='{index_var}'")
            except Exception:
                pass
        
        # ğŸ”¥ R ricu vent_flag: val_var æ˜¯åŸå§‹åˆ—åï¼ˆå¦‚ ventstartoffsetï¼‰ï¼Œä¸æ˜¯æ¦‚å¿µå
        # vent_flag ä¼šå°† val_var çš„å€¼ä½œä¸ºæ–°çš„æ—¶é—´ç´¢å¼•ï¼Œç„¶åå°† val_var è®¾ä¸º TRUE
        # ğŸ”§ FIX: å¦‚æœ value_var å·²è¢«é‡å‘½åä¸º concept_nameï¼Œä½¿ç”¨ concept_name
        val_col = source.value_var if hasattr(source, 'value_var') and source.value_var else concept_name
        if val_col not in frame.columns and concept_name in frame.columns:
            val_col = concept_name
        return vent_flag(
            frame,
            val_col=val_col,
            index_var=index_var,
            id_cols=id_cols,
        )

    match = re.fullmatch(r"eicu_duration\(\s*gap_length\s*=\s*(.+)\)", expr, flags=re.DOTALL)
    if match:
        from .callback_utils import eicu_duration_callback

        gap_arg = match.group(1)
        # Parse interval expression directly
        gap_expr = gap_arg.strip()
        interval_match = re.fullmatch(r"([a-zA-Z]+)\((.+)\)", gap_expr)
        if interval_match:
            unit = interval_match.group(1).lower()
            value = _parse_literal(interval_match.group(2))
            if unit in {"min", "mins", "minute", "minutes"}:
                gap = pd.to_timedelta(value, unit="m")
            elif unit in {"hour", "hours"}:
                gap = pd.to_timedelta(value, unit="h")
            elif unit in {"sec", "secs", "second", "seconds"}:
                gap = pd.to_timedelta(value, unit="s")
            elif unit in {"day", "days"}:
                gap = pd.to_timedelta(value, unit="d")
            else:
                raise ValueError(f"Unsupported interval unit '{unit}' in expression '{gap_expr}'")
        else:
            raise ValueError(f"Unsupported interval expression '{gap_arg}'")
        
        callback_fn = eicu_duration_callback(gap)
        # åªä½¿ç”¨æ‚£è€…çº§åˆ«çš„IDåˆ—è¿›è¡Œåˆ†ç»„ï¼Œä¸è¦ä½¿ç”¨è¡Œçº§åˆ«çš„å”¯ä¸€IDï¼ˆå¦‚infusiondrugidï¼‰
        # å¦åˆ™æ¯ç»„åªæœ‰ä¸€è¡Œï¼Œdurationè®¡ç®—ä¼šå˜æˆ0
        patient_id_cols = ['patientunitstayid', 'stay_id', 'icustay_id', 'hadm_id', 'admissionid', 'patientid']
        id_cols = [col for col in patient_id_cols if col in frame.columns]
        if not id_cols:
            # å›é€€åˆ°é€šç”¨æ£€æµ‹ï¼Œä½†æ’é™¤æ˜æ˜¾çš„è¡Œçº§åˆ«ID
            excluded_patterns = ['infusion', 'drug', 'event', 'row', 'fluid']
            id_cols = [col for col in frame.columns 
                      if 'id' in col.lower() 
                      and not any(pat in col.lower() for pat in excluded_patterns)]
        index_var = source.index_var
        return callback_fn(
            frame,
            val_col=concept_name,
            index_var=index_var,
            id_cols=id_cols,
        )

    # Handle eicu_rate_kg(ml_to_mcg = VALUE) - eICU dose rate conversion with weight
    match = re.fullmatch(r"eicu_rate_kg\(\s*ml_to_mcg\s*=\s*(.+)\)", expr, flags=re.DOTALL)
    if match:
        from .callback_utils import eicu_rate_kg_callback
        
        ml_to_mcg = float(match.group(1))
        callback_fn = eicu_rate_kg_callback(ml_to_mcg)
        
        # Get necessary variables
        val_var = source.value_var or concept_name
        sub_var = source.sub_var
        weight_var = source.params.get('weight_var', 'admissionweight') if source.params else 'admissionweight'
        
        return callback_fn(
            frame,
            val_var=val_var,
            sub_var=sub_var,
            weight_var=weight_var,
            concept_name=concept_name,
            data_source=data_source,
            patient_ids=patient_ids,
        )
        
    match = re.fullmatch(r"eicu_rate_units\((.+)\)", expr, flags=re.DOTALL)
    if match:
        from .callback_utils import eicu_rate_units_callback

        args = _split_arguments(match.group(1))
        if len(args) < 2:
            raise ValueError(f"eicu_rate_units requires two arguments, got '{expr}'")

        def _arg_to_float(text: str) -> float:
            part = text.split("=", 1)[1] if "=" in text else text
            return float(_parse_literal(part.strip()))

        ml_to_mcg = _arg_to_float(args[0])
        mcg_to_units = _arg_to_float(args[1])
        callback_fn = eicu_rate_units_callback(ml_to_mcg, mcg_to_units)

        val_var = source.value_var or concept_name
        sub_var = source.sub_var

        return callback_fn(
            frame,
            val_var=val_var,
            sub_var=sub_var,
            concept_name=concept_name,
        )

    if expr == "aumc_rate_kg":
        from .callback_utils import aumc_rate_kg

        val_var = source.value_var or concept_name
        unit_var = source.unit_var or unit_column
        rel_weight = source.params.get("rel_weight") if source.params else None
        rate_uom = source.params.get("rate_uom") if source.params else None
        if rate_uom is None and "rateunit" in frame.columns:
            rate_uom = "rateunit"
        stop_var = source.params.get("stop_var") if source.params else None
        index_var = source.index_var
        
        # source.index_var may be None, use table default as fallback
        # For AUMC drugitems, the index_var should be 'start'
        if not index_var and source.table == 'drugitems':
            index_var = 'start'

        # ğŸ”§ FIX: è·å–ä½“é‡æ¦‚å¿µå¹¶åˆå¹¶åˆ° frame ä¸­
        # R ricu åœ¨å›è°ƒä¸­ä½¿ç”¨ add_weight(res, env, "weight") è·å–ä½“é‡
        # pyricu éœ€è¦åœ¨è°ƒç”¨å›è°ƒå‰åŠ è½½ weight æ¦‚å¿µ
        if 'weight' not in frame.columns and resolver is not None and data_source is not None:
            try:
                # è·å–æ‚£è€…IDåˆ—
                id_cols = [c for c in frame.columns if c.lower().endswith('id') and c != 'itemid']
                if id_cols:
                    unique_ids = frame[id_cols[0]].unique().tolist()
                    # åŠ è½½ weight æ¦‚å¿µ
                    weight_table = resolver._load_single_concept(
                        'weight',
                        data_source,
                        aggregator=False,  # ä¸èšåˆï¼Œä¿ç•™åŸå§‹å€¼
                        patient_ids={id_cols[0]: unique_ids},
                        verbose=False,
                        _bypass_callback=True,  # é¿å…å›è°ƒå¾ªç¯
                    )
                    if weight_table is not None and not weight_table.data.empty:
                        weight_df = weight_table.data
                        # ç¡®ä¿weightåˆ—æ˜¯æ•°å€¼å‹
                        if 'weight' in weight_df.columns:
                            weight_df['weight'] = pd.to_numeric(weight_df['weight'], errors='coerce')
                            # åˆå¹¶åˆ°frame
                            merge_cols = [c for c in id_cols if c in weight_df.columns]
                            if merge_cols:
                                frame = frame.merge(
                                    weight_df[merge_cols + ['weight']].drop_duplicates(),
                                    on=merge_cols,
                                    how='left'
                                )
            except Exception as e:
                # å¦‚æœè·å–ä½“é‡å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
                if DEBUG_MODE:
                    print(f"   âš ï¸  è·å–ä½“é‡å¤±è´¥: {e}")
                pass

        return aumc_rate_kg(
            frame,
            concept_name=concept_name,
            val_col=val_var,
            unit_col=unit_var,
            rel_weight_col=rel_weight,
            rate_unit_col=rate_uom,
            index_col=index_var,
            stop_col=stop_var,
        )

    # Handle aumc_rate callback - combine unit_var and rate_var into unit/rate format
    # R: x <- x[, c(unit_var) := do_call(.SD, paste, sep = "/"), .SDcols = c(unit_var, rate_var)]
    if expr == "aumc_rate":
        rate_var = getattr(source, 'rate_var', None)
        if not rate_var and source.params:
            rate_var = source.params.get("rate_var")
        unit_var = source.unit_var or unit_column
        
        if rate_var and unit_var and rate_var in frame.columns and unit_var in frame.columns:
            frame = frame.copy()
            # Combine unit and rate into "unit/rate" format
            frame[unit_var] = frame[unit_var].astype(str) + "/" + frame[rate_var].astype(str)
        return frame

    match = re.fullmatch(r"aumc_rate_units\(\s*([0-9eE+\-\.]+)\s*\)", expr)
    if match:
        from .callback_utils import aumc_rate_units_callback

        factor = float(match.group(1))
        callback_fn = aumc_rate_units_callback(factor)

        val_var = source.value_var or concept_name
        unit_var = source.unit_var or unit_column
        rate_uom = source.params.get("rate_uom") if source.params else None
        if rate_uom is None and "rateunit" in frame.columns:
            rate_uom = "rateunit"
        stop_var = source.params.get("stop_var") if source.params else None

        return callback_fn(
            frame,
            val_col=val_var,
            unit_col=unit_var,
            rate_unit_col=rate_uom,
            stop_col=stop_var,
            concept_name=concept_name,
        )

    if expr == "aumc_dur":
        from .callback_utils import aumc_dur

        val_var = source.value_var or concept_name
        # stop_var and grp_var can be direct attributes on source or in source.params
        stop_var = getattr(source, 'stop_var', None)
        if not stop_var and source.params:
            stop_var = source.params.get("stop_var")
        grp_var = getattr(source, 'grp_var', None)
        if not grp_var and source.params:
            grp_var = source.params.get("grp_var")
        index_var = source.index_var

        return aumc_dur(
            frame,
            val_col=val_var,
            stop_var=stop_var,
            grp_var=grp_var,
            index_var=index_var,
            concept_name=concept_name,
        )

    # Handle aumc_bxs callback - negate values where direction is '-'
    # R implementation: x[get(dir_var) == "-", val_var := -1L * get(val_var)]
    if expr == "aumc_bxs":
        dir_var = getattr(source, 'dir_var', None)
        if not dir_var and source.params:
            dir_var = source.params.get("dir_var")
        if not dir_var:
            dir_var = "tag"  # default for AUMC
        
        val_var = concept_name  # Value column has already been renamed to concept_name
        
        if dir_var in frame.columns and val_var in frame.columns:
            # Negate values where direction is '-'
            mask = frame[dir_var] == '-'
            if mask.any():
                frame = frame.copy()
                frame.loc[mask, val_var] = -1 * frame.loc[mask, val_var]
        return frame

    # Handle eicu_age callback
    if expr == "transform_fun(eicu_age)":
        from .callback_utils import eicu_age
        return eicu_age(frame, val_col=concept_name)

    # Handle aumc_rass callback
    if expr == "transform_fun(aumc_rass)":
        # Apply aumc_rass transformation: extract first 2 characters as integer
        # Similar to ricu's: as.integer(substr(x, 1L, 2L))
        series = frame[concept_name].copy()
        series = series.astype(str).str[:2]
        series = pd.to_numeric(series, errors='coerce')
        frame[concept_name] = series
        return frame

    if expr.strip() == "distribute_amount":
        from .callback_utils import distribute_amount
        end_col = source.params.get("end_var") if source.params else None
        if not end_col:
            end_col = source.params.get("dur_var") if source.params else None
        if not end_col and "endtime" in frame.columns:
            end_col = "endtime"
        index_col = source.index_var
        # ğŸ”§ FIX: æ·»åŠ  starttime ä½œä¸º fallbackï¼Œç”¨äº inputevents è¡¨çš„æ•°æ® (å¦‚ ins)
        if not index_col:
            for candidate in ["charttime", "starttime", "time"]:
                if candidate in frame.columns:
                    index_col = candidate
                    break
        unit_col = unit_column or source.unit_var
        if not unit_col:
            if "rateuom" in frame.columns:
                unit_col = "rateuom"
            elif "valueuom" in frame.columns:
                unit_col = "valueuom"
        if not end_col or end_col not in frame.columns:
            return frame
        if not index_col or index_col not in frame.columns:
            return frame
        return distribute_amount(
            frame,
            val_col=concept_name,
            unit_col=unit_col,
            end_col=end_col,
            index_col=index_col,
        )

    if expr.strip() == "mimv_rate":
        from .callback_utils import mimv_rate
        duration_col = None
        start_col = source.index_var
        if not start_col:
            if "starttime" in frame.columns:
                start_col = "starttime"
        end_col = None
        if source.params:
            end_col = source.params.get("dur_var") or source.params.get("end_var")
        if not end_col and "endtime" in frame.columns:
            end_col = "endtime"
        
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»æœ‰è®¡ç®—å¥½çš„durationåˆ— (æ¦‚å¿µå_duræ ¼å¼)
        possible_dur_cols = [concept_name + '_dur', 'duration', '__duration__']
        for col in possible_dur_cols:
            if col in frame.columns:
                duration_col = col
                break
        
        # å¦‚æœæ²¡æœ‰ç°æˆçš„durationåˆ—ï¼Œå°è¯•ä»startå’Œendè®¡ç®—
        if not duration_col:
            if end_col and end_col in frame.columns and start_col and start_col in frame.columns:
                start = pd.to_datetime(frame[start_col], errors="coerce")
                stop = pd.to_datetime(frame[end_col], errors="coerce")
                frame = frame.copy()
                frame["__duration__"] = stop - start
                duration_col = "__duration__"
            elif end_col and end_col in frame.columns:
                duration_col = end_col
        
        if not duration_col or duration_col not in frame.columns:
            return frame
        amount_col = concept_name
        if source.params:
            alt_amount = source.params.get("amount_var")
            if alt_amount and alt_amount in frame.columns:
                amount_col = alt_amount
        unit_col = unit_column or source.unit_var
        if not unit_col:
            if "rateuom" in frame.columns:
                unit_col = "rateuom"
            elif "valueuom" in frame.columns:
                unit_col = "valueuom"
        auom_col = None
        if source.params:
            auom_col = source.params.get("auom_var")
        if not auom_col or auom_col not in frame.columns:
            if "amountuom" in frame.columns:
                auom_col = "amountuom"
            else:
                auom_col = unit_col
        return mimv_rate(
            frame,
            val_col=concept_name,
            unit_col=unit_col,
            dur_var=duration_col,
            amount_var=amount_col,
            auom_var=auom_col,
        )

    match = re.fullmatch(r"dex_to_10\((.+)\)", expr, flags=re.DOTALL)
    if match:
        from .callback_utils import dex_to_10

        args = _split_arguments(match.group(1))
        if len(args) < 2:
            return frame

        ids = _parse_r_value(args[0])
        factors = _parse_r_value(args[1])
        if not isinstance(ids, list):
            ids = [ids]
        if not isinstance(factors, list):
            factors = [factors]

        callback_fn = dex_to_10(ids, factors)
        sub_var = source.sub_var
        if not sub_var or sub_var not in frame.columns:
            return frame
        return callback_fn(
            frame,
            sub_var=sub_var,
            val_col=concept_name,
        )

    if expr.strip() == "eicu_dex_med":
        from .callback_utils import eicu_dex_med as eicu_dex_med_cb

        val_var = source.value_var or concept_name
        dur_var = None
        if source.params:
            dur_var = source.params.get("dur_var") or source.params.get("stop_var")
        if not dur_var or dur_var not in frame.columns:
            if "duration" in frame.columns:
                dur_var = "duration"
            elif "drugstopoffset" in frame.columns:
                dur_var = "drugstopoffset"
        if not dur_var or dur_var not in frame.columns:
            return frame

        return eicu_dex_med_cb(
            frame,
            val_var=val_var,
            dur_var=dur_var,
            concept_name=concept_name,
        )

    if expr.strip() == "eicu_dex_inf":
        from .callback_utils import eicu_dex_inf as eicu_dex_inf_cb

        val_var = source.value_var or concept_name
        index_var = source.index_var

        return eicu_dex_inf_cb(
            frame,
            val_var=val_var,
            index_var=index_var,
        )

    # blood_cell_ratio callback - convert absolute cell counts to percentage
    # R ricu logic: 100 * value / wbc
    # Used for lymphocytes, neutrophils, etc.
    if expr.strip() == "blood_cell_ratio":
        DEBUG_CALLBACK = False  # Toggle for debugging
        if DEBUG_CALLBACK:
            print(f"  [CALLBACK DEBUG] {concept_name} blood_cell_ratio å¼€å§‹")
            print(f"    frame.shape = {frame.shape}, columns = {list(frame.columns)}")
            if concept_name in frame.columns:
                print(f"    è¾“å…¥å€¼: {frame[concept_name].values}")
        
        if resolver is None:
            if DEBUG_CALLBACK:
                print(f"    [SKIP] resolver is None")
            # Cannot convert without resolver to load WBC, return as-is
            if concept_name in frame.columns:
                frame[concept_name] = pd.to_numeric(frame[concept_name], errors='coerce')
            return frame
        
        # Determine ID column based on database
        # AUMC uses 'admissionid', MIMIC uses 'stay_id', eICU uses 'patientunitstayid'
        id_col = None
        for possible_id in ['admissionid', 'stay_id', 'patientunitstayid', 'subject_id', 'icustay_id']:
            if possible_id in frame.columns:
                id_col = possible_id
                break
        
        if id_col is None:
            if DEBUG_CALLBACK:
                print(f"    [SKIP] id_col is None")
            if concept_name in frame.columns:
                frame[concept_name] = pd.to_numeric(frame[concept_name], errors='coerce')
            return frame
        
        if DEBUG_CALLBACK:
            print(f"    id_col = {id_col}")
        
        frame_patient_ids = frame[id_col].unique().tolist()
        if len(frame_patient_ids) == 0:
            if DEBUG_CALLBACK:
                print(f"    [SKIP] no patients")
            if concept_name in frame.columns:
                frame[concept_name] = pd.to_numeric(frame[concept_name], errors='coerce')
            return frame
        
        if DEBUG_CALLBACK:
            print(f"    patients = {frame_patient_ids}")
        
        try:
            # Load WBC concept for the same patients
            # IMPORTANT: Use merge=False to get Dict[str, ICUTable] instead of merged DataFrame
            # IMPORTANT: Must pass data_source for resolver.load_concepts to work
            # IMPORTANT: Use _skip_concept_cache=True to avoid polluting the main cache
            # This way, the internal wbc load won't affect subsequent wbc loads
            if data_source is None:
                if DEBUG_CALLBACK:
                    print(f"    [SKIP] data_source is None")
                if concept_name in frame.columns:
                    frame[concept_name] = pd.to_numeric(frame[concept_name], errors='coerce')
                return frame
            
            if DEBUG_CALLBACK:
                print(f"    åŠ è½½ WBC (è·³è¿‡ç¼“å­˜)...")
            
            wbc_result = resolver.load_concepts(
                ['wbc'],
                data_source,
                patient_ids=frame_patient_ids,  # Only load for needed patients
                ricu_compatible=False,
                merge=False,
                _skip_concept_cache=True,  # Don't cache this internal call
            )
            
            if 'wbc' not in wbc_result or wbc_result['wbc'].data.empty:
                if DEBUG_CALLBACK:
                    print(f"    [SKIP] WBC ä¸ºç©ºæˆ–ä¸å­˜åœ¨")
                if concept_name in frame.columns:
                    frame[concept_name] = pd.to_numeric(frame[concept_name], errors='coerce')
                return frame
            
            wbc_df = wbc_result['wbc'].data.copy()
            if DEBUG_CALLBACK:
                print(f"    WBC loaded: {len(wbc_df)} rows, columns = {list(wbc_df.columns)}")
                print(f"    WBCæ ·æœ¬:\n{wbc_df.head(10)}")
            
            # Find index column for merging (time column)
            index_col = source.index_var
            if not index_col:
                for possible_idx in ['measuredat', 'charttime', 'starttime', 'labresultoffset']:
                    if possible_idx in frame.columns:
                        index_col = possible_idx
                        break
            
            if DEBUG_CALLBACK:
                print(f"    index_col = {index_col}")
            
            # Prepare WBC for merge - rename value column
            wbc_val_col = wbc_result['wbc'].value_column or 'wbc'
            if DEBUG_CALLBACK:
                print(f"    wbc_val_col = {wbc_val_col}")
            if wbc_val_col != 'wbc' and wbc_val_col in wbc_df.columns:
                wbc_df = wbc_df.rename(columns={wbc_val_col: 'wbc'})
            
            # Ensure ID column exists in WBC data
            if id_col not in wbc_df.columns:
                if DEBUG_CALLBACK:
                    print(f"    [SKIP] id_col {id_col} not in wbc_df")
                if concept_name in frame.columns:
                    frame[concept_name] = pd.to_numeric(frame[concept_name], errors='coerce')
                return frame
            
            # Ensure numeric types
            frame[concept_name] = pd.to_numeric(frame[concept_name], errors='coerce')
            wbc_df['wbc'] = pd.to_numeric(wbc_df['wbc'], errors='coerce')
            
            # Ensure matching dtypes for merge columns (fix int32 vs int64 issue)
            if id_col in frame.columns and id_col in wbc_df.columns:
                wbc_df[id_col] = wbc_df[id_col].astype(frame[id_col].dtype)
            
            # For each row in frame, find the closest WBC measurement
            # This is a time-based merge (asof merge)
            if index_col and index_col in frame.columns and index_col in wbc_df.columns:
                # CRITICAL FIX: For AUMC, frame's measuredat is in MINUTES (raw from datasource),
                # but wbc_df's measuredat is in HOURS (after load_concepts processing).
                # We need to convert frame's time to HOURS before merge.
                # Detect AUMC by checking for large time values (>1000 typically means minutes)
                frame_time_max = frame[index_col].abs().max()
                wbc_time_max = wbc_df[index_col].abs().max() if not wbc_df.empty else 0
                
                # Create copies to avoid modifying original
                frame_work = frame.copy()
                wbc_work = wbc_df.copy()
                
                # If frame has much larger time values than wbc, convert frame from minutes to hours
                if frame_time_max > 1000 and wbc_time_max < 1000 and wbc_time_max > 0:
                    if DEBUG_CALLBACK:
                        print(f"    [TIME FIX] æ£€æµ‹åˆ°æ—¶é—´å•ä½ä¸åŒ¹é…:")
                        print(f"      frame max time: {frame_time_max} (å¯èƒ½æ˜¯åˆ†é’Ÿ)")
                        print(f"      wbc max time: {wbc_time_max} (å¯èƒ½æ˜¯å°æ—¶)")
                        print(f"      -> å°† frame æ—¶é—´ä»åˆ†é’Ÿè½¬æ¢ä¸ºå°æ—¶")
                    frame_work[index_col] = frame_work[index_col] / 60.0
                elif frame_time_max < 1000 and wbc_time_max > 1000:
                    # Opposite case: wbc is in minutes, frame is in hours
                    if DEBUG_CALLBACK:
                        print(f"    [TIME FIX] æ£€æµ‹åˆ°æ—¶é—´å•ä½ä¸åŒ¹é…ï¼ˆåå‘ï¼‰:")
                        print(f"      frame max time: {frame_time_max}")
                        print(f"      wbc max time: {wbc_time_max}")
                        print(f"      -> å°† wbc æ—¶é—´ä»åˆ†é’Ÿè½¬æ¢ä¸ºå°æ—¶")
                    wbc_work[index_col] = wbc_work[index_col] / 60.0
                
                # Ensure matching dtypes for index column
                wbc_work[index_col] = wbc_work[index_col].astype(frame_work[index_col].dtype)
                
                # CRITICAL: merge_asof requires the 'on' column to be sorted globally.
                # With multiple patients, their time ranges may overlap. 
                # Solution: Process each patient separately and concat results.
                merged_parts = []
                for patient_id in frame_work[id_col].unique():
                    frame_patient = frame_work[frame_work[id_col] == patient_id].sort_values(index_col)
                    wbc_patient = wbc_work[wbc_work[id_col] == patient_id].sort_values(index_col)
                    
                    if wbc_patient.empty:
                        # No WBC data for this patient, keep original frame
                        merged_parts.append(frame_patient)
                        continue
                    
                    try:
                        merged_patient = pd.merge_asof(
                            frame_patient,
                            wbc_patient[[id_col, index_col, 'wbc']],
                            on=index_col,
                            by=id_col,
                            direction='nearest',
                        )
                        merged_parts.append(merged_patient)
                    except Exception as e:
                        if DEBUG_CALLBACK:
                            print(f"    [WARN] merge_asof failed for patient {patient_id}: {e}")
                        merged_parts.append(frame_patient)
                
                if merged_parts:
                    frame_merged = pd.concat(merged_parts, ignore_index=True)
                else:
                    frame_merged = frame_work.copy()
                
                if DEBUG_CALLBACK:
                    print(f"    Frame before merge:\n{frame_work[[id_col, index_col, concept_name]]}")
                    print(f"    After merge_asof:\n{frame_merged[[id_col, index_col, concept_name] + (['wbc'] if 'wbc' in frame_merged.columns else [])]}")
                
                # Calculate ratio: 100 * value / wbc
                if 'wbc' in frame_merged.columns:
                    valid_mask = (frame_merged['wbc'].notna()) & (frame_merged['wbc'] != 0)
                    if DEBUG_CALLBACK:
                        print(f"    valid_mask: {valid_mask.values}, sum={valid_mask.sum()}")
                    frame_merged.loc[valid_mask, concept_name] = (
                        100 * frame_merged.loc[valid_mask, concept_name] / 
                        frame_merged.loc[valid_mask, 'wbc']
                    )
                    if DEBUG_CALLBACK:
                        print(f"    è®¡ç®—åå€¼: {frame_merged[concept_name].values}")
                    # Set unit to %
                    if unit_column and unit_column in frame_merged.columns:
                        frame_merged.loc[valid_mask, unit_column] = '%'
                    # Drop WBC column
                    frame_merged = frame_merged.drop(columns=['wbc'])
                else:
                    if DEBUG_CALLBACK:
                        print(f"    [WARNING] 'wbc' not in frame_merged.columns!")
                
                # CRITICAL: Convert time back to original format (minutes) for AUMC
                # The subsequent processing will apply the minutes->hours conversion again
                if frame_time_max > 1000 and wbc_time_max < 1000 and wbc_time_max > 0:
                    # We converted frame from minutes to hours, now convert back
                    frame_merged[index_col] = frame_merged[index_col] * 60.0
                    if DEBUG_CALLBACK:
                        print(f"    [TIME RESTORE] å°†æ—¶é—´ä»å°æ—¶è½¬æ¢å›åˆ†é’Ÿ")
                
                if DEBUG_CALLBACK:
                    print(f"    è¿”å› frame_merged, shape={frame_merged.shape}")
                return frame_merged
            else:
                if DEBUG_CALLBACK:
                    print(f"    [FALLBACK] index_col ä¸åœ¨ä¸¤ä¸ª frame ä¸­, ä½¿ç”¨å¹³å‡ WBC")
                # No index column, use simple merge on ID (average WBC per patient)
                wbc_grouped = wbc_df.groupby(id_col)['wbc'].mean().reset_index()
                frame = frame.merge(wbc_grouped, on=id_col, how='left')
                
                valid_mask = (frame['wbc'].notna()) & (frame['wbc'] != 0)
                frame.loc[valid_mask, concept_name] = (
                    100 * frame.loc[valid_mask, concept_name] / 
                    frame.loc[valid_mask, 'wbc']
                )
                if unit_column and unit_column in frame.columns:
                    frame.loc[valid_mask, unit_column] = '%'
                frame = frame.drop(columns=['wbc'])
                
                return frame
                
        except Exception as e:
            if DEBUG_CALLBACK:
                print(f"    [EXCEPTION] {type(e).__name__}: {e}")
                import traceback
                traceback.print_exc()
            # On error, return frame as-is with numeric conversion
            if concept_name in frame.columns:
                frame[concept_name] = pd.to_numeric(frame[concept_name], errors='coerce')
            return frame

    raise NotImplementedError(
        f"Callback '{callback}' is not yet supported."
    )

def _apply_binary_op(symbol: str, series: pd.Series, value: object) -> pd.Series:
    """Apply binary operation or conversion function."""
    # Import conversion functions
    from .callback_utils import fahr_to_cels
    from .unit_conversion import celsius_to_fahrenheit, fahrenheit_to_celsius
    
    # Special case: set_val_na - set all values to NA
    if symbol == "set_val_na":
        return pd.Series([np.nan] * len(series), index=series.index)
    
    # Function map for unit conversions
    func_map = {
        "fahr_to_cels": fahr_to_cels,
        "fahrenheit_to_celsius": fahrenheit_to_celsius,
        "celsius_to_fahrenheit": celsius_to_fahrenheit,
    }
    
    # If it's a known function name, apply it
    if symbol in func_map:
        return func_map[symbol](series)
    
    # Otherwise treat as binary operator
    op_map = {
        "*": operator.mul,
        "/": operator.truediv,
        "+": operator.add,
        "-": operator.sub,
        "^": operator.pow,
    }

    if symbol not in op_map:
        raise NotImplementedError(f"Unsupported binary operator '{symbol}'")

    # Safe handling for division operations
    if symbol == "/":
        from .callback_utils import binary_op
        # Convert series to apply safe binary operation element-wise
        safe_op = binary_op(op_map[symbol], value)
        return series.apply(safe_op)
    else:
        try:
            return op_map[symbol](series, value)
        except (TypeError, ZeroDivisionError):
            return series  # Return original series on error

def _parse_binary_op(expr: str) -> tuple[str, object]:
    """Parse binary_op expression.
    
    Handles:
    - binary_op(`+`, 10)
    - fahr_to_cels (function name only)
    - set_val(NA) (special: set all values to NA)
    """
    # Check for set_val(NA) - special case for convert_unit
    if re.fullmatch(r'set_val\(NA\)', expr.strip(), re.IGNORECASE):
        return 'set_val_na', None
    
    # Check if it's just a function name (like fahr_to_cels)
    if re.fullmatch(r'[a-zA-Z_][a-zA-Z0-9_]*', expr.strip()):
        # It's a function name - return it as a special operator
        return expr.strip(), None
    
    # Otherwise parse as binary_op(symbol, value)
    match = re.fullmatch(r"binary_op\(`(.+?)`,\s*(.+)\)", expr.strip(), flags=re.DOTALL)
    if not match:
        raise NotImplementedError(f"Unsupported binary_op expression '{expr}'")
    symbol = match.group(1)
    value = _parse_literal(match.group(2))
    return symbol, value

def _parse_mapping(body: str) -> Dict[object, object]:
    mapping: Dict[object, object] = {}
    for pair in _split_arguments(body):
        if "=" not in pair:
            continue
        key_text, value_text = pair.split("=", 1)
        key = _parse_literal(key_text.strip())
        value = _parse_literal(value_text.strip())
        mapping[key] = value
    return mapping

def _parse_r_arguments(expr: str) -> list:
    return [_parse_r_value(arg) for arg in _split_arguments(expr)]

def _parse_r_value(token: str):
    text = token.strip()
    if text.startswith("list(") and text.endswith(")"):
        inner = text[5:-1]
        return [_parse_r_value(arg) for arg in _split_arguments(inner)]
    if text.startswith("c(") and text.endswith(")"):
        inner = text[2:-1]
        return [_parse_r_value(arg) for arg in _split_arguments(inner)]
    return _parse_literal(text)

def _split_arguments(argument_str: str) -> List[str]:
    args: List[str] = []
    level = 0
    current: List[str] = []

    for char in argument_str:
        if char == "(":
            level += 1
        elif char == ")":
            level = max(level - 1, 0)
        elif char == "," and level == 0:
            arg = "".join(current).strip()
            if arg:
                args.append(arg)
            current = []
            continue
        current.append(char)

    tail = "".join(current).strip()
    if tail:
        args.append(tail)

    return args

def _strip_quotes(token: str | None) -> Optional[str]:
    if token is None:
        return None
    text = token.strip()
    if text in {"NA", "NULL", ""}:
        return None
    if (text.startswith("'") and text.endswith("'")) or (
        text.startswith('"') and text.endswith('"')
    ):
        text = text[1:-1]
    return text.encode("utf8").decode("unicode_escape")

def _maybe_float(value: object) -> Optional[float]:
    if value is None:
        return None
    try:
        return float(value)
    except (TypeError, ValueError):
        return None

def _default_aggregator_for_dtype(series: pd.Series) -> str:
    dtype = series.dtype
    if pd.api.types.is_bool_dtype(dtype):
        return "sum"
    if pd.api.types.is_numeric_dtype(dtype):
        return "median"
    return "first"

def _maybe_int(value: object) -> Optional[int]:
    if value is None:
        return None
    try:
        return int(value)
    except (TypeError, ValueError):
        return None

def _maybe_timedelta(value: object) -> Optional[pd.Timedelta]:
    if value in (None, False, ""):
        return None
    if isinstance(value, pd.Timedelta):
        return value
    try:
        return pd.to_timedelta(value)
    except (TypeError, ValueError):
        return None

def _parse_literal(token: str):
    raw = token.strip()
    if raw in {"TRUE", "True"}:
        return True
    if raw in {"FALSE", "False"}:
        return False
    if raw in {"NA", "NA_real_", "NA_integer_", "NA_character_"}:
        return pd.NA
    if raw in {"NULL", "null"}:
        return None
    # æ”¯æŒåå¼•å·ï¼ˆRè¯­è¨€ä¸­ç”¨äºæ ‡è¯†ç¬¦ï¼‰
    if raw.startswith("`") and raw.endswith("`"):
        # å»æ‰åå¼•å·ï¼Œç„¶åå°è¯•è§£æä¸ºæ•°å­—æˆ–è¿”å›å­—ç¬¦ä¸²
        raw = raw[1:-1]
        try:
            # ä¼˜å…ˆå°è¯•æ•´æ•°ï¼Œå¦‚æœå¤±è´¥å†å°è¯•æµ®ç‚¹æ•°
            if "." not in raw:
                return int(raw)
            return float(raw)
        except ValueError:
            return raw
    if (raw.startswith("'") and raw.endswith("'")) or (raw.startswith('"') and raw.endswith('"')):
        return _strip_quotes(raw)
    if raw.endswith("L"):
        raw = raw[:-1]
    try:
        # ä¼˜å…ˆå°è¯•æ•´æ•°ï¼Œå¦‚æœå¤±è´¥å†å°è¯•æµ®ç‚¹æ•°
        if "." not in raw:
            return int(raw)
        return float(raw)
    except ValueError:
        return raw

# åˆ«å - ä¸ºäº†å…¼å®¹æ€§
Concept = ConceptDefinition  # Concept ç±»åˆ«åï¼ŒæŒ‡å‘ ConceptDefinition

def load_dictionary(src_name: Optional[str] = None, include_sofa2: bool = False) -> ConceptDictionary:
    """
    åŠ è½½æ¦‚å¿µå­—å…¸ - å…¼å®¹å‡½æ•°
    
    Args:
        src_name: æ•°æ®æºåç§°ï¼ˆå¯é€‰ï¼‰
        include_sofa2: æ˜¯å¦åŒ…å« SOFA-2 æ¦‚å¿µå­—å…¸
        
    Returns:
        ConceptDictionary å®ä¾‹
    """
    from .resources import load_dictionary as _load_dictionary

    # å½“å‰å®ç°ä¸æ ¹æ®æ•°æ®æºè¿‡æ»¤æ¦‚å¿µï¼Œä½†ä¿ç•™å‚æ•°ä»¥å…¼å®¹æ—¢æœ‰è°ƒç”¨
    return _load_dictionary(include_sofa2=include_sofa2)
