"""
ä¼˜åŒ–çš„APIæ¥å£

æä¾›é«˜æ€§èƒ½çš„æ¦‚å¿µåŠ è½½åŠŸèƒ½ï¼Œé›†æˆåˆ—è£å‰ªå’Œæ™ºèƒ½è¿‡æ»¤
"""

from __future__ import annotations

import logging
import time
from pathlib import Path
from typing import List, Union, Optional, Dict, Any
import pandas as pd

from .optimized_loader import get_optimized_datasource, OptimizedLoaderFactory
from .concept import ConceptDictionary, ConceptResolver
from .base import BaseICULoader
from .cache_manager import get_cache_manager
from .datasource import FilterSpec, FilterOp

logger = logging.getLogger(__name__)


class OptimizedConceptLoader(BaseICULoader):
    """ä¼˜åŒ–çš„æ¦‚å¿µåŠ è½½å™¨"""

    def __init__(
        self,
        data_path: Optional[Union[str, Path]] = None,
        database: Optional[str] = None,
        dict_path: Optional[Union[str, Path, List[Union[str, Path]]]] = None,
        use_sofa2: bool = False,
        verbose: bool = False,
        enable_optimizations: bool = True,
        benchmark_mode: bool = False
    ):
        """
        åˆå§‹åŒ–ä¼˜åŒ–æ¦‚å¿µåŠ è½½å™¨

        Args:
            data_path: æ•°æ®è·¯å¾„
            database: æ•°æ®åº“ç±»å‹
            dict_path: å­—å…¸è·¯å¾„
            use_sofa2: æ˜¯å¦ä½¿ç”¨SOFA2å­—å…¸
            verbose: è¯¦ç»†æ—¥å¿—
            enable_optimizations: å¯ç”¨ä¼˜åŒ–
            benchmark_mode: åŸºå‡†æµ‹è¯•æ¨¡å¼
        """
        super().__init__(data_path, database, dict_path, use_sofa2, verbose)

        self.enable_optimizations = enable_optimizations
        self.benchmark_mode = benchmark_mode

        if enable_optimizations:
            # æ›¿æ¢æ•°æ®æºä¸ºä¼˜åŒ–ç‰ˆæœ¬
            self.datasource = get_optimized_datasource(
                database=self.database,
                data_path=self.data_path,
                enable_column_pruning=True,
                enable_itemid_filtering=True
            )

        self.benchmark_results = []

    def load_concepts_optimized(
        self,
        concepts: Union[str, List[str]],
        patient_ids: Optional[Union[List, Dict]] = None,
        interval: Optional[Union[str, pd.Timedelta]] = None,
        win_length: Optional[Union[str, pd.Timedelta]] = None,
        aggregate: Optional[Union[str, Dict]] = None,
        keep_components: bool = False,
        use_sofa2: bool = False,
        merge: bool = True,
        verbose: bool = False,
        benchmark: bool = False,
        **kwargs
    ) -> Union[pd.DataFrame, Dict[str, pd.DataFrame]]:
        """
        ä¼˜åŒ–çš„æ¦‚å¿µåŠ è½½æ–¹æ³•

        Args:
            concepts: æ¦‚å¿µåç§°æˆ–åˆ—è¡¨
            patient_ids: æ‚£è€…ID
            interval: æ—¶é—´é—´éš”
            win_length: çª—å£é•¿åº¦
            aggregate: èšåˆæ–¹å¼
            keep_components: ä¿ç•™ç»„ä»¶
            use_sofa2: ä½¿ç”¨SOFA2
            merge: æ˜¯å¦åˆå¹¶ç»“æœ
            verbose: è¯¦ç»†æ—¥å¿—
            benchmark: æ˜¯å¦è¿›è¡ŒåŸºå‡†æµ‹è¯•
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            åŠ è½½çš„æ¦‚å¿µæ•°æ®
        """
        if not self.enable_optimizations:
            # å›é€€åˆ°åŸå§‹æ–¹æ³•
            return self.load_concepts(
                concepts=concepts,
                patient_ids=patient_ids,
                interval=interval,
                win_length=win_length,
                aggregate=aggregate,
                keep_components=keep_components,
                use_sofa2=use_sofa2,
                merge=merge,
                verbose=verbose,
                **kwargs
            )

        start_time = time.time()

        if verbose:
            logger.info(f"ğŸš€ å¼€å§‹ä¼˜åŒ–åŠ è½½æ¦‚å¿µ: {concepts}")

        # æ ‡å‡†åŒ–æ¦‚å¿µåˆ—è¡¨
        if isinstance(concepts, str):
            concepts = [concepts]

        # å‡†å¤‡æ‚£è€…ID
        if patient_ids is None:
            patient_ids_list = None
        elif isinstance(patient_ids, dict):
            patient_ids_list = list(patient_ids.values())[0] if patient_ids else None
        else:
            patient_ids_list = patient_ids

        # åŠ è½½æ¦‚å¿µæ•°æ®
        results = {}
        for concept in concepts:
            if benchmark:
                concept_start = time.time()

            try:
                # ä½¿ç”¨ä¼˜åŒ–çš„æ•°æ®æºåŠ è½½
                concept_data = self._load_single_concept_optimized(
                    concept=concept,
                    patient_ids=patient_ids_list,
                    interval=interval,
                    win_length=win_length,
                    aggregate=aggregate,
                    use_sofa2=use_sofa2
                )

                results[concept] = concept_data

                if benchmark:
                    concept_time = time.time() - concept_start
                    self.benchmark_results.append({
                        'concept': concept,
                        'load_time': concept_time,
                        'rows': len(concept_data) if hasattr(concept_data, '__len__') else 0,
                        'patient_count': len(patient_ids_list) if patient_ids_list else 0
                    })

            except Exception as e:
                logger.error(f"âŒ åŠ è½½æ¦‚å¿µå¤±è´¥ {concept}: {e}")
                results[concept] = pd.DataFrame()

        # åˆå¹¶ç»“æœ
        if merge and len(results) > 1:
            # ç®€å•åˆå¹¶æ‰€æœ‰ç»“æœ
            all_dataframes = []
            for concept_name, df in results.items():
                if not df.empty:
                    # æ·»åŠ æ¦‚å¿µååˆ—ä»¥åŒºåˆ†ä¸åŒæ¦‚å¿µ
                    df_copy = df.copy()
                    df_copy['concept'] = concept_name
                    all_dataframes.append(df_copy)

            if all_dataframes:
                final_result = pd.concat(all_dataframes, ignore_index=True)
            else:
                final_result = pd.DataFrame()
        elif merge and len(results) == 1:
            final_result = list(results.values())[0]
        else:
            final_result = results

        total_time = time.time() - start_time
        if verbose:
            logger.info(f"âœ… ä¼˜åŒ–åŠ è½½å®Œæˆ: {total_time:.2f}ç§’")

        return final_result

    def _load_single_concept_optimized(
        self,
        concept: str,
        patient_ids: Optional[List],
        interval: Optional[Union[str, pd.Timedelta]],
        win_length: Optional[Union[str, pd.Timedelta]],
        aggregate: Optional[Union[str, Dict]],
        use_sofa2: bool
    ) -> pd.DataFrame:
        """
        åŠ è½½å•ä¸ªä¼˜åŒ–æ¦‚å¿µ

        Args:
            concept: æ¦‚å¿µåç§°
            patient_ids: æ‚£è€…IDåˆ—è¡¨
            interval: æ—¶é—´é—´éš”
            win_length: çª—å£é•¿åº¦
            aggregate: èšåˆæ–¹å¼
            use_sofa2: ä½¿ç”¨SOFA2

        Returns:
            æ¦‚å¿µæ•°æ®
        """
        # è·å–æ¦‚å¿µå®šä¹‰
        concept_info = self.concept_resolver.dictionary.get(concept)
        if not concept_info:
            logger.warning(f"âš ï¸  æ¦‚å¿µæœªæ‰¾åˆ°: {concept}")
            return pd.DataFrame()

        # åˆ¤æ–­æ˜¯å¦ä¸ºSOFAç›¸å…³æ¦‚å¿µ
        is_sofa_concept = any(sofa_comp in concept.lower()
                            for sofa_comp in ['sofa', 'resp', 'coag', 'liver', 'cardio', 'cns', 'renal'])

        # åŠ è½½æ•°æ®
        data_frames = []
        for db_name, sources in concept_info.sources.items():
            # åªå¤„ç†å½“å‰æ•°æ®åº“çš„æº
            if db_name != self.database:
                continue

            for i, source in enumerate(sources):
                # è¶…è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
                logger.debug(f"ğŸ” å¤„ç†æº{i}: ç±»å‹={type(source)}, å†…å®¹={repr(source)}")

                # å¢å¼ºçš„ç±»å‹æ£€æŸ¥å’Œé”™è¯¯å¤„ç†
                try:
                    # æ£€æŸ¥sourcesåˆ—è¡¨æœ¬èº«çš„å†…å®¹
                    if isinstance(source, str):
                        logger.error(f"âŒ æº{i}æ˜¯å­—ç¬¦ä¸²: '{source}', è¿™è¡¨æ˜å­˜åœ¨åºåˆ—åŒ–é—®é¢˜")
                        logger.error(f"âŒ sourcesåˆ—è¡¨ç±»å‹: {type(sources)}")
                        logger.error(f"âŒ sourcesåˆ—è¡¨å†…å®¹: {[type(s) for s in sources]}")

                        # å°è¯•é‡å»ºæ¦‚å¿µæº
                        try:
                            logger.info(f"ğŸ”„ å°è¯•é‡å»ºæ¦‚å¿µå­—å…¸...")
                            # é‡æ–°è·å–æ¦‚å¿µä¿¡æ¯
                            fresh_concept_info = self.concept_resolver.dictionary.get(concept)
                            if fresh_concept_info and 'miiv' in fresh_concept_info.sources:
                                fresh_sources = fresh_concept_info.sources['miiv']
                                logger.info(f"ğŸ”„ é‡å»ºåçš„æº: {fresh_sources}")
                                if i < len(fresh_sources):
                                    source = fresh_sources[i]
                                    logger.info(f"ğŸ”„ ä½¿ç”¨é‡å»ºçš„æº{i}: {source}")
                        except Exception as rebuild_error:
                            logger.error(f"âŒ é‡å»ºå¤±è´¥: {rebuild_error}")

                        continue

                    if isinstance(source, dict):
                        table_name = source.get('table')
                        itemids = source.get('ids')
                        logger.debug(f"âœ… æº{i}æ˜¯å­—å…¸: table={table_name}, itemids={itemids}")
                    elif hasattr(source, 'table'):
                        table_name = source.table
                        itemids = source.ids if hasattr(source, 'ids') else None
                        logger.debug(f"âœ… æº{i}æ˜¯å¯¹è±¡: table={table_name}, itemids={itemids}")
                    else:
                        # å¤„ç†æ„å¤–çš„æ•°æ®ç±»å‹
                        logger.error(f"âŒ æ¦‚å¿µæº{i}ç±»å‹é”™è¯¯: {type(source)}")
                        logger.error(f"âŒ æº{i}å†…å®¹: {repr(source)}")
                        logger.error(f"âŒ æº{i}å±æ€§: {[attr for attr in dir(source) if not attr.startswith('_')]}")
                        continue

                    # éªŒè¯æå–çš„å€¼
                    if not table_name:
                        logger.warning(f"âš ï¸  æ— æ³•è·å–table_nameï¼Œè·³è¿‡æº{i}: {source}")
                        continue

                except Exception as e:
                    logger.error(f"âŒ å¤„ç†æ¦‚å¿µæº{i}æ—¶å‡ºé”™: {e}")
                    logger.error(f"âŒ é”™è¯¯ç±»å‹: {type(e)}")
                    logger.error(f"âŒ æº{i}ç±»å‹: {type(source)}")
                    logger.error(f"âŒ æº{i}å†…å®¹: {repr(source)}")

                    # æ‰“å°å®Œæ•´çš„å †æ ˆè·Ÿè¸ªç”¨äºè°ƒè¯•
                    import traceback
                    logger.error(f"âŒ å †æ ˆè·Ÿè¸ª: {traceback.format_exc()}")
                    continue

                # åˆ›å»ºè¿‡æ»¤å™¨
                filters = []
                if patient_ids:
                    # è·å–æ­£ç¡®çš„IDåˆ—å
                    id_col = self._get_id_column_for_table(table_name)
                    # ç›´æ¥ä½¿ç”¨FilterOpï¼Œé¿å…å¯¼å…¥é—®é¢˜
                    from pyricu.datasource import FilterSpec, FilterOp
                    filters.append(FilterSpec(id_col, FilterOp.IN, patient_ids))

                # ä½¿ç”¨ä¼˜åŒ–çš„æ•°æ®æºåŠ è½½
                try:
                    if hasattr(self.datasource, 'load_table_optimized'):
                        df = self.datasource.load_table_optimized(
                            table_name=table_name,
                            columns=None,  # è®©ä¼˜åŒ–å™¨å†³å®šéœ€è¦çš„åˆ—
                            filters=filters,
                            concept_name=concept if is_sofa_concept else None,
                            verbose=False
                        )
                    else:
                        # å›é€€åˆ°åŸå§‹æ–¹æ³•
                        df = self.datasource.load_table(
                            table_name=table_name,
                            filters=filters,
                            verbose=False
                        )

                    if not df.empty:
                        # åº”ç”¨itemidè¿‡æ»¤ï¼ˆå¦‚æœæ²¡æœ‰åœ¨åŠ è½½é˜¶æ®µåº”ç”¨ï¼‰
                        if itemids and 'itemid' in df.columns:
                            original_count = len(df)
                            df = df[df['itemid'].isin(itemids)]
                            filtered_count = len(df)

                            if filtered_count == 0 and original_count > 0:
                                logger.warning(f"âš ï¸  itemidè¿‡æ»¤åæ— æ•°æ®: {table_name}, æœŸæœ›itemids: {itemids}, å®é™…itemids: {sorted(df['itemid'].unique())}")
                            elif filtered_count < original_count:
                                logger.info(f"âœ… itemidè¿‡æ»¤: {table_name}, {original_count} â†’ {filtered_count} è¡Œ")

                        data_frames.append(df)
                    else:
                        if itemids:
                            logger.warning(f"âš ï¸  è¡¨ {table_name} ä¸ºç©ºï¼Œæ— æ³•åº”ç”¨itemidè¿‡æ»¤ï¼ŒæœŸæœ›itemids: {itemids}")

                except Exception as e:
                    logger.warning(f"âš ï¸  åŠ è½½è¡¨å¤±è´¥ {table_name}: {e}")

        if not data_frames:
            return pd.DataFrame()

        # åˆå¹¶æ•°æ®å¸§
        if len(data_frames) == 1:
            combined_data = data_frames[0]
        else:
            combined_data = pd.concat(data_frames, ignore_index=True)

        # åº”ç”¨æ—¶é—´çª—å£å’Œèšåˆ
        if interval or win_length or aggregate:
            combined_data = self._apply_time_processing(
                combined_data, interval, win_length, aggregate
            )

        return combined_data

    def _get_id_column_for_table(self, table_name: str) -> str:
        """è·å–è¡¨çš„IDåˆ—å"""
        id_mapping = {
            'chartevents': 'stay_id',
            'labevents': 'stay_id',
            'inputevents': 'stay_id',
            'outputevents': 'stay_id',
            'procedureevents': 'stay_id',
            'microbiologyevents': 'stay_id',
            'icustays': 'stay_id',
            'patients': 'subject_id'
        }
        return id_mapping.get(table_name, 'stay_id')

    def _apply_time_processing(
        self,
        df: pd.DataFrame,
        interval: Optional[Union[str, pd.Timedelta]],
        win_length: Optional[Union[str, pd.Timedelta]],
        aggregate: Optional[Union[str, Dict]]
    ) -> pd.DataFrame:
        """åº”ç”¨æ—¶é—´å¤„ç†å’Œèšåˆ"""
        # è¿™é‡Œåº”è¯¥å®ç°æ—¶é—´å¤„ç†é€»è¾‘
        # ä¸ºç®€åŒ–ï¼Œæš‚æ—¶è¿”å›åŸå§‹æ•°æ®
        return df

    def benchmark_concepts(
        self,
        concepts: List[str],
        patient_ids: List[int],
        **kwargs
    ) -> Dict[str, Any]:
        """
        å¯¹æ¦‚å¿µåŠ è½½è¿›è¡ŒåŸºå‡†æµ‹è¯•

        Args:
            concepts: æ¦‚å¿µåˆ—è¡¨
            patient_ids: æ‚£è€…IDåˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            åŸºå‡†æµ‹è¯•ç»“æœ
        """
        logger.info(f"ğŸ å¼€å§‹æ¦‚å¿µåŠ è½½åŸºå‡†æµ‹è¯•")

        # æµ‹è¯•ä¼˜åŒ–ç‰ˆæœ¬
        self.benchmark_results = []
        optimized_start = time.time()

        optimized_result = self.load_concepts_optimized(
            concepts=concepts,
            patient_ids=patient_ids,
            benchmark=True,
            **kwargs
        )

        optimized_time = time.time() - optimized_start

        # æ±‡æ€»ç»“æœ
        benchmark_summary = {
            'concepts': concepts,
            'patient_count': len(patient_ids),
            'total_time': optimized_time,
            'concept_details': self.benchmark_results,
            'total_rows': sum(detail['rows'] for detail in self.benchmark_results)
        }

        logger.info(f"ğŸ“Š åŸºå‡†æµ‹è¯•å®Œæˆ:")
        logger.info(f"   æ¦‚å¿µæ•°: {len(concepts)}")
        logger.info(f"   æ‚£è€…æ•°: {len(patient_ids)}")
        logger.info(f"   æ€»æ—¶é—´: {optimized_time:.2f}ç§’")
        logger.info(f"   æ€»è¡Œæ•°: {benchmark_summary['total_rows']:,}")

        return benchmark_summary


# å…¨å±€ä¼˜åŒ–åŠ è½½å™¨å®ä¾‹
_global_optimized_loader = None


def get_optimized_loader(
    data_path: Optional[Union[str, Path]] = None,
    database: Optional[str] = None,
    **kwargs
) -> OptimizedConceptLoader:
    """
    è·å–å…¨å±€ä¼˜åŒ–åŠ è½½å™¨å®ä¾‹

    Args:
        data_path: æ•°æ®è·¯å¾„
        database: æ•°æ®åº“ç±»å‹
        **kwargs: å…¶ä»–å‚æ•°

    Returns:
        ä¼˜åŒ–åŠ è½½å™¨å®ä¾‹
    """
    global _global_optimized_loader

    if _global_optimized_loader is None:
        _global_optimized_loader = OptimizedConceptLoader(
            data_path=data_path,
            database=database,
            **kwargs
        )

    return _global_optimized_loader


def load_concepts_optimized(
    concepts: Union[str, List[str]],
    patient_ids: Optional[Union[List, Dict]] = None,
    database: Optional[str] = None,
    data_path: Optional[Union[str, Path]] = None,
    interval: Optional[Union[str, pd.Timedelta]] = None,
    win_length: Optional[Union[str, pd.Timedelta]] = None,
    aggregate: Optional[Union[str, Dict]] = None,
    keep_components: bool = False,
    use_sofa2: bool = False,
    merge: bool = True,
    verbose: bool = False,
    benchmark: bool = False,
    **kwargs
) -> Union[pd.DataFrame, Dict[str, pd.DataFrame]]:
    """
    ä¼˜åŒ–çš„æ¦‚å¿µåŠ è½½å‡½æ•°ï¼ˆä¾¿æ·æ¥å£ï¼‰

    Args:
        concepts: æ¦‚å¿µåç§°æˆ–åˆ—è¡¨
        patient_ids: æ‚£è€…ID
        database: æ•°æ®åº“ç±»å‹
        data_path: æ•°æ®è·¯å¾„
        interval: æ—¶é—´é—´éš”
        win_length: çª—å£é•¿åº¦
        aggregate: èšåˆæ–¹å¼
        keep_components: ä¿ç•™ç»„ä»¶
        use_sofa2: ä½¿ç”¨SOFA2
        merge: æ˜¯å¦åˆå¹¶ç»“æœ
        verbose: è¯¦ç»†æ—¥å¿—
        benchmark: æ˜¯å¦è¿›è¡ŒåŸºå‡†æµ‹è¯•
        **kwargs: å…¶ä»–å‚æ•°

    Returns:
        åŠ è½½çš„æ¦‚å¿µæ•°æ®
    """
    loader = get_optimized_loader(data_path, database)
    return loader.load_concepts_optimized(
        concepts=concepts,
        patient_ids=patient_ids,
        interval=interval,
        win_length=win_length,
        aggregate=aggregate,
        keep_components=keep_components,
        use_sofa2=use_sofa2,
        merge=merge,
        verbose=verbose,
        benchmark=benchmark,
        **kwargs
    )


# SOFAä¸“ç”¨çš„ä¾¿æ·å‡½æ•°
def load_sofa_optimized(
    patient_ids: Optional[Union[List, Dict]] = None,
    database: Optional[str] = None,
    data_path: Optional[Union[str, Path]] = None,
    keep_components: bool = False,
    use_sofa2: bool = False,
    verbose: bool = False,
    **kwargs
) -> pd.DataFrame:
    """
    ä¼˜åŒ–çš„SOFAè¯„åˆ†åŠ è½½

    Args:
        patient_ids: æ‚£è€…ID
        database: æ•°æ®åº“ç±»å‹
        data_path: æ•°æ®è·¯å¾„
        keep_components: ä¿ç•™ç»„ä»¶
        use_sofa2: ä½¿ç”¨SOFA2
        verbose: è¯¦ç»†æ—¥å¿—
        **kwargs: å…¶ä»–å‚æ•°

    Returns:
        SOFAè¯„åˆ†æ•°æ®
    """
    concept = 'sofa2' if use_sofa2 else 'sofa'
    return load_concepts_optimized(
        concepts=concept,
        patient_ids=patient_ids,
        database=database,
        data_path=data_path,
        keep_components=keep_components,
        use_sofa2=use_sofa2,
        verbose=verbose,
        **kwargs
    )