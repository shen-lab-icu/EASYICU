"""Data loading utilities for ICU datasets."""

from __future__ import annotations

import enum
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Callable, Iterable, List, Mapping, MutableMapping, Optional

import pandas as pd

from .config import DataSourceConfig, DataSourceRegistry
from .table import ICUTable

# å…¨å±€è°ƒè¯•å¼€å…³ - è®¾ç½®ä¸º False å¯ä»¥å‡å°‘è¾“å‡º
DEBUG_MODE = False


class FilterOp(str, enum.Enum):
    """Supported filter operations for table loading."""

    EQ = "=="
    IN = "in"
    BETWEEN = "between"


@dataclass
class FilterSpec:
    """Declarative filter specification for table loading."""

    column: str
    op: FilterOp
    value: Any

    def apply(self, frame: pd.DataFrame) -> pd.DataFrame:
        if self.op == FilterOp.EQ:
            mask = frame[self.column] == self.value
            return frame.loc[mask].copy()
        if self.op == FilterOp.IN:
            if isinstance(self.value, str):
                candidate = [self.value]
            else:
                candidate = list(self.value)
            mask = frame[self.column].isin(candidate)
            return frame.loc[mask].copy()
        if self.op == FilterOp.BETWEEN:
            lower, upper = self.value
            mask = frame[self.column].between(lower, upper)
            return frame.loc[mask].copy()
        raise ValueError(f"Unsupported filter operation: {self.op}")


class ICUDataSource:
    """Lightweight facade that loads tables for a concrete dataset instance."""

    # å…¨å±€æ ¼å¼ä¼˜å…ˆçº§é…ç½®
    _global_format_priority: Optional[List[str]] = None

    @classmethod
    def set_format_priority(cls, priority: List[str]) -> None:
        """è®¾ç½®å…¨å±€æ–‡ä»¶æ ¼å¼ä¼˜å…ˆçº§
        
        Args:
            priority: æ ¼å¼åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åºï¼Œä¾‹å¦‚ ['parquet', 'fst', 'csv']
        
        Examples:
            >>> # ä¼˜å…ˆä½¿ç”¨ Parquetï¼ˆçº¯ Pythonï¼Œæ— éœ€ Rï¼‰
            >>> ICUDataSource.set_format_priority(['parquet', 'fst', 'csv'])
            >>> 
            >>> # åªä½¿ç”¨ Parquetï¼ˆè·³è¿‡ FSTï¼‰
            >>> ICUDataSource.set_format_priority(['parquet', 'csv'])
            >>> 
            >>> # ä¼˜å…ˆ FSTï¼ˆæ—§è¡Œä¸ºï¼Œéœ€è¦ R ç¯å¢ƒï¼‰
            >>> ICUDataSource.set_format_priority(['fst', 'parquet', 'csv'])
        """
        cls._global_format_priority = priority

    @classmethod
    def get_format_priority(cls) -> List[str]:
        """è·å–å½“å‰çš„æ ¼å¼ä¼˜å…ˆçº§é…ç½®
        
        Returns:
            æ ¼å¼åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
        """
        # 1. å¦‚æœè®¾ç½®äº†å…¨å±€ä¼˜å…ˆçº§ï¼Œä½¿ç”¨å…¨å±€é…ç½®
        if cls._global_format_priority is not None:
            return cls._global_format_priority
        
        # 2. æ£€æŸ¥ç¯å¢ƒå˜é‡
        import os
        env_priority = os.environ.get('PYRICU_FORMAT_PRIORITY')
        if env_priority:
            return [fmt.strip() for fmt in env_priority.split(',')]
        
        # 3. é»˜è®¤ä¼˜å…ˆçº§ï¼šParquet > FST > CSV
        # Parquet ä¼˜å…ˆå› ä¸ºï¼šçº¯ Pythonï¼Œæ— éœ€ Rï¼Œåˆ—å¼å­˜å‚¨ï¼Œå‹ç¼©å¥½
        return ['parquet', 'fst', 'csv']

    def __init__(
        self,
        config: DataSourceConfig,
        *,
        base_path: str | Path | None = None,
        table_sources: Optional[Mapping[str, Any]] = None,
        registry: Optional[DataSourceRegistry] = None,
        default_format: str = "parquet",
        enable_cache: bool = True,
        format_priority: Optional[List[str]] = None,
    ) -> None:
        """åˆå§‹åŒ–æ•°æ®æº
        
        Args:
            config: æ•°æ®æºé…ç½®
            base_path: æ•°æ®æ–‡ä»¶åŸºç¡€è·¯å¾„
            table_sources: è¡¨æ•°æ®æºæ˜ å°„ï¼ˆå¯é€‰ï¼‰
            registry: æ•°æ®æºæ³¨å†Œè¡¨ï¼ˆå¯é€‰ï¼‰
            default_format: é»˜è®¤æ ¼å¼ï¼ˆå·²åºŸå¼ƒï¼Œä½¿ç”¨ format_priorityï¼‰
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
            format_priority: æ–‡ä»¶æ ¼å¼ä¼˜å…ˆçº§ï¼ˆå¯é€‰ï¼‰ï¼Œä¾‹å¦‚ ['parquet', 'fst', 'csv']
                           å¦‚æœæœªæŒ‡å®šï¼Œä½¿ç”¨å…¨å±€é…ç½®æˆ–ç¯å¢ƒå˜é‡
        """
        self.config = config
        self.base_path = Path(base_path) if base_path else None
        self._table_sources: MutableMapping[str, Any] = dict(table_sources or {})
        self.default_format = default_format
        self.registry = registry
        self.enable_cache = enable_cache
        self._table_cache: dict = {}  # ç¼“å­˜å·²åŠ è½½çš„åŸå§‹è¡¨æ•°æ®
        self.format_priority = format_priority or self.get_format_priority()

    def register_table_source(self, table: str, source: Any) -> None:
        """Register a callable/file path used to load ``table``."""
        self._table_sources[table] = source
    
    def clear_cache(self) -> None:
        """æ¸…é™¤è¡¨ç¼“å­˜,é‡Šæ”¾å†…å­˜ã€‚"""
        self._table_cache.clear()
    
    def get_cache_info(self) -> dict:
        """è·å–ç¼“å­˜ä¿¡æ¯ã€‚"""
        total_size = sum(df.memory_usage(deep=True).sum() for df in self._table_cache.values())
        return {
            'cached_tables': len(self._table_cache),
            'memory_mb': total_size / (1024 * 1024)
        }

    def load_table(
        self,
        table_name: str,
        *,
        columns: Optional[Iterable[str]] = None,
        filters: Optional[Iterable[FilterSpec]] = None,
        verbose: bool = False,
    ) -> ICUTable:
        """Load and wrap a table according to the stored configuration."""
        
        table_cfg = self.config.get_table(table_name)
        
        # æå– patient_ids è¿‡æ»¤å™¨ç”¨äºåˆ†åŒºé¢„è¿‡æ»¤
        patient_ids_filter = None
        if filters:
            for spec in filters:
                # æ”¯æŒå„æ•°æ®åº“çš„IDåˆ—å
                id_columns = ['subject_id', 'icustay_id', 'hadm_id', 'stay_id',  # MIMIC
                             'admissionid', 'patientid',  # AUMC
                             'patientunitstayid',  # eICU
                             'patientid']  # HiRID
                if spec.op == FilterOp.IN and spec.column in id_columns:
                    patient_ids_filter = spec
                    # åªåœ¨verboseæ¨¡å¼ä¸‹è¾“å‡ºï¼Œä¸”åªè¾“å‡ºä¸€æ¬¡
                    if verbose:
                        cache_key = f"_filter_logged_{table_name}"
                        if not hasattr(self, cache_key) or not getattr(self, cache_key, False):
                            if DEBUG_MODE: print(f"   ğŸ¯ æ£€æµ‹åˆ°æ‚£è€…IDè¿‡æ»¤å™¨: {len(spec.value)} ä¸ªæ‚£è€…, åˆ—={spec.column}")
                            setattr(self, cache_key, True)
                    break
        
        frame = self._load_raw_frame(table_name, columns, patient_ids_filter=patient_ids_filter)
        
        if filters:
            for spec in filters:
                frame = spec.apply(frame)
        else:
            frame = frame.copy()

        defaults = table_cfg.defaults
        id_columns = (
            [defaults.id_var]
            if defaults.id_var and defaults.id_var in frame.columns
            else []
        )
        index_column = defaults.index_var if defaults.index_var in frame.columns else None
        time_columns = [
            column for column in defaults.time_vars if column in frame.columns
        ]
        value_column = defaults.val_var if defaults.val_var in frame.columns else None
        unit_column = defaults.unit_var if defaults.unit_var in frame.columns else None

        time_like_cols = set(time_columns)
        if index_column:
            time_like_cols.add(index_column)
        
        # ğŸ”§ AUMCç‰¹æ®Šå¤„ç†ï¼šæ—¶é—´åˆ—æ˜¯æ¯«ç§’,éœ€è¦è½¬æ¢ä¸ºåˆ†é’Ÿ (å‚è€ƒR ricuçš„ms_as_mins)
        # R ricu: ms_as_mins <- function(x) min_as_mins(as.integer(x / 6e4))
        # è¿™æ ·å¤„ç†å,AUMCçš„æ—¶é—´å•ä½ä¸å…¶ä»–æ•°æ®åº“ä¸€è‡´(éƒ½æ˜¯åˆ†é’Ÿ)
        if self.config.name == 'aumc':
            for column in time_like_cols:
                if column in frame.columns and pd.api.types.is_numeric_dtype(frame[column]):
                    # å°†æ¯«ç§’è½¬æ¢ä¸ºåˆ†é’Ÿ: ms / 60000
                    frame[column] = (frame[column] / 60000.0).astype('float64')
        
        for column in time_like_cols:
            # åªæœ‰å½“åˆ—å­˜åœ¨ä¸”ä¸æ˜¯numericç±»å‹æ—¶æ‰è½¬æ¢
            # å¦‚æœå·²ç»æ˜¯numericï¼Œå¯èƒ½æ˜¯å·²ç»å¯¹é½è¿‡çš„å°æ—¶æ•°
            if column in frame.columns:
                frame[column] = _coerce_datetime(frame[column])

        return ICUTable(
            data=frame,
            id_columns=id_columns,
            index_column=index_column,
            value_column=value_column,
            unit_column=unit_column,
            time_columns=time_columns,
        )

    def _load_raw_frame(
        self,
        table_name: str,
        columns: Optional[Iterable[str]],
        patient_ids_filter: Optional[FilterSpec] = None,
    ) -> pd.DataFrame:
        # ç¼“å­˜é”®ï¼šè¡¨å + åˆ—é›†åˆ + æ‚£è€…è¿‡æ»¤å™¨
        # å¯¹äºæœ‰æ‚£è€…è¿‡æ»¤å™¨çš„æƒ…å†µ,ä¹Ÿä½¿ç”¨ç¼“å­˜(å› ä¸ºåŒä¸€æ‰¹æ‚£è€…ä¼šè¢«å¤šä¸ªæ¦‚å¿µä½¿ç”¨)
        if patient_ids_filter:
            # ä½¿ç”¨frozensetæ¥ç¡®ä¿æ‚£è€…IDåˆ—è¡¨çš„å“ˆå¸Œä¸€è‡´æ€§
            patient_ids_set = frozenset(patient_ids_filter.value) if not isinstance(patient_ids_filter.value, str) else frozenset([patient_ids_filter.value])
            cache_key = (table_name, tuple(sorted(columns)) if columns else None, patient_ids_filter.column, patient_ids_set)
        else:
            cache_key = (table_name, tuple(sorted(columns)) if columns else None, None, None)
        
        # æ£€æŸ¥ç¼“å­˜
        if self.enable_cache and cache_key in self._table_cache:
            return self._table_cache[cache_key].copy()
        
        loader = self._table_sources.get(table_name)
        if loader is None:
            loader = self._resolve_loader_from_disk(table_name)
        if loader is None:
            # å¯¹äºmiivæ•°æ®æºï¼Œå¦‚æœè¡¨åœ¨é…ç½®ä¸­å®šä¹‰äº†ä½†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºDataFrame
            # è¿™å…è®¸åœ¨demoæ•°æ®ä¸­ç¼ºå°‘æŸäº›è¡¨æ—¶ç»§ç»­è¿è¡Œ
            if self.config.name == 'miiv' and table_name in self.config.tables:
                # è¿”å›ç©ºDataFrameï¼Œä¿æŒä¸é…ç½®ä¸­è¡¨ç»“æ„ä¸€è‡´çš„åˆ—
                table_cfg = self.config.get_table(table_name)
                defaults = table_cfg.defaults
                # å°è¯•ä»é…ç½®ä¸­è·å–é¢„æœŸçš„åˆ—
                expected_cols = []
                if defaults.id_var:
                    expected_cols.append(defaults.id_var)
                if defaults.index_var:
                    expected_cols.append(defaults.index_var)
                if defaults.val_var:
                    expected_cols.append(defaults.val_var)
                if defaults.unit_var:
                    expected_cols.append(defaults.unit_var)
                if defaults.time_vars:
                    expected_cols.extend(defaults.time_vars)
                
                # è¿”å›ç©ºDataFrameï¼Œé¿å…æŠ›å‡ºé”™è¯¯
                return pd.DataFrame(columns=expected_cols if expected_cols else ['index'])
            
            raise KeyError(
                f"No table source registered for '{table_name}' "
                f"in data source '{self.config.name}'"
            )
        if callable(loader):
            frame = loader()
        else:
            frame = self._read_file(Path(loader), columns, patient_ids_filter=patient_ids_filter)

        if columns is not None:
            missing = set(columns) - set(frame.columns)
            if missing:
                raise KeyError(
                    f"Columns {sorted(missing)} not found in table '{table_name}'"
                )
            frame = frame[list(columns)]
        
        # ç¼“å­˜åŠ è½½çš„æ•°æ®ï¼ˆä½¿ç”¨ä¹‹å‰æ„å»ºçš„cache_keyï¼‰
        if self.enable_cache:
            self._table_cache[cache_key] = frame.copy()
        
        return frame

    def _resolve_loader_from_disk(self, table_name: str) -> Optional[Callable[[], pd.DataFrame] | Path]:
        if not self.base_path:
            return None
        
        table_cfg = self.config.get_table(table_name)
        explicit = table_cfg.first_file()
        if explicit:
            explicit_path = self.base_path / explicit
            # Only use explicit path if it actually exists
            if explicit_path.exists():
                return explicit_path
        
        # MIMIC-IV æ–‡ä»¶åæ˜ å°„: é…ç½®ä¸­çš„è¡¨å -> æ–‡ä»¶ç³»ç»Ÿä¸­çš„å®é™…æ–‡ä»¶å
        # å› ä¸º MIMIC-IV æ”¹äº†è¡¨å,ä½†é…ç½®æ–‡ä»¶è¿˜æ˜¯ç”¨çš„æ—§å
        # MIMIC-IV å°† MIMIC-III çš„ä¸¤ä¸ªè¡¨åˆå¹¶äº†:
        # - procedureevents_mv -> procedureevents
        # - inputevents_cv å’Œ inputevents_mv -> inputevents
        # æ³¨æ„ï¼šå¯¹äºmiivæ•°æ®æºï¼Œæ¦‚å¿µå­—å…¸ä¸­ç›´æ¥ä½¿ç”¨procedureeventså’Œinputevents
        # ä½†å¦‚æœæ•°æ®æºé…ç½®ä¸­æ²¡æœ‰å®šä¹‰è¿™äº›è¡¨ï¼Œéœ€è¦ä»æ—§è¡¨åæ˜ å°„

        config_to_file_mappings = {
            'procedureevents_mv': 'procedureevents',  # é…ç½®å -> æ–‡ä»¶å
            'inputevents_mv': 'inputevents',
            'inputevents_cv': 'inputevents',  # MIMIC-IV åˆå¹¶äº†è¿™ä¸¤ä¸ªè¡¨
        }
        
        # å¯¹äºmiivæ•°æ®æºï¼Œå¦‚æœè¯·æ±‚çš„è¡¨åœ¨é…ç½®ä¸­ä¸å­˜åœ¨ï¼Œå°è¯•ä»æ˜ å°„ä¸­æŸ¥æ‰¾
        # ä¾‹å¦‚ï¼šæ¦‚å¿µå­—å…¸è¯·æ±‚procedureeventsï¼Œä½†é…ç½®ä¸­å¯èƒ½åªæœ‰procedureevents_mvçš„å®šä¹‰
        if self.config.name == 'miiv':
            # åå‘æ˜ å°„ï¼šå¦‚æœè¯·æ±‚çš„è¡¨åä¸åœ¨é…ç½®ä¸­ï¼Œå°è¯•æŸ¥æ‰¾å¯¹åº”çš„æ—§è¡¨å
            reverse_mapping = {
                'procedureevents': 'procedureevents_mv',
                'inputevents': 'inputevents_mv',  # ä¼˜å…ˆä½¿ç”¨inputevents_mv
            }
            if table_name not in self.config.tables and table_name in reverse_mapping:
                # ä½¿ç”¨æ˜ å°„åçš„è¡¨åæŸ¥æ‰¾æ–‡ä»¶
                mapped_table = reverse_mapping[table_name]
                file_base_name = config_to_file_mappings.get(mapped_table, table_name)
            else:
                file_base_name = config_to_file_mappings.get(table_name, table_name)
        else:
            # è·å–å®é™…è¦æŸ¥æ‰¾çš„æ–‡ä»¶å
            file_base_name = config_to_file_mappings.get(table_name, table_name)

            # ä»è¡¨é…ç½®ä¸­è·å–å®é™…çš„æ–‡ä»¶åï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if table_name in self.config.tables:
                table_config = self.config.tables[table_name]
                if hasattr(table_config, 'files') and table_config.files:
                    # è·å–ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„è·¯å¾„ï¼Œå»æ‰æ‰©å±•å
                    file_info = table_config.files[0]
                    if isinstance(file_info, dict) and 'path' in file_info:
                        file_path = file_info['path']
                    elif hasattr(file_info, 'path'):
                        file_path = file_info.path
                    else:
                        file_path = str(file_info)
                    # å»æ‰æ‰©å±•åï¼Œè·å–åŸºç¡€æ–‡ä»¶åï¼ˆå¤„ç†å¤åˆæ‰©å±•åå¦‚.csv.gzï¼‰
                    parts = file_path.split('.')
                    if len(parts) >= 2:
                        # å¤„ç†å¤åˆæ‰©å±•åå¦‚ .csv.gz
                        if parts[-1] == 'gz' and len(parts) >= 3 and parts[-2] == 'csv':
                            file_base_name = '.'.join(parts[:-2])
                        else:
                            file_base_name = '.'.join(parts[:-1])
                    else:
                        file_base_name = file_path
        
        # Try different formats in order of preference
        # 1. FST (R ricu format) - highest priority for existing ricu data
        # Try both original case and lowercase
        for name in [file_base_name, file_base_name.lower()]:
            fst_candidate = self.base_path / f"{name}.fst"
            if fst_candidate.exists():
                return fst_candidate
        
        # 2. Parquet (Python default)
        for name in [file_base_name, file_base_name.lower(), table_name, table_name.lower()]:
            candidate = self.base_path / f"{name}.{self.default_format}"
            if candidate.exists():
                return candidate
        
        # 3. CSV (fallback)
        if self.base_path is not None:
            for name in [table_name, table_name.lower()]:
                csv_candidate = self.base_path / f"{name}.csv"
                if csv_candidate.exists():
                    return csv_candidate
                # Also try .csv.gz
                csv_gz_candidate = self.base_path / f"{name}.csv.gz"
                if csv_gz_candidate.exists():
                    return csv_gz_candidate
        
        # 4. Check subdirectory for partitioned data (common in hirid observations)
        if self.base_path is not None:
            for name in [table_name, table_name.lower()]:
                subdir = self.base_path / name
            if subdir.is_dir():
                # Look for FST files first
                fst_files = list(subdir.glob("*.fst"))
                if fst_files:
                    return subdir  # Return directory, will handle in _read_file
                # Then Parquet
                parquet_files = list(subdir.glob("*.parquet")) + list(subdir.glob("*.pq"))
                if parquet_files:
                    return subdir
        
        return None

    def _read_file(self, path: Path, columns: Optional[Iterable[str]], patient_ids_filter: Optional[FilterSpec] = None) -> pd.DataFrame:
        # Handle directory (partitioned data)
        if path.is_dir():
            return self._read_partitioned_data(path, columns, patient_ids_filter=patient_ids_filter)
        
        suffix = path.suffix.lower()
        
        # Handle .csv.gz files (compressed CSV)
        if str(path).endswith('.csv.gz') or str(path).endswith('.CSV.GZ'):
            return pd.read_csv(path, compression='gzip', usecols=list(columns) if columns else None)
        
        # Handle regular formats
        if suffix == ".csv":
            return pd.read_csv(path, usecols=list(columns) if columns else None)
        if suffix == ".gz":
            # Try to read as compressed CSV
            return pd.read_csv(path, compression='gzip', usecols=list(columns) if columns else None)
        if suffix in {".parquet", ".pq"}:
            # ğŸš€ ä½¿ç”¨PyArrowè¿‡æ»¤å™¨ä¼˜åŒ–å¤§æ–‡ä»¶è¯»å–
            if patient_ids_filter:
                try:
                    import pyarrow.parquet as pq
                    import pyarrow as pa
                    # ä½¿ç”¨ DNF (Disjunctive Normal Form) æ ¼å¼ï¼Œå…¼å®¹æ€§æ›´å¥½
                    target_ids = patient_ids_filter.value if isinstance(patient_ids_filter.value, list) else [patient_ids_filter.value]
                    
                    # ä½¿ç”¨PyArrowè¯»å–å¹¶è¿‡æ»¤ - ä½¿ç”¨ DNF æ ¼å¼
                    df = pq.read_table(
                        path,
                        columns=list(columns) if columns else None,
                        filters=[[( patient_ids_filter.column, 'in', target_ids)]]
                    ).to_pandas()
                except (ImportError, Exception) as e:
                    # å¦‚æœPyArrowè¿‡æ»¤å¤±è´¥ï¼Œå›é€€åˆ°pandasåè¿‡æ»¤
                    df = pd.read_parquet(path, columns=list(columns) if columns else None)
                    if patient_ids_filter.column in df.columns:
                        target_ids = set(patient_ids_filter.value) if isinstance(patient_ids_filter.value, list) else {patient_ids_filter.value}
                        df = df[df[patient_ids_filter.column].isin(target_ids)]
            else:
                df = pd.read_parquet(path, columns=list(columns) if columns else None)
            
            # å¤„ç†é‡å¤åˆ—åï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if df.columns.duplicated().any():
                import pandas.io.common
                df.columns = pandas.io.common.dedup_names(df.columns, is_potential_multiindex=False)
            return df
        if suffix == ".feather":
            return pd.read_feather(path, columns=list(columns) if columns else None)
        if suffix == ".fst":
            return self._read_fst_file(path, columns)
        
        raise ValueError(f"Unsupported file format for table loading: {path.suffix}")
    
    def _read_partitioned_data(self, directory: Path, columns: Optional[Iterable[str]], patient_ids_filter: Optional[FilterSpec] = None) -> pd.DataFrame:
        """Read partitioned data from a directory, respecting format priority."""
        
        # æŒ‰ä¼˜å…ˆçº§é¡ºåºæŸ¥æ‰¾æ–‡ä»¶
        format_map = {
            'parquet': lambda: sorted(directory.glob("*.parquet")) + sorted(directory.glob("*.pq")),
            'fst': lambda: sorted(directory.glob("*.fst")),
            'csv': lambda: sorted(directory.glob("*.csv")) + sorted(directory.glob("*.csv.gz")),
        }
        
        # å°è¯•æŒ‰ä¼˜å…ˆçº§è¯»å–
        for fmt in self.format_priority:
            if fmt not in format_map:
                continue
                
            files = format_map[fmt]()
            if not files:
                continue
            
            # æ‰¾åˆ°æ–‡ä»¶ï¼Œæ ¹æ®æ ¼å¼è¯»å–
            num_files = len(files)
            
            # å‡†å¤‡æ‚£è€…IDè¿‡æ»¤å™¨ (æ”¯æŒå¤šç§æ•°æ®åº“çš„IDåˆ—)
            filter_tuple = None
            if patient_ids_filter and patient_ids_filter.column in ['subject_id', 'hadm_id', 'icustay_id', 'stay_id', 'admissionid', 'patientid']:
                target_ids = set(patient_ids_filter.value) if not isinstance(patient_ids_filter.value, str) else {patient_ids_filter.value}
                filter_tuple = (patient_ids_filter.column, target_ids)
                if DEBUG_MODE: print(f"   ğŸ“ åŠ è½½ {directory.name} ({num_files} ä¸ª {fmt} åˆ†åŒº) - è¿‡æ»¤ {len(target_ids)} ä¸ªæ‚£è€…...")
            else:
                if DEBUG_MODE: print(f"   ğŸ“ åŠ è½½ {directory.name} ({num_files} ä¸ª {fmt} åˆ†åŒº)...")
            
            if fmt == 'fst':
                # FST ç‰¹æ®Šå¤„ç†ï¼šæ”¯æŒå¹¶è¡Œè¯»å–
                if num_files > 3:
                    try:
                        from .fst_reader_fast import read_fst_parallel
                        return read_fst_parallel(
                            files, 
                            columns=list(columns) if columns else None, 
                            verbose=True,
                            patient_ids_filter=filter_tuple
                        )
                    except Exception:
                        pass  # Fallback to sequential
                # Sequential FST reading
                dfs = [self._read_fst_file(f, columns) for f in files]
                
            elif fmt == 'parquet':
                # Parquet è¯»å–ï¼ˆæ”¯æŒåˆ—é€‰æ‹©ï¼‰
                dfs = []
                for f in files:
                    df = pd.read_parquet(f, columns=list(columns) if columns else None)
                    # å¦‚æœæœ‰æ‚£è€…è¿‡æ»¤å™¨ï¼Œåº”ç”¨è¿‡æ»¤
                    if filter_tuple:
                        col_name, target_ids = filter_tuple
                        if col_name in df.columns:
                            df = df[df[col_name].isin(target_ids)]
                    dfs.append(df)
                    
            elif fmt == 'csv':
                # CSV è¯»å–
                dfs = []
                for f in files:
                    compression = 'gzip' if str(f).endswith('.gz') else None
                    df = pd.read_csv(f, usecols=list(columns) if columns else None, compression=compression)
                    # å¦‚æœæœ‰æ‚£è€…è¿‡æ»¤å™¨ï¼Œåº”ç”¨è¿‡æ»¤
                    if filter_tuple:
                        col_name, target_ids = filter_tuple
                        if col_name in df.columns:
                            df = df[df[col_name].isin(target_ids)]
                    dfs.append(df)
            
            # åˆå¹¶æ‰€æœ‰åˆ†åŒº
            if dfs:
                return pd.concat(dfs, ignore_index=True)
        
        # æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ”¯æŒçš„æ–‡ä»¶
        tried_formats = ', '.join(self.format_priority)
        raise ValueError(f"No supported data files found in directory: {directory} (tried: {tried_formats})")
    
    def _read_fst_file(self, path: Path, columns: Optional[Iterable[str]]) -> pd.DataFrame:
        """Read an FST file using the fst_reader module."""
        try:
            # First try the fast reader (uses R fst package directly)
            try:
                from .fst_reader_fast import read_fst_fast
                df = read_fst_fast(path, columns=list(columns) if columns else None)
                return df
            except Exception as e:
                # Fallback to regular fst_reader if fast reader fails
                from .fst_reader import read_fst
                df = read_fst(path)
                if columns is not None:
                    missing = set(columns) - set(df.columns)
                    if missing:
                        raise KeyError(f"Columns {sorted(missing)} not found in FST file '{path}'")
                    df = df[list(columns)]
                return df
        except ImportError:
            raise ImportError(
                "FST file support requires either:\n"
                "  1. Python fst package: pip install fst\n"
                "  2. R with fst package installed (recommended for fst_reader_fast)\n"
                f"Cannot read: {path}"
            )


def load_table(
    data_source: ICUDataSource,
    table_name: str,
    *,
    columns: Optional[Iterable[str]] = None,
    filters: Optional[Iterable[FilterSpec]] = None,
) -> ICUTable:
    """Functional faÃ§ade delegating to :meth:`ICUDataSource.load_table`."""

    return data_source.load_table(table_name, columns=columns, filters=filters)


def _coerce_datetime(series: pd.Series) -> pd.Series:
    """Coerce a Series to datetime type, handling various edge cases."""
    if pd.api.types.is_datetime64_any_dtype(series):
        # å¦‚æœå·²ç»æ˜¯datetimeï¼Œç§»é™¤æ—¶åŒºä¿¡æ¯ä»¥é¿å…åç»­æ—¶åŒºä¸ä¸€è‡´é”™è¯¯
        if hasattr(series.dt, 'tz') and series.dt.tz is not None:
            return series.dt.tz_localize(None)
        return series
    
    # å¦‚æœå·²ç»æ˜¯numericç±»å‹ï¼Œä¸è¦è½¬æ¢ï¼
    # è¿™å¯èƒ½æ˜¯å·²ç»å¯¹é½åˆ°å…¥é™¢æ—¶é—´çš„å°æ—¶æ•°
    if pd.api.types.is_numeric_dtype(series):
        return series
    
    # Reset index if it has duplicates (which can cause "cannot assemble with duplicate keys")
    if series.index.duplicated().any():
        series = series.reset_index(drop=True)
    
    try:
        # Try direct conversion first, with UTC then remove timezone
        converted = pd.to_datetime(series, errors="raise", utc=True).dt.tz_localize(None)
        return converted
    except (TypeError, ValueError) as e:
        # If raise fails, try with coerce
        try:
            converted = pd.to_datetime(series, errors="coerce", utc=True).dt.tz_localize(None)
            return converted
        except (TypeError, ValueError):
            # If all else fails, return original series
            # This handles cases where conversion is not possible
            return series
