"""Data loading utilities for ICU datasets."""

from __future__ import annotations

import enum
from dataclasses import dataclass, field
from pathlib import Path
import logging
from typing import Any, Callable, Dict, Iterable, List, Mapping, MutableMapping, Optional, Sequence
from threading import RLock

import pandas as pd

from .config import DataSourceConfig, DataSourceRegistry, DatasetOptions, TableConfig
from .table import ICUTable

# å…¨å±€è°ƒè¯•å¼€å…³ - è®¾ç½®ä¸º False å¯ä»¥å‡å°‘è¾“å‡º
DEBUG_MODE = False
logger = logging.getLogger(__name__)

# ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šæœ€å°å¿…è¦åˆ—é›†ï¼ˆè‡ªåŠ¨åº”ç”¨ï¼‰
MINIMAL_COLUMNS = {
    'chartevents': ['stay_id', 'charttime', 'itemid', 'valuenum', 'valueuom', 'value'],
    'labevents': ['subject_id', 'hadm_id', 'charttime', 'itemid', 'valuenum', 'valueuom'],
    'outputevents': ['stay_id', 'charttime', 'itemid', 'value'],
    'procedureevents': ['stay_id', 'starttime', 'endtime', 'itemid', 'value'],  # æ·»åŠ endtimeåˆ—ç”¨äºWinTbl
    'datetimeevents': ['stay_id', 'charttime', 'itemid', 'value'],
    'inputevents': ['stay_id', 'starttime', 'endtime', 'itemid', 'amount', 'amountuom'],
    'icustays': ['stay_id', 'subject_id', 'hadm_id', 'intime', 'outtime', 'los'],
    'd_items': ['itemid', 'label', 'category'],
}

class FilterOp(str, enum.Enum):
    """Supported filter operations for table loading."""

    EQ = "=="
    IN = "in"
    BETWEEN = "between"
    REGEX = "regex"

@dataclass
class FilterSpec:
    """Declarative filter specification for table loading."""

    column: str
    op: FilterOp
    value: Any
    metadata: Optional[Dict[str, Any]] = field(default=None)  # âœ… å­˜å‚¨é¢å¤–ä¿¡æ¯ï¼Œå¦‚åŸå§‹ stay_id
    _value_set: Optional[set] = field(default=None, init=False, repr=False)  # âš¡ ç¼“å­˜setç‰ˆæœ¬çš„value

    def __post_init__(self):
        """âš¡ æ€§èƒ½ä¼˜åŒ–: é¢„è®¡ç®—valueçš„setå½¢å¼ç”¨äºisinæ“ä½œ"""
        if self.op == FilterOp.IN:
            if isinstance(self.value, str):
                self._value_set = {self.value}
            elif hasattr(self.value, '__iter__'):
                self._value_set = set(self.value)
            else:
                self._value_set = {self.value}

    def apply(self, frame: pd.DataFrame) -> pd.DataFrame:
        # âš¡ æ€§èƒ½ä¼˜åŒ–: è¿”å›è§†å›¾è€Œéå‰¯æœ¬ï¼Œç”±è°ƒç”¨è€…å†³å®šæ˜¯å¦éœ€è¦copy
        if self.op == FilterOp.EQ:
            mask = frame[self.column] == self.value
            return frame.loc[mask]
        if self.op == FilterOp.IN:
            # âš¡ ä½¿ç”¨é¢„è®¡ç®—çš„setï¼Œé¿å…æ¯æ¬¡éƒ½listè½¬æ¢
            mask = frame[self.column].isin(self._value_set)
            return frame.loc[mask]
        if self.op == FilterOp.BETWEEN:
            lower, upper = self.value
            mask = frame[self.column].between(lower, upper)
            return frame.loc[mask]
        if self.op == FilterOp.REGEX:
            # Regex filtering for rgx_itm concepts (e.g., drug names)
            mask = frame[self.column].str.contains(self.value, case=False, na=False, regex=True)
            return frame.loc[mask]
        raise ValueError(f"Unsupported filter operation: {self.op}")

class ICUDataSource:
    """Lightweight facade that loads tables for a concrete dataset instance."""

    # å…¨å±€æ ¼å¼ä¼˜å…ˆçº§é…ç½®
    _global_format_priority: Optional[List[str]] = None

    @classmethod
    def set_format_priority(cls, priority: List[str]) -> None:
        """è®¾ç½®å…¨å±€æ–‡ä»¶æ ¼å¼ä¼˜å…ˆçº§
        
        Args:
            priority: æ ¼å¼åˆ—è¡¨ï¼ˆå½“å‰åªæ”¯æŒ ['parquet']ï¼‰
        
        Examples:
            >>> # åªä½¿ç”¨ Parquet æ ¼å¼ï¼ˆçº¯ Pythonï¼Œæ— éœ€ Rï¼‰
            >>> ICUDataSource.set_format_priority(['parquet'])
        """
        cls._global_format_priority = priority

    @classmethod
    def get_format_priority(cls) -> List[str]:
        """è·å–å½“å‰çš„æ ¼å¼ä¼˜å…ˆçº§é…ç½®
        
        Returns:
            æ ¼å¼åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
        """
        # 1. å¦‚æœè®¾ç½®äº†å…¨å±€ä¼˜å…ˆçº§ï¼Œä½¿ç”¨å…¨å±€é…ç½®
        if cls._global_format_priority is not None:
            return cls._global_format_priority
        
        # 2. æ£€æŸ¥ç¯å¢ƒå˜é‡
        import os
        env_priority = os.environ.get('PYRICU_FORMAT_PRIORITY')
        if env_priority:
            return [fmt.strip() for fmt in env_priority.split(',')]
        
        # åªæ”¯æŒ Parquet æ ¼å¼
        return ['parquet']

    def __init__(
        self,
        config: DataSourceConfig,
        *,
        base_path: str | Path | None = None,
        table_sources: Optional[Mapping[str, Any]] = None,
        registry: Optional[DataSourceRegistry] = None,
        default_format: str = "parquet",
        enable_cache: bool = True,
        format_priority: Optional[List[str]] = None,
    ) -> None:
        """åˆå§‹åŒ–æ•°æ®æº
        
        Args:
            config: æ•°æ®æºé…ç½®
            base_path: æ•°æ®æ–‡ä»¶åŸºç¡€è·¯å¾„
            table_sources: è¡¨æ•°æ®æºæ˜ å°„ï¼ˆå¯é€‰ï¼‰
            registry: æ•°æ®æºæ³¨å†Œè¡¨ï¼ˆå¯é€‰ï¼‰
            default_format: é»˜è®¤æ ¼å¼ï¼ˆå·²åºŸå¼ƒï¼Œä½¿ç”¨ format_priorityï¼‰
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
            format_priority: æ–‡ä»¶æ ¼å¼ä¼˜å…ˆçº§ï¼ˆå¯é€‰ï¼‰ï¼Œä¾‹å¦‚ ['parquet', 'fst', 'csv']
                           å¦‚æœæœªæŒ‡å®šï¼Œä½¿ç”¨å…¨å±€é…ç½®æˆ–ç¯å¢ƒå˜é‡
        """
        self.config = config
        self.base_path = Path(base_path) if base_path else None
        self._table_sources: MutableMapping[str, Any] = dict(table_sources or {})
        self.default_format = default_format
        self.registry = registry
        self._dataset_sources: Dict[str, DatasetOptions] = {
            name: table.dataset
            for name, table in self.config.tables.items()
            if table.dataset is not None
        }
        self.enable_cache = enable_cache
        self._table_cache: dict = {}  # ç¼“å­˜å·²åŠ è½½çš„åŸå§‹è¡¨æ•°æ®
        self._preloaded_tables: dict = {}  # ğŸš€ é¢„åŠ è½½çš„å®Œæ•´è¡¨ï¼ˆç”¨äºå¤šæ‚£è€…æ‰¹å¤„ç†ï¼‰
        self.format_priority = format_priority or self.get_format_priority()
        self._lock = RLock()

    def register_table_source(self, table: str, source: Any) -> None:
        """Register a callable/file path used to load ``table``."""
        self._table_sources[table] = source
    
    def clear_cache(self) -> None:
        """æ¸…é™¤è¡¨ç¼“å­˜,é‡Šæ”¾å†…å­˜ã€‚"""
        with self._lock:
            self._table_cache.clear()
            self._preloaded_tables.clear()
    
    def preload_tables(self, table_names: List[str], patient_ids: Optional[List[int]] = None) -> None:
        """
        ğŸš€ é¢„åŠ è½½å¤§è¡¨åˆ°å†…å­˜ï¼Œé¿å…é‡å¤I/O
        
        Args:
            table_names: è¦é¢„åŠ è½½çš„è¡¨ååˆ—è¡¨
            patient_ids: å¯é€‰çš„æ‚£è€…IDåˆ—è¡¨ï¼Œç”¨äºé¢„è¿‡æ»¤
        """
        base_patient_ids = list(patient_ids) if patient_ids is not None else None
        for table_name in table_names:
            with self._lock:
                if table_name in self._preloaded_tables:
                    continue
                
            # åŠ è½½å®Œæ•´è¡¨ï¼ˆä½¿ç”¨æœ€å°åˆ—é›†ï¼‰
            columns = MINIMAL_COLUMNS.get(table_name)
            
            # ä¸ä½¿ç”¨filtersï¼Œç›´æ¥åŠ è½½å®Œæ•´è¡¨
            table = self.load_table(table_name, columns=columns, verbose=False)
            df = table.dataframe()  # ä¿®æ­£ï¼šè¿™æ˜¯ä¸ªæ–¹æ³•
            
            # å¦‚æœæä¾›äº†patient_idsï¼Œé¢„è¿‡æ»¤
            if base_patient_ids is not None:
                id_col = None
                filter_ids = base_patient_ids
                if 'stay_id' in df.columns:
                    id_col = 'stay_id'
                elif 'subject_id' in df.columns:
                    # éœ€è¦ä»icustaysè·å–subject_idæ˜ å°„
                    if table_name != 'icustays':
                        icustays = self.load_table('icustays', columns=['stay_id', 'subject_id'], verbose=False)
                        icustays_df = icustays.dataframe()
                        subject_ids = icustays_df[icustays_df['stay_id'].isin(base_patient_ids)]['subject_id'].dropna().astype(int).unique()
                        id_col = 'subject_id'
                        filter_ids = subject_ids.tolist()
                
                if id_col and filter_ids is not None:
                    df = df[df[id_col].isin(filter_ids)]
            
            with self._lock:
                self._preloaded_tables[table_name] = df
            logger.info(f"ğŸ“¦ é¢„åŠ è½½è¡¨ {table_name}: {len(df):,}è¡Œ")
    
    def get_preloaded_table(self, table_name: str) -> Optional[pd.DataFrame]:
        """è·å–é¢„åŠ è½½çš„è¡¨"""
        with self._lock:
            table = self._preloaded_tables.get(table_name)
        return table
    
    def get_cache_info(self) -> dict:
        """è·å–ç¼“å­˜ä¿¡æ¯ã€‚"""
        with self._lock:
            total_size = sum(df.memory_usage(deep=True).sum() for df in self._table_cache.values())
            cached_tables = len(self._table_cache)
        return {
            'cached_tables': cached_tables,
            'memory_mb': total_size / (1024 * 1024)
        }

    def load_table(
        self,
        table_name: str,
        *,
        columns: Optional[Iterable[str]] = None,
        filters: Optional[Iterable[FilterSpec]] = None,
        verbose: bool = False,
    ) -> ICUTable:
        """Load and wrap a table according to the stored configuration."""
        
        table_cfg = self.config.get_table(table_name)

        # âœ… å…³é”®ä¿®å¤ï¼šæå‰ä¿å­˜åŸå§‹ stay_id è¿‡æ»¤å™¨å€¼
        # å› ä¸ºåç»­å¯¹äº hospital tables (labeventsç­‰) ä¼šå°† stay_id è½¬æ¢æˆ subject_id/hadm_id
        # ä½†è½¬æ¢åæ— æ³•æ¢å¤åŸå§‹ stay_idï¼Œå¯¼è‡´ join åå¼•å…¥é¢å¤–æ‚£è€…
        hospital_tables = ['prescriptions', 'labevents', 'microbiologyevents', 'emar', 'pharmacy']
        original_stay_ids = None
        if table_name in hospital_tables and self.config.name in ['miiv', 'mimic_demo']:
            if filters:
                for spec in filters:
                    if spec.column == 'stay_id' and spec.op == FilterOp.IN:
                        original_stay_ids = set(spec.value)  # ä¿å­˜åŸå§‹ç›®æ ‡ stay_ids
                        print(f"ğŸ’¾ [{table_name}] ä¿å­˜åŸå§‹ stay_id è¿‡æ»¤å™¨: {len(original_stay_ids)} ä¸ªæ‚£è€…")
                        break

        # ğŸš€ ä¼˜åŒ–1ï¼šä¼˜å…ˆä½¿ç”¨é¢„åŠ è½½çš„è¡¨
        preloaded_frame = None
        with self._lock:
            if table_name in self._preloaded_tables:
                preloaded_frame = self._preloaded_tables[table_name]

        if preloaded_frame is not None:
            frame_view = preloaded_frame

            # åº”ç”¨åˆ—è¿‡æ»¤ï¼ˆé¿å…æå‰å¤åˆ¶æ•´å¼ è¡¨ï¼‰
            if columns is not None:
                available_cols = [c for c in columns if c in frame_view.columns]
                frame_view = frame_view.loc[ :, available_cols]

            # åº”ç”¨è¡Œè¿‡æ»¤
            if filters:
                frame_filtered = frame_view
                for spec in filters:
                    frame_filtered = spec.apply(frame_filtered)
            else:
                frame_filtered = frame_view

            frame = frame_filtered.copy()
        else:
            # ğŸš€ ä¼˜åŒ–2ï¼šä½¿ç”¨æœ€å°åˆ—é›† + ä¼ å…¥çš„é¢å¤–åˆ—ï¼ˆå¦‚ value_varï¼‰
            # åªå¯¹åœ¨ MINIMAL_COLUMNS_MAP ä¸­å®šä¹‰çš„è¡¨åº”ç”¨åˆ—ä¼˜åŒ–
            # å¯¹äºå…¶ä»–è¡¨ï¼ˆå¦‚ AUMC numericitemsï¼‰ï¼ŒåŠ è½½æ‰€æœ‰åˆ—ä»¥ç¡®ä¿åŒ…å«å¿…è¦çš„ ID/æ—¶é—´/å€¼åˆ—
            from .load_concepts import MINIMAL_COLUMNS_MAP, USE_MINIMAL_COLUMNS
            if USE_MINIMAL_COLUMNS and table_name in MINIMAL_COLUMNS_MAP:
                base_columns = list(MINIMAL_COLUMNS_MAP[table_name])
                if columns is not None:
                    # åˆå¹¶æœ€å°åˆ—é›†å’Œä¼ å…¥çš„é¢å¤–åˆ—ï¼ˆå»é‡ï¼‰
                    extra_cols = [c for c in columns if c not in base_columns]
                    columns = base_columns + extra_cols
                    if DEBUG_MODE and extra_cols:
                        logger.debug(f"æ‰©å±•æœ€å°åˆ—é›†: {table_name} + {extra_cols} -> {len(columns)}åˆ—")
                else:
                    columns = base_columns
                    if DEBUG_MODE:
                        logger.debug(f"åº”ç”¨æœ€å°åˆ—é›†ä¼˜åŒ–: {table_name} -> {len(columns)}åˆ—")
            else:
                # å¯¹äºä¸åœ¨ MINIMAL_COLUMNS_MAP ä¸­çš„è¡¨ï¼ŒåŠ è½½æ‰€æœ‰åˆ—
                # è¿™ç¡®ä¿ AUMC/HiRID ç­‰æ•°æ®åº“çš„è¡¨èƒ½æ­£ç¡®åŠ è½½å¿…è¦çš„ ID å’Œå€¼åˆ—
                columns = None

            # æå– patient_ids è¿‡æ»¤å™¨ç”¨äºåˆ†åŒºé¢„è¿‡æ»¤
            patient_ids_filter = None
            
            # ğŸš€ ä¼˜åŒ–ï¼šå¯¹äºç¼ºå°‘ stay_id çš„è¡¨ï¼ˆå¦‚ labeventsï¼‰ï¼Œå¦‚æœè¿‡æ»¤æ¡ä»¶æ˜¯ stay_idï¼Œ
            # éœ€è¦å…ˆæŸ¥ icustays è½¬æ¢æˆ hadm_id æˆ– subject_idï¼Œä»¥ä¾¿åœ¨è¯»å– parquet æ—¶å°±èƒ½è¿‡æ»¤
            hospital_tables = ['prescriptions', 'labevents', 'microbiologyevents', 'emar', 'pharmacy']
            mapped_filter = None
            
            if filters:
                for spec in filters:
                    # æ”¯æŒå„æ•°æ®åº“çš„IDåˆ—å
                    id_columns = ['subject_id', 'icustay_id', 'hadm_id', 'stay_id',  # MIMIC
                                 'admissionid', 'patientid',  # AUMC
                                 'patientunitstayid',  # eICU
                                 'patientid']  # HiRID
                    
                    if spec.op == FilterOp.IN and spec.column in id_columns:
                        patient_ids_filter = spec
                        
                        # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœè¡¨æ˜¯ hospital table ä¸”è¿‡æ»¤å™¨æ˜¯ stay_id
                        if table_name in hospital_tables and self.config.name in ['miiv', 'mimic_demo'] and spec.column == 'stay_id':
                            try:
                                if verbose:
                                    logger.info(f"ğŸ”„ [{table_name}] å°† stay_id è¿‡æ»¤å™¨è½¬æ¢ä¸º hadm_id ä»¥ä¼˜åŒ–è¯»å–...")
                                
                                # åŠ è½½ icustays è·å–æ˜ å°„
                                icustays_map = self.load_table(
                                    'icustays', 
                                    columns=['stay_id', 'hadm_id'], 
                                    filters=[spec],
                                    verbose=False
                                )
                                icustays_df = icustays_map.dataframe()
                                
                                # è·å–å¯¹åº”çš„ hadm_id åˆ—è¡¨
                                valid_hadm_ids = icustays_df['hadm_id'].dropna().unique()
                                
                                if len(valid_hadm_ids) > 0:
                                    # åˆ›å»ºæ–°çš„è¿‡æ»¤å™¨
                                    mapped_filter = FilterSpec(column='hadm_id', op=FilterOp.IN, value=valid_hadm_ids)
                                    patient_ids_filter = mapped_filter
                                    if verbose:
                                        logger.info(f"âœ… [{table_name}] æ˜ å°„æˆåŠŸ: {len(spec.value)} stay_ids -> {len(valid_hadm_ids)} hadm_ids")
                            except Exception as e:
                                logger.warning(f"âš ï¸ [{table_name}] è¿‡æ»¤å™¨æ˜ å°„å¤±è´¥: {e}")
                        
                        # åªåœ¨verboseæ¨¡å¼ä¸‹è¾“å‡ºï¼Œä¸”åªè¾“å‡ºä¸€æ¬¡
                        if verbose:
                            cache_key = f"_filter_logged_{table_name}"
                            if not hasattr(self, cache_key) or not getattr(self, cache_key, False):
                                if DEBUG_MODE:
                                    logger.debug(f"æ£€æµ‹åˆ°æ‚£è€…IDè¿‡æ»¤å™¨: {len(spec.value)} ä¸ªæ‚£è€…, åˆ—={spec.column}")
                                setattr(self, cache_key, True)
                        break

            frame = self._load_raw_frame(table_name, columns, patient_ids_filter=patient_ids_filter)

            if filters:
                for spec in filters:
                    frame = spec.apply(frame)
            else:
                frame = frame.copy()

        defaults = table_cfg.defaults
        id_columns = (
            [defaults.id_var]
            if defaults.id_var and defaults.id_var in frame.columns
            else []
        )
        index_column = defaults.index_var if defaults.index_var in frame.columns else None
        time_columns = [
            column for column in defaults.time_vars if column in frame.columns
        ]
        value_column = defaults.val_var if defaults.val_var in frame.columns else None
        unit_column = defaults.unit_var if defaults.unit_var in frame.columns else None

        time_like_cols = set(time_columns)
        if index_column:
            time_like_cols.add(index_column)
        
        # AUMCç‰¹æ®Šå¤„ç†ï¼šæ—¶é—´åˆ—æ˜¯æ¯«ç§’,éœ€è¦è½¬æ¢ä¸ºåˆ†é’Ÿ (å‚è€ƒR ricuçš„ms_as_mins)
        # R ricu: ms_as_mins <- function(x) min_as_mins(as.integer(x / 6e4))
        # å…³é”®: as.integer() ä¼š floor åˆ°æ•´æ•°åˆ†é’Ÿ!
        # è¿™æ ·å¤„ç†å,AUMCçš„æ—¶é—´å•ä½ä¸å…¶ä»–æ•°æ®åº“ä¸€è‡´(éƒ½æ˜¯åˆ†é’Ÿ)
        if self.config.name == 'aumc':
            for column in time_like_cols:
                if column in frame.columns and pd.api.types.is_numeric_dtype(frame[column]):
                    # å°†æ¯«ç§’è½¬æ¢ä¸ºæ•´æ•°åˆ†é’Ÿ: floor(ms / 60000) - åŒ¹é… R ricu çš„ as.integer()
                    frame[column] = (frame[column] / 60000.0).apply(lambda x: int(x) if pd.notna(x) else x).astype('float64')
        
        for column in time_like_cols:
            # åªæœ‰å½“åˆ—å­˜åœ¨ä¸”ä¸æ˜¯numericç±»å‹æ—¶æ‰è½¬æ¢
            # å¦‚æœå·²ç»æ˜¯numericï¼Œå¯èƒ½æ˜¯å·²ç»å¯¹é½è¿‡çš„å°æ—¶æ•°
            if column in frame.columns:
                frame[column] = _coerce_datetime(frame[column])

        # è‡ªåŠ¨è¡¥å…¨ stay_idï¼šæŸäº›è¡¨ï¼ˆå¦‚ prescriptions, labeventsï¼‰åªæœ‰ hadm_idï¼Œéœ€è¦ JOIN icustays
        # è¿™å¯¹äºä½¿ç”¨è¿™äº›è¡¨çš„æ¦‚å¿µï¼ˆå¦‚ delirium_txï¼‰è‡³å…³é‡è¦
        if ('stay_id' not in frame.columns or frame['stay_id'].isna().all()) and 'hadm_id' in frame.columns:
            # âš ï¸ é—®é¢˜ï¼šå¯¹äº hospital tables (å¦‚ labevents), åŸè¡¨æ²¡æœ‰ stay_idï¼Œéœ€è¦é€šè¿‡ hadm_id join icustays è¡¥å…¨
            # ä½† join ä¼šå¼•å…¥è¯¥ hadm_id çš„æ‰€æœ‰ stay_id (åŒä¸€ä½é™¢å¯èƒ½å¤šæ¬¡ICUå…¥ä½)
            # è§£å†³æ–¹æ¡ˆï¼šåœ¨å‡½æ•°å¼€å§‹æ—¶å·²ä¿å­˜ original_stay_idsï¼Œjoin åå†è¿‡æ»¤
            hospital_tables = ['prescriptions', 'labevents', 'microbiologyevents', 'emar', 'pharmacy']
            if table_name in hospital_tables and self.config.name in ['miiv', 'mimic_demo']:
                try:
                    # ğŸ” æå–å½“å‰çš„æ‚£è€…IDè¿‡æ»¤å™¨ï¼ˆstay_id æˆ– subject_idï¼‰
                    # è¿™æ · icustays åªåŠ è½½æˆ‘ä»¬éœ€è¦çš„æ‚£è€…ï¼Œé¿å… join æ—¶äº§ç”Ÿé¢å¤–çš„åŒ¹é…
                    icustays_filters = []
                    if filters:
                        for spec in filters:
                            # stay_id æˆ– subject_id è¿‡æ»¤å™¨éƒ½å¯ä»¥ç”¨äºè¿‡æ»¤ icustays
                            if spec.column in ['stay_id', 'subject_id'] and spec.op == FilterOp.IN:
                                icustays_filters.append(spec)
                                if verbose:
                                    logger.debug(f"[{table_name}] æå–æ‚£è€…IDè¿‡æ»¤å™¨: {spec.column} IN ({len(spec.value)} ä¸ªå€¼)")
                                # ä¸è¦ breakï¼Œå¯èƒ½æœ‰å¤šä¸ªè¿‡æ»¤å™¨
                    
                    # åŠ è½½ icustays æ˜ å°„ï¼ˆéœ€è¦ hadm_id, stay_id, subject_idï¼‰
                    # å¦‚æœæœ‰æ‚£è€…IDè¿‡æ»¤å™¨ï¼Œä¼ é€’ç»™ icustays ä»¥é¿å…åŠ è½½å…¨è¡¨
                    if verbose:
                        logger.debug(f"[{table_name}] åŠ è½½ icustaysï¼Œfilters={len(icustays_filters)}ä¸ª")
                    icustays_map = self.load_table(
                        'icustays', 
                        columns=['hadm_id', 'stay_id', 'subject_id', 'intime', 'outtime'],  # éœ€è¦ intime å’Œ outtime ç”¨äº rolling join
                        filters=icustays_filters if icustays_filters else None,
                        verbose=False
                    )
                    icustays_df = icustays_map.data if hasattr(icustays_map, 'data') else icustays_map
                    if verbose:
                        logger.debug(f"[{table_name}] icustays åŠ è½½å®Œæˆ: {len(icustays_df)} è¡Œ")
                    
                    # ğŸ”¥ CRITICAL FIX: ä¸ºäº†æ­£ç¡®å®ç° rolling joinï¼Œéœ€è¦åŠ è½½åŒä¸€ hadm_id ä¸‹çš„æ‰€æœ‰ stays
                    # å½“è¯·æ±‚å•ä¸ª stay æ—¶ï¼Œå¯èƒ½åŒä¸€ hadm_id æœ‰å¤šä¸ª ICU stays
                    # ricu çš„ rolling join éœ€è¦çŸ¥é“æ‰€æœ‰ stays çš„ intime æ¥æ­£ç¡®åˆ†é…æ•°æ®
                    requested_hadm_ids = icustays_df['hadm_id'].unique().tolist()
                    if requested_hadm_ids and len(icustays_df) > 0:
                        # åŠ è½½è¿™äº› hadm_ids å¯¹åº”çš„æ‰€æœ‰ staysï¼ˆå¯èƒ½æ¯”è¯·æ±‚çš„æ›´å¤šï¼‰
                        all_stays_for_hadms = self.load_table(
                            'icustays',
                            columns=['hadm_id', 'stay_id', 'subject_id', 'intime', 'outtime'],
                            filters=[FilterSpec(column='hadm_id', op=FilterOp.IN, value=requested_hadm_ids)],
                            verbose=False
                        )
                        all_stays_df = all_stays_for_hadms.data if hasattr(all_stays_for_hadms, 'data') else all_stays_for_hadms
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°å¢çš„ staysï¼ˆåŒä¸€ hadm_id ä¸‹çš„å…¶ä»– staysï¼‰
                        if len(all_stays_df) > len(icustays_df):
                            if verbose:
                                logger.debug(f"[{table_name}] å‘ç°åŒä¸€ hadm_id ä¸‹æœ‰é¢å¤–çš„ stays: {len(icustays_df)} â†’ {len(all_stays_df)}")
                            # ä½¿ç”¨å®Œæ•´çš„ stays åˆ—è¡¨è¿›è¡Œ join
                            icustays_df = all_stays_df
                    
                    # ä¿å­˜åŸå§‹è¡Œæ•°ç”¨äºæ—¥å¿—
                    before_rows = len(frame)
                    
                    # JOIN è¡¥å…¨ stay_idï¼ˆåŒ…å« intime å’Œ outtime ç”¨äº rolling joinï¼‰
                    # æ³¨æ„ï¼šåŒä¸€ hadm_id å¯èƒ½å¯¹åº”å¤šä¸ª stay_idï¼ˆå¤šæ¬¡ ICU å…¥ä½ï¼‰
                    frame = frame.merge(
                        icustays_df[['hadm_id', 'stay_id', 'intime', 'outtime']],
                        on='hadm_id',
                        how='inner',  # åªä¿ç•™æœ‰ ICU ä½é™¢çš„è®°å½•
                        suffixes=('', '_icu')
                    )
                    
                    # æ¸…ç†å¯èƒ½çš„é‡å¤åˆ—
                    if 'stay_id_icu' in frame.columns:
                        # å¦‚æœåŸæ¥æœ‰ stay_id åˆ—ä½†æ˜¯å…¨ NaNï¼Œç”¨æ–°çš„æ›¿æ¢
                        if 'stay_id' not in frame.columns or frame['stay_id'].isna().all():
                            frame['stay_id'] = frame['stay_id_icu']
                        frame = frame.drop(columns=['stay_id_icu'], errors='ignore')
                    
                    after_join_rows = len(frame)
                    
                    # ğŸ”¥ CRITICAL FIX: å®ç° ricu çš„ rolling join é€»è¾‘
                    # 
                    # ricu ä½¿ç”¨ roll = -Inf, rollends = TRUEï¼š
                    # - å…³é”®å‘ç°ï¼šricu ä½¿ç”¨ **ICU outtime** ä½œä¸º rolling join çš„ keyï¼
                    # - roll = -Infï¼šå‘æœªæ¥æ»šåŠ¨ï¼Œæ‰¾ outtime >= charttime çš„æœ€è¿‘ stay
                    # - rollends = TRUEï¼šè¾¹ç•Œå¤–çš„æ•°æ®ä¹Ÿä¼šè¢«åˆ†é…ç»™æœ€è¿‘çš„è¾¹ç•Œ stay
                    #
                    # è¿™æ„å‘³ç€ï¼š
                    # - å¦‚æœ charttime < ç¬¬ä¸€ä¸ª stay çš„ outtimeï¼Œåˆ†é…ç»™ç¬¬ä¸€ä¸ª stay
                    # - å¦‚æœ charttime >= ç¬¬ä¸€ä¸ª stay çš„ outtime ä½† < ç¬¬äºŒä¸ª stay çš„ outtimeï¼Œ
                    #   åˆ†é…ç»™ç¬¬äºŒä¸ª stay
                    # - ä»¥æ­¤ç±»æ¨
                    #
                    # å½“åŒä¸€ hadm_id æœ‰å¤šä¸ª stay_id æ—¶ï¼Œéœ€è¦ä½¿ç”¨çœŸæ­£çš„ rolling join
                    time_col = None
                    for cand in ['charttime', 'storetime', 'starttime', 'specimen_time']:
                        if cand in frame.columns:
                            time_col = cand
                            break
                    
                    if time_col and 'stay_id' in frame.columns and 'outtime' in frame.columns:
                        # æ£€æŸ¥æ˜¯å¦æœ‰åŒä¸€ hadm_id ä¸‹çš„å¤šä¸ª stay_id
                        stays_per_hadm = frame.groupby('hadm_id')['stay_id'].nunique()
                        multi_stay_hadms = stays_per_hadm[stays_per_hadm > 1].index.tolist()
                        
                        if multi_stay_hadms:
                            if verbose:
                                logger.debug(f"[{table_name}] æ£€æµ‹åˆ° {len(multi_stay_hadms)} ä¸ª hadm_id æœ‰å¤šä¸ª stay_idï¼Œæ‰§è¡Œ rolling join (ä½¿ç”¨ outtime)")
                            
                            # è§„èŒƒåŒ–æ—¶é—´åˆ—
                            frame[time_col] = pd.to_datetime(frame[time_col], errors='coerce', utc=True)
                            if frame[time_col].dt.tz is not None:
                                frame[time_col] = frame[time_col].dt.tz_localize(None)
                            if 'intime' in frame.columns:
                                frame['intime'] = pd.to_datetime(frame['intime'], errors='coerce', utc=True)
                                if frame['intime'].dt.tz is not None:
                                    frame['intime'] = frame['intime'].dt.tz_localize(None)
                            frame['outtime'] = pd.to_datetime(frame['outtime'], errors='coerce', utc=True)
                            if frame['outtime'].dt.tz is not None:
                                frame['outtime'] = frame['outtime'].dt.tz_localize(None)
                            
                            # åˆ†ç¦»éœ€è¦ rolling join çš„æ•°æ®å’Œä¸éœ€è¦çš„æ•°æ®
                            single_stay_mask = ~frame['hadm_id'].isin(multi_stay_hadms)
                            single_stay_data = frame[single_stay_mask].copy()
                            multi_stay_data = frame[~single_stay_mask].copy()
                            
                            # ğŸ”¥ ä½¿ç”¨ pd.merge_asof å®ç°çœŸæ­£çš„ rolling join
                            # é¦–å…ˆï¼Œè·å–å”¯ä¸€çš„æ•°æ®è®°å½•ï¼ˆå»é™¤ join å¯¼è‡´çš„é‡å¤ï¼‰
                            data_cols = [c for c in multi_stay_data.columns 
                                        if c not in ['stay_id', 'intime', 'outtime']]
                            unique_data = multi_stay_data[data_cols].drop_duplicates()
                            
                            # è·å–æ¯ä¸ª hadm_id çš„ stay ä¿¡æ¯ï¼ŒæŒ‰ outtime æ’åº
                            stay_cols = ['hadm_id', 'stay_id', 'outtime']
                            if 'intime' in multi_stay_data.columns:
                                stay_cols.append('intime')
                            stay_info = multi_stay_data[stay_cols].drop_duplicates()
                            stay_info = stay_info.sort_values(['hadm_id', 'outtime'])
                            
                            # å¯¹æ¯ä¸ª hadm_id åˆ†åˆ«åš merge_asof
                            result_frames = [single_stay_data]
                            
                            for hadm_id in multi_stay_hadms:
                                # è·å–è¯¥ hadm_id çš„æ•°æ®
                                hadm_unique = unique_data[unique_data['hadm_id'] == hadm_id].copy()
                                if hadm_unique.empty:
                                    continue
                                    
                                # è·å–è¯¥ hadm_id çš„ stay ä¿¡æ¯ï¼ŒæŒ‰ outtime æ’åº
                                hadm_stays = stay_info[stay_info['hadm_id'] == hadm_id].copy()
                                hadm_stays = hadm_stays.sort_values('outtime')
                                stays_list = hadm_stays['stay_id'].tolist()
                                outtimes_list = hadm_stays['outtime'].tolist()
                                
                                # ç¡®ä¿æ•°æ®æŒ‰æ—¶é—´æ’åº
                                hadm_unique = hadm_unique.sort_values(time_col)
                                
                                # ğŸ”¥ å…³é”®ä¿®æ­£ï¼šä½¿ç”¨ outtime è€Œä¸æ˜¯ intime åš rolling join
                                # direction='forward' ç­‰ä»·äº roll = -Infï¼ˆå‘æœªæ¥æ»šåŠ¨ï¼‰
                                # æ‰¾ outtime >= charttime çš„æœ€è¿‘ stay
                                merge_cols = ['stay_id', 'outtime']
                                if 'intime' in hadm_stays.columns:
                                    merge_cols.append('intime')
                                merged = pd.merge_asof(
                                    hadm_unique,
                                    hadm_stays[merge_cols],
                                    left_on=time_col,
                                    right_on='outtime',
                                    direction='forward',  # å‘æœªæ¥æ»šåŠ¨ï¼šæ‰¾ outtime >= charttime
                                    allow_exact_matches=True
                                )
                                
                                # å¤„ç† rollends = TRUE: 
                                # å¦‚æœ charttime > æœ€åä¸€ä¸ª outtimeï¼Œåˆ†é…ç»™æœ€åä¸€ä¸ª stay
                                last_stay = stays_list[-1]
                                last_outtime = outtimes_list[-1]
                                merged.loc[merged['stay_id'].isna(), 'stay_id'] = last_stay
                                merged.loc[merged['outtime'].isna(), 'outtime'] = last_outtime
                                
                                # ç¡®ä¿ stay_id æ˜¯æ•´æ•°
                                merged['stay_id'] = merged['stay_id'].astype(int)
                                
                                result_frames.append(merged)
                            
                            frame = pd.concat(result_frames, ignore_index=True)
                            
                            if verbose:
                                logger.debug(f"[{table_name}] rolling join å®Œæˆ: {after_join_rows} â†’ {len(frame)} è¡Œ")
                    
                    # æ¸…ç†ä¸´æ—¶çš„ intime å’Œ outtime åˆ—
                    for col in ['intime', 'outtime']:
                        if col in frame.columns:
                            frame = frame.drop(columns=[col], errors='ignore')
                    
                    after_rows = len(frame)
                    
                    # âœ… å…³é”®ä¿®å¤ï¼šjoin åå¿…é¡»å†æ¬¡åº”ç”¨åŸå§‹ stay_id è¿‡æ»¤
                    # å› ä¸º join å¯èƒ½äº§ç”Ÿäº†é¢å¤–çš„ stay_ids (åŒä¸€ subject æˆ– hadm_id çš„å¤šä¸ª ICU stays)
                    # 
                    # ä¸‰ç§æƒ…å†µï¼š
                    # 1. å¦‚æœåŸå§‹è¿‡æ»¤å™¨æ˜¯ stay_idï¼Œä½¿ç”¨ä¿å­˜çš„ original_stay_ids
                    # 2. å¦‚æœåŸå§‹è¿‡æ»¤å™¨æ˜¯ subject_idï¼Œä» FilterSpec.metadata ä¸­æå–åŸå§‹ stay_id
                    # 3. ä» icustays_filters ä¸­æŸ¥æ‰¾
                    target_stay_ids = original_stay_ids
                    
                    if not target_stay_ids and icustays_filters:
                        for spec in icustays_filters:
                            if spec.column == 'stay_id' and spec.op == FilterOp.IN:
                                target_stay_ids = set(spec.value)
                                if verbose:
                                    logger.debug(f"[{table_name}] ä» stay_id è¿‡æ»¤å™¨è·å–: {len(target_stay_ids)} stays")
                                break
                            elif spec.column == 'subject_id' and spec.op == FilterOp.IN:
                                # ä» metadata ä¸­æå–åŸå§‹ stay_ids
                                if spec.metadata and 'original_stay_ids' in spec.metadata:
                                    target_stay_ids = set(spec.metadata['original_stay_ids'])
                                    if verbose:
                                        logger.debug(f"[{table_name}] ä» subject_id è¿‡æ»¤å™¨çš„ metadata è·å–åŸå§‹ stay_id: {len(target_stay_ids)} stays")
                                    break
                    
                    if target_stay_ids:
                        before_filter = len(frame)
                        if 'stay_id' in frame.columns:
                            frame = frame[frame['stay_id'].isin(target_stay_ids)]
                            if verbose:
                                logger.debug(
                                    f"[{table_name}] åº”ç”¨ stay_id è¿‡æ»¤: {before_filter}è¡Œ â†’ {len(frame)}è¡Œ "
                                    f"(ä¿ç•™ {len(target_stay_ids)} ä¸ªç›®æ ‡ stay_id)"
                                )
                        else:
                            if verbose:
                                logger.warning(f"[{table_name}] join åä»æ—  stay_id åˆ—ï¼Œæ— æ³•åº”ç”¨è¿‡æ»¤")
                    
                    # è®°å½•è¡¥å…¨æ“ä½œ
                    if verbose and before_rows != after_rows:
                        logger.info(
                            "è¡¨ %s: é€šè¿‡ hadm_id è¡¥å…¨ stay_id (%d â†’ %d è¡Œ)",
                            table_name,
                            before_rows,
                            after_rows
                        )
                    
                    # âœ… å…³é”®ä¿®å¤ï¼šè¡¥å…¨ stay_id åï¼Œæ›´æ–° id_columns
                    # è¿™æ ·ä¸‹æ¸¸ concept.py ä¼šä¿ç•™ stay_id åˆ—è€Œä¸æ˜¯åªä¿ç•™ subject_id
                    if 'stay_id' in frame.columns:
                        id_columns = ['stay_id']
                        if verbose:
                            logger.debug(f"[{table_name}] è¡¥å…¨ stay_id åæ›´æ–° id_columns: subject_id â†’ stay_id")
                        
                except Exception as e:
                    # å¦‚æœè¡¥å…¨å¤±è´¥ï¼Œè®°å½•è­¦å‘Šä½†ä¸ä¸­æ–­æµç¨‹
                    logger.warning(
                        "âš ï¸  è¡¨ %s: æ— æ³•è¡¥å…¨ stay_id: %s",
                        table_name,
                        str(e)
                    )

        if verbose and logger.isEnabledFor(logging.INFO):
            id_label = id_columns[0] if id_columns else defaults.id_var or "N/A"
            unique_count = (
                frame[id_label].nunique()
                if id_label in frame.columns
                else "N/A"
            )
            # å‡å°‘æ—¥å¿—è¾“å‡ºï¼Œåªåœ¨ DEBUG æ¨¡å¼ä¸‹æ˜¾ç¤º
            if DEBUG_MODE:
                logger.debug(
                    "è¡¨ %s: %d è¡Œ, %d ä¸ª %s",
                    table_name,
                    len(frame),
                    frame[id_label].nunique() if id_label in frame.columns else 0,
                    id_label,
                )

        return ICUTable(
            data=frame,
            id_columns=id_columns,
            index_column=index_column,
            value_column=value_column,
            unit_column=unit_column,
            time_columns=time_columns,
        )

    def _load_raw_frame(
        self,
        table_name: str,
        columns: Optional[Iterable[str]],
        patient_ids_filter: Optional[FilterSpec] = None,
    ) -> pd.DataFrame:
        # ğŸ” è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºè¯·æ±‚çš„åˆ—ï¼ˆä»…åœ¨DEBUGçº§åˆ«æ˜¾ç¤ºï¼‰
        if columns:
            logger.debug(f"_load_raw_frame: table={table_name}, columns={list(columns)}")
        
        # ğŸš€ OPTIMIZATION: ç¼“å­˜é”®ä¸åŒ…å«patient_ids_filterä»¥å®ç°è·¨æ¦‚å¿µå…±äº«
        # å¯¹äºåŒä¸€æ‰¹æ‚£è€…çš„å¤šä¸ªæ¦‚å¿µåŠ è½½,åªåœ¨ç¬¬ä¸€æ¬¡è¯»å–è¡¨,åç»­ä»ç¼“å­˜ä¸­è¿‡æ»¤
        # è¿™å°†charteventsç­‰å¤§è¡¨çš„åŠ è½½ä»Næ¬¡(æ¯æ¦‚å¿µä¸€æ¬¡)å‡å°‘åˆ°1æ¬¡
        # è·³è¿‡éœ€è¦subject_idâ†’stay_idæ˜ å°„çš„è¡¨ï¼Œè¿™äº›è¡¨ç¼“å­˜ä¼šå¯¼è‡´patientè¿‡æ»¤å¤±æ•ˆ
        # ğŸ”§ FIX: labevents ä¹Ÿå¯ä»¥ç¼“å­˜ï¼Œåªè¦æˆ‘ä»¬åœ¨keyä¸­åŒ…å«filterä¿¡æ¯
        skip_cache_tables = ['microbiologyevents', 'inputevents', 'admissions']
        enable_caching = self.enable_cache and table_name not in skip_cache_tables
        
        # ğŸ”§ FIX: å¦‚æœè¡¨æ˜¯ç»è¿‡è¿‡æ»¤åŠ è½½çš„ï¼Œå¿…é¡»å°†filteråŒ…å«åœ¨cache keyä¸­
        # å¦åˆ™ä¸åŒæ‰¹æ¬¡çš„åŠ è½½ä¼šæ··æ·†
        filter_key = None
        if patient_ids_filter:
            val = patient_ids_filter.value
            if isinstance(val, (list, tuple)):
                val = tuple(val)
            elif isinstance(val, set):
                val = tuple(sorted(val))
            # åŒ…å«åˆ—åå’Œæ“ä½œç¬¦ï¼Œç¡®ä¿å”¯ä¸€æ€§
            filter_key = (patient_ids_filter.column, patient_ids_filter.op, val)

        cache_key = (table_name, tuple(sorted(columns)) if columns else None, filter_key)
        
        # æ£€æŸ¥ç¼“å­˜
        cached_frame = None
        if enable_caching:
            with self._lock:
                cached_frame = self._table_cache.get(cache_key)
        
        if cached_frame is not None:
            # ğŸš€ OPTIMIZATION: ä»ç¼“å­˜ä¸­å–æ•°æ®åå†åº”ç”¨patientè¿‡æ»¤
            # è¿™æ ·å¤šä¸ªæ¦‚å¿µå¯ä»¥å…±äº«åŒä¸€ä¸ªç¼“å­˜çš„è¡¨å‰¯æœ¬
            # âš¡ æ€§èƒ½ä¼˜åŒ–: é¿å…copy(),ç›´æ¥è¿”å›è¿‡æ»¤åçš„è§†å›¾
            logger.debug(f"ä»ç¼“å­˜åŠ è½½: table={table_name}, cached_columns={list(cached_frame.columns)}")
            if patient_ids_filter:
                # å¦‚æœç¼“å­˜çš„keyå·²ç»åŒ…å«äº†filterï¼Œé‚£ä¹ˆcached_frameå·²ç»æ˜¯è¿‡æ»¤è¿‡çš„äº†
                # ä½†ä¸ºäº†å®‰å…¨èµ·è§ï¼ˆæˆ–è€…å¦‚æœfilter_keyé€»è¾‘æœ‰å˜ï¼‰ï¼Œå†æ¬¡æ£€æŸ¥
                # å¦‚æœfilter_keyå­˜åœ¨ï¼Œè¯´æ˜cached_frameå·²ç»æ˜¯é’ˆå¯¹è¯¥filterçš„å­é›†
                # æ­¤æ—¶å†æ¬¡åº”ç”¨filteråº”è¯¥æ˜¯å®‰å…¨çš„ï¼ˆno-opï¼‰
                return patient_ids_filter.apply(cached_frame)
            # å¦‚æœä¸éœ€è¦è¿‡æ»¤ï¼Œè¿”å›åˆ‡ç‰‡è§†å›¾è€Œéå‰¯æœ¬
            return cached_frame[:]
        
        loader = self._table_sources.get(table_name)
        dataset_cfg = self._dataset_sources.get(table_name)
        if loader is None and dataset_cfg is not None:
            frame = self._read_dataset(table_name, dataset_cfg, columns, patient_ids_filter)
        elif loader is None:
            # ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦ä¸ºå¤šæ–‡ä»¶é…ç½®ï¼Œå¦‚æœæ˜¯ï¼Œä½¿ç”¨ç›®å½•è·¯å¾„
            table_cfg = self.config.get_table(table_name)
            if len(table_cfg.files) > 1:
                # å¤šæ–‡ä»¶é…ç½®ï¼šä½¿ç”¨ç›®å½•è·¯å¾„ä»¥å¯ç”¨å¤šæ–‡ä»¶è¯»å–
                base_path = self.base_path or Path.cwd()
                if table_cfg.files:
                    # è·å–ç›®å½•è·¯å¾„ï¼ˆä»ç¬¬ä¸€ä¸ªæ–‡ä»¶è·¯å¾„ä¸­æå–ï¼‰
                    first_file = table_cfg.files[0]
                    # å¤„ç†å­—ç¬¦ä¸²æˆ–å­—å…¸æ ¼å¼
                    if isinstance(first_file, dict):
                        first_path = Path(first_file.get('path', first_file.get('name', '')))
                    else:
                        first_path = Path(first_file)
                    
                    multi_file_dir = base_path / first_path.parent
                    if multi_file_dir.is_dir():
                        loader = multi_file_dir
                    else:
                        # å›é€€åˆ°å•ä¸ªæ–‡ä»¶è§£æ
                        loader = self._resolve_loader_from_disk(table_name)
                else:
                    # å›é€€åˆ°å•ä¸ªæ–‡ä»¶è§£æ
                    loader = self._resolve_loader_from_disk(table_name)
            else:
                loader = self._resolve_loader_from_disk(table_name)
            
            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›ç©º DataFrameï¼ˆå…¼å®¹æ€§å¤„ç†ï¼Œé¿å…é˜»æ–­æ•´ä¸ªæµç¨‹ï¼‰
            if loader is None:
                # å¯¹äºmiivæ•°æ®æºï¼Œå¦‚æœè¡¨åœ¨é…ç½®ä¸­å®šä¹‰äº†ä½†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºDataFrame
                # è¿™å…è®¸åœ¨demoæ•°æ®ä¸­ç¼ºå°‘æŸäº›è¡¨æ—¶ç»§ç»­è¿è¡Œ
                if self.config.name == 'miiv' and table_name in self.config.tables:
                    logger.warning(f"Table {table_name} not found on disk, returning empty DataFrame")
                    # è¿”å›ç©ºDataFrameï¼Œä¿æŒä¸é…ç½®ä¸­è¡¨ç»“æ„ä¸€è‡´çš„åˆ—
                    table_cfg = self.config.get_table(table_name)
                    defaults = table_cfg.defaults
                    # å°è¯•ä»é…ç½®ä¸­è·å–é¢„æœŸçš„åˆ—
                    expected_cols = []
                    if defaults.id_var:
                        expected_cols.append(defaults.id_var)
                    if defaults.index_var:
                        expected_cols.append(defaults.index_var)
                    if defaults.val_var:
                        expected_cols.append(defaults.val_var)
                    if defaults.unit_var:
                        expected_cols.append(defaults.unit_var)
                    if defaults.time_vars:
                        expected_cols.extend(defaults.time_vars)
                    
                    # è¿”å›ç©ºDataFrameï¼Œé¿å…æŠ›å‡ºé”™è¯¯
                    return pd.DataFrame(columns=expected_cols if expected_cols else ['index'])
                
                raise KeyError(
                    f"No table source registered for '{table_name}' "
                    f"in data source '{self.config.name}'"
                )
        if callable(loader):
            frame = loader()
        else:
            frame = self._read_file(Path(loader), columns, patient_ids_filter=patient_ids_filter)

        if columns is not None:
            missing = set(columns) - set(frame.columns)
            if missing:
                raise KeyError(
                    f"Columns {sorted(missing)} not found in table '{table_name}'"
                )
            frame = frame[list(columns)]
        
        # ğŸš€ OPTIMIZATION: ç¼“å­˜å®Œæ•´è¡¨(æœªç»patientè¿‡æ»¤)ä»¥å®ç°è·¨æ¦‚å¿µå…±äº«
        # patientè¿‡æ»¤åœ¨ä»ç¼“å­˜è¯»å–æ—¶åº”ç”¨(è§ä¸Šé¢cached_frameåˆ†æ”¯)
        # âš¡ æ€§èƒ½ä¼˜åŒ–: ç¼“å­˜åŸå§‹frameï¼Œè¿”å›è¿‡æ»¤åçš„ç»“æœ
        # ä¸ç¼“å­˜éœ€è¦ç‰¹æ®Šå¤„ç†çš„è¡¨ï¼ˆlabevents/admissionsç­‰ï¼‰
        if enable_caching:
            with self._lock:
                # ç¼“å­˜åŸå§‹æœªè¿‡æ»¤çš„è¡¨
                self._table_cache[cache_key] = frame
        
        # åº”ç”¨patientè¿‡æ»¤(å¦‚æœæœ‰)
        if patient_ids_filter:
            return patient_ids_filter.apply(frame)
        
        # æœªè¿‡æ»¤ä¸”æœªç¼“å­˜æ—¶è¿”å›åˆ‡ç‰‡
        return frame[:] if self.enable_cache else frame

    def _resolve_loader_from_disk(self, table_name: str) -> Optional[Callable[[], pd.DataFrame] | Path]:
        if not self.base_path:
            return None
        
        table_cfg = self.config.get_table(table_name)
        explicit = table_cfg.first_file()
        
        if explicit:
            explicit_path = self.base_path / explicit
            if explicit_path.exists():
                # Accept directories (partitioned datasets) and Parquet files immediately
                if explicit_path.is_dir():
                    return explicit_path
                if explicit_path.suffix.lower() in {".parquet", ".pq"}:
                    return explicit_path
                # Otherwise continue searching for a Parquet counterpart below
        
        # MIMIC-IV æ–‡ä»¶åæ˜ å°„: é…ç½®ä¸­çš„è¡¨å -> æ–‡ä»¶ç³»ç»Ÿä¸­çš„å®é™…æ–‡ä»¶å
        # å› ä¸º MIMIC-IV æ”¹äº†è¡¨å,ä½†é…ç½®æ–‡ä»¶è¿˜æ˜¯ç”¨çš„æ—§å
        # MIMIC-IV å°† MIMIC-III çš„ä¸¤ä¸ªè¡¨åˆå¹¶äº†:
        # - procedureevents_mv -> procedureevents
        # - inputevents_cv å’Œ inputevents_mv -> inputevents
        # æ³¨æ„ï¼šå¯¹äºmiivæ•°æ®æºï¼Œæ¦‚å¿µå­—å…¸ä¸­ç›´æ¥ä½¿ç”¨procedureeventså’Œinputevents
        # ä½†å¦‚æœæ•°æ®æºé…ç½®ä¸­æ²¡æœ‰å®šä¹‰è¿™äº›è¡¨ï¼Œéœ€è¦ä»æ—§è¡¨åæ˜ å°„

        config_to_file_mappings = {
            'procedureevents_mv': 'procedureevents',  # é…ç½®å -> æ–‡ä»¶å
            'inputevents_mv': 'inputevents',
            'inputevents_cv': 'inputevents',  # MIMIC-IV åˆå¹¶äº†è¿™ä¸¤ä¸ªè¡¨
        }
        
        # å¯¹äºmiivæ•°æ®æºï¼Œå¦‚æœè¯·æ±‚çš„è¡¨åœ¨é…ç½®ä¸­ä¸å­˜åœ¨ï¼Œå°è¯•ä»æ˜ å°„ä¸­æŸ¥æ‰¾
        # ä¾‹å¦‚ï¼šæ¦‚å¿µå­—å…¸è¯·æ±‚procedureeventsï¼Œä½†é…ç½®ä¸­å¯èƒ½åªæœ‰procedureevents_mvçš„å®šä¹‰
        if self.config.name == 'miiv':
            # åå‘æ˜ å°„ï¼šå¦‚æœè¯·æ±‚çš„è¡¨åä¸åœ¨é…ç½®ä¸­ï¼Œå°è¯•æŸ¥æ‰¾å¯¹åº”çš„æ—§è¡¨å
            reverse_mapping = {
                'procedureevents': 'procedureevents_mv',
                'inputevents': 'inputevents_mv',  # ä¼˜å…ˆä½¿ç”¨inputevents_mv
            }
            if table_name not in self.config.tables and table_name in reverse_mapping:
                # ä½¿ç”¨æ˜ å°„åçš„è¡¨åæŸ¥æ‰¾æ–‡ä»¶
                mapped_table = reverse_mapping[table_name]
                file_base_name = config_to_file_mappings.get(mapped_table, table_name)
            else:
                file_base_name = config_to_file_mappings.get(table_name, table_name)
        else:
            # è·å–å®é™…è¦æŸ¥æ‰¾çš„æ–‡ä»¶å
            file_base_name = config_to_file_mappings.get(table_name, table_name)

            # ä»è¡¨é…ç½®ä¸­è·å–å®é™…çš„æ–‡ä»¶åï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if table_name in self.config.tables:
                table_config = self.config.tables[table_name]
                if hasattr(table_config, 'files') and table_config.files:
                    # è·å–ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„è·¯å¾„ï¼Œå»æ‰æ‰©å±•å
                    file_info = table_config.files[0]
                    if isinstance(file_info, dict) and 'path' in file_info:
                        file_path = file_info['path']
                    elif hasattr(file_info, 'path'):
                        file_path = file_info.path
                    else:
                        file_path = str(file_info)
                    # å»æ‰æ‰©å±•åï¼Œè·å–åŸºç¡€æ–‡ä»¶åï¼ˆå¤„ç†å¤åˆæ‰©å±•åå¦‚.csv.gzï¼‰
                    parts = file_path.split('.')
                    if len(parts) >= 2:
                        # å¤„ç†å¤åˆæ‰©å±•åå¦‚ .csv.gz
                        if parts[-1] == 'gz' and len(parts) >= 3 and parts[-2] == 'csv':
                            file_base_name = '.'.join(parts[:-2])
                        else:
                            file_base_name = '.'.join(parts[:-1])
                    else:
                        file_base_name = file_path
        
        # Only support Parquet format - try different name variations
        for name in [file_base_name, file_base_name.lower(), table_name, table_name.lower()]:
            # Try .parquet extension
            parquet_candidate = self.base_path / f"{name}.parquet"
            if parquet_candidate.exists():
                return parquet_candidate
            # Try .pq extension (short form)
            pq_candidate = self.base_path / f"{name}.pq"
            if pq_candidate.exists():
                return pq_candidate
        
        # Check subdirectory for partitioned parquet data (common in hirid observations)
        if self.base_path is not None:
            for name in [table_name, table_name.lower()]:
                subdir = self.base_path / name
                if subdir.is_dir():
                    # Look for Parquet files
                    parquet_files = list(subdir.glob("*.parquet")) + list(subdir.glob("*.pq"))
                    if parquet_files:
                        return subdir
        
        # Fall back to explicit file if it exists (e.g., CSV) so that callers can handle it
        if explicit:
            explicit_path = self.base_path / explicit
            if explicit_path.exists():
                return explicit_path

        return None

    def _get_minimal_columns(self, table_name: str) -> Optional[List[str]]:
        """è·å–è¡¨çš„æœ€å°å¿…è¦åˆ—é›†ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰"""
        return MINIMAL_COLUMNS.get(table_name)
    
    def _read_file(self, path: Path, columns: Optional[Iterable[str]], patient_ids_filter: Optional[FilterSpec] = None) -> pd.DataFrame:
        # Handle directory (partitioned data)
        if path.is_dir():
            if DEBUG_MODE:
                logger.debug(f"è¯»å–åˆ†åŒºç›®å½•: {path.name}, è¯·æ±‚åˆ—: {list(columns) if columns else 'å…¨éƒ¨åˆ—'}")
            # ğŸš€ ä¼˜å…ˆä½¿ç”¨ DuckDBï¼ˆå•æ‚£è€…/å°æ‰¹é‡æŸ¥è¯¢å¿« 5-6 å€ï¼‰
            # å¯¹äºå¤§æ‰¹é‡æ‚£è€…ï¼ˆ>100ï¼‰ï¼ŒPyArrow çš„å¹¶è¡Œè¯»å–æ›´ä¼˜
            use_duckdb = True
            if patient_ids_filter and patient_ids_filter.value:
                values = patient_ids_filter.value
                if isinstance(values, (list, tuple, set)):
                    use_duckdb = len(values) <= 100
                elif isinstance(values, pd.Series):
                    use_duckdb = len(values) <= 100
            
            if use_duckdb:
                return self._read_partitioned_data_duckdb(path, columns, patient_ids_filter)
            else:
                return self._read_partitioned_data_optimized(path, columns, patient_ids_filter)
        
        suffix = path.suffix.lower()
        suffixes = [s.lower() for s in path.suffixes]
        
        # Preferred: Parquet format
        if suffix in {".parquet", ".pq"}:
            # ğŸš€ ä½¿ç”¨PyArrowè¿‡æ»¤å™¨ä¼˜åŒ–å¤§æ–‡ä»¶è¯»å–
            if patient_ids_filter and patient_ids_filter.op == FilterOp.IN:
                try:
                    import pyarrow.parquet as pq
                    import pyarrow as pa
                    # âš¡ ä½¿ç”¨é¢„è®¡ç®—çš„set
                    target_ids = list(patient_ids_filter._value_set)
                    
                    # ä½¿ç”¨PyArrowè¯»å–å¹¶è¿‡æ»¤ - ä½¿ç”¨ DNF æ ¼å¼
                    df = pq.read_table(
                        path,
                        columns=list(columns) if columns else None,
                        filters=[[(patient_ids_filter.column, 'in', target_ids)]]
                    ).to_pandas()
                except (ImportError, Exception) as e:
                    # å¦‚æœPyArrowè¿‡æ»¤å¤±è´¥ï¼Œå›é€€åˆ°pandasåè¿‡æ»¤
                    df = pd.read_parquet(path, columns=list(columns) if columns else None, engine='pyarrow')
                    if patient_ids_filter.column in df.columns:
                        df = patient_ids_filter.apply(df)
            else:
                df = pd.read_parquet(path, columns=list(columns) if columns else None, engine='pyarrow')
            
            # å¤„ç†é‡å¤åˆ—åï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if df.columns.duplicated().any():
                import pandas.io.common
                df.columns = pandas.io.common.dedup_names(df.columns, is_potential_multiindex=False)
            return df
        
        raise ValueError(
            f"Unsupported file format '{path.suffix}' for {path.name}. Only Parquet format is supported."
        )
    
    def _read_partitioned_data_duckdb(self, directory: Path, columns: Optional[Iterable[str]], patient_ids_filter: Optional[FilterSpec] = None) -> pd.DataFrame:
        """ä½¿ç”¨ DuckDB è¯»å–åˆ†åŒºæ•°æ®ï¼ˆé«˜æ€§èƒ½ç‰ˆæœ¬ï¼‰
        
        DuckDB å¯¹å•æ‚£è€…/å°æ‰¹é‡æ‚£è€…æŸ¥è¯¢ç‰¹åˆ«é«˜æ•ˆï¼Œæ¯” PyArrow å¿« 5-6 å€ã€‚
        """
        try:
            import duckdb
        except ImportError:
            # DuckDB æœªå®‰è£…ï¼Œå›é€€åˆ° PyArrow
            return self._read_partitioned_data_optimized(directory, columns, patient_ids_filter)
        
        # æ„å»º SQL æŸ¥è¯¢
        glob_pattern = str(directory / "*.parquet")
        
        # åˆ—é€‰æ‹©
        if columns:
            select_cols = ", ".join(list(columns))
        else:
            select_cols = "*"
        
        # WHERE å­å¥
        where_clause = ""
        if patient_ids_filter and patient_ids_filter.value:
            id_col = patient_ids_filter.column
            values = patient_ids_filter.value
            
            if isinstance(values, (list, tuple, set)):
                value_list = list(values)
            elif isinstance(values, pd.Series):
                value_list = values.tolist()
            else:
                try:
                    value_list = list(values)
                except TypeError:
                    value_list = [values]
            
            if value_list:
                if len(value_list) == 1:
                    where_clause = f"WHERE {id_col} = {value_list[0]}"
                else:
                    values_str = ", ".join(map(str, value_list))
                    where_clause = f"WHERE {id_col} IN ({values_str})"
        
        query = f"SELECT {select_cols} FROM read_parquet('{glob_pattern}') {where_clause}"
        
        try:
            con = duckdb.connect()
            # ğŸ”§ CRITICAL FIX: è®¾ç½® DuckDB æ—¶åŒºä¸º UTC
            # DuckDB é»˜è®¤å°† UTC æ—¶é—´è½¬æ¢ä¸ºæœ¬åœ°æ—¶åŒºï¼Œè¿™ä¼šå¯¼è‡´æ—¶é—´åç§»
            # ä¾‹å¦‚ï¼šUTC 15:37 ä¼šè¢«è½¬æ¢æˆ Asia/Shanghai 23:37 (+8 å°æ—¶)
            # è®¾ç½®æ—¶åŒºä¸º UTC å¯ä»¥ä¿æŒåŸå§‹ UTC æ—¶é—´ä¸å˜
            con.execute("SET timezone='UTC'")
            df = con.execute(query).fetchdf()
            con.close()
            return df
        except Exception as e:
            logger.warning(f"DuckDB è¯»å–å¤±è´¥ï¼Œå›é€€åˆ° PyArrow: {e}")
            return self._read_partitioned_data_optimized(directory, columns, patient_ids_filter)
    
    def _read_partitioned_data_optimized(self, directory: Path, columns: Optional[Iterable[str]], patient_ids_filter: Optional[FilterSpec] = None) -> pd.DataFrame:
        """è¯»å–åˆ†åŒºæ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            import pyarrow.dataset as ds
            
            # ğŸš€ ä½¿ç”¨PyArrow Dataset - æœ€å¿«çš„æ–¹å¼
            dataset = ds.dataset(
                directory,
                format='parquet',
                partitioning=None,
                exclude_invalid_files=True
            )
            
            filter_expr = None
            if patient_ids_filter:
                id_col = patient_ids_filter.column
                values = patient_ids_filter.value
                if isinstance(values, (list, tuple, set)):
                    value_list = list(values)
                elif isinstance(values, pd.Series):
                    value_list = values.tolist()
                else:
                    try:
                        value_list = list(values)
                    except TypeError:
                        value_list = [values]

                if not value_list:
                    wanted_cols = list(columns) if columns else dataset.schema.names
                    return pd.DataFrame(columns=wanted_cols)

                try:
                    filter_expr = ds.field(id_col).isin(value_list)
                except Exception:
                    filter_expr = None

            # æ‰¹é‡è¯»å–ï¼Œå¯ç”¨å¤šçº¿ç¨‹ï¼ˆä¼˜åŒ–å¤§è§„æ¨¡æå–ï¼‰
            # ğŸš€ ä¼˜åŒ–ï¼šä¸º90000+æ‚£è€…æå–å¢åŠ çº¿ç¨‹æ± 
            import os
            thread_count = 32  # æœ€ä¼˜é…ç½®ï¼š32çº¿ç¨‹
            
            if columns:
                table = dataset.to_table(
                    columns=list(columns), 
                    filter=filter_expr,
                    use_threads=thread_count  # æ˜ç¡®çº¿ç¨‹æ•°
                )
            else:
                table = dataset.to_table(
                    filter=filter_expr,
                    use_threads=thread_count
                )

            # è½¬æ¢ä¸º pandasï¼Œä½¿ç”¨ zero-copy ä¼˜åŒ–
            return table.to_pandas(split_blocks=True, self_destruct=True)
            
        except Exception:
            # å›é€€åˆ°ç®€å•æ–¹å¼
            parquet_files = sorted(directory.glob("*.parquet"))
            if not parquet_files:
                parquet_files = sorted(directory.glob("*.pq"))
            
            if not parquet_files:
                return pd.DataFrame(columns=list(columns) if columns else [])
            
            # å‡†å¤‡è¿‡æ»¤æ¡ä»¶
            filter_ids = None
            id_column = None
            if patient_ids_filter:
                id_column = patient_ids_filter.column
                if isinstance(patient_ids_filter.value, (list, tuple, set)):
                    filter_ids = set(patient_ids_filter.value)
                else:
                    filter_ids = {patient_ids_filter.value}
            
            # å¿«é€Ÿè¯»å–+è¿‡æ»¤
            chunks = []
            for file_path in parquet_files:
                if columns:
                    df_chunk = pd.read_parquet(file_path, columns=list(columns))
                else:
                    df_chunk = pd.read_parquet(file_path)
                
                # ç«‹å³åº”ç”¨è¿‡æ»¤ï¼ˆå‡å°‘å†…å­˜å ç”¨ï¼‰
                if filter_ids and id_column and id_column in df_chunk.columns:
                    df_chunk = df_chunk[df_chunk[id_column].isin(filter_ids)]
                
                # åªä¿ç•™æœ‰æ•°æ®çš„chunk
                if len(df_chunk) > 0:
                    chunks.append(df_chunk)
            
            # åˆå¹¶æ‰€æœ‰chunks
            if chunks:
                return pd.concat(chunks, ignore_index=True)
            else:
                # è¿”å›ç©ºDataFrameï¼Œä¿æŒåˆ—ç»“æ„
                if columns:
                    return pd.DataFrame(columns=list(columns))
                else:
                    return pd.DataFrame()
            
        except Exception as e:
            # æœ€ç»ˆå›é€€åˆ°åŸå§‹å®ç°
            logger.warning(f"ä¼˜åŒ–è¯»å–å¤±è´¥: {e}ï¼Œå›é€€åˆ°fallbackæ–¹æ³•")
            return self._read_partitioned_data_fallback(directory, columns, patient_ids_filter)
    
    def _read_partitioned_data_fallback(self, directory: Path, columns: Optional[Iterable[str]], patient_ids_filter: Optional[FilterSpec] = None) -> pd.DataFrame:
        """Read partitioned data from a directory, respecting format priority."""
        
        # ğŸ” è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºåˆ†åŒºåŠ è½½è¯·æ±‚çš„åˆ—
        if DEBUG_MODE and columns:
            logger.debug(f"åˆ†åŒºè¡¨ {directory.name} è¯·æ±‚çš„åˆ—: {list(columns)}")
        
        # åªæ”¯æŒ Parquet æ ¼å¼
        files = sorted(directory.glob("*.parquet")) + sorted(directory.glob("*.pq"))
        if not files:
            # æ²¡æœ‰æ‰¾åˆ° parquet æ–‡ä»¶
            return pd.DataFrame()
        
        num_files = len(files)
        
        # å‡†å¤‡æ‚£è€…IDè¿‡æ»¤å™¨ (æ”¯æŒå¤šç§æ•°æ®åº“çš„IDåˆ—)
        filter_tuple = None
        if patient_ids_filter and patient_ids_filter.column in ['subject_id', 'hadm_id', 'icustay_id', 'stay_id', 'admissionid', 'patientid']:
            target_ids = set(patient_ids_filter.value) if not isinstance(patient_ids_filter.value, str) else {patient_ids_filter.value}
            filter_tuple = (patient_ids_filter.column, target_ids)
            if DEBUG_MODE: print(f"   ğŸ“ åŠ è½½ {directory.name} ({num_files} ä¸ª parquet åˆ†åŒº) - è¿‡æ»¤ {len(target_ids)} ä¸ªæ‚£è€…...")
        else:
            if DEBUG_MODE: print(f"   ğŸ“ åŠ è½½ {directory.name} ({num_files} ä¸ª parquet åˆ†åŒº)...")
        
        # ä¿®å¤ï¼šä¼ é€’å…·ä½“çš„parquetæ–‡ä»¶åˆ—è¡¨ï¼Œè€Œä¸æ˜¯ç›®å½•ï¼Œé¿å…æ··åˆæ ¼å¼é—®é¢˜
        dataset_df = self._read_parquet_dataset(
            directory,
            files,  # ä¼ é€’å…·ä½“çš„parquetæ–‡ä»¶åˆ—è¡¨
            columns=list(columns) if columns else None,
            filter_spec=patient_ids_filter,
        )
        if dataset_df is not None:
            return dataset_df
        # Fallback: iterate individual parquet files
        dfs = []
        arrow_filters = None
        if patient_ids_filter:
            arrow_filters = self._build_dataset_filter(patient_ids_filter)
        for f in files:
            if arrow_filters is not None or columns is not None:
                try:
                    import pyarrow.parquet as pq  # type: ignore
                    table = pq.read_table(
                        f,
                        columns=list(columns) if columns else None,
                    )
                    if arrow_filters is not None:
                        import pyarrow.compute as pc  # type: ignore
                        table = table.filter(arrow_filters)
                    df = table.to_pandas()
                    dfs.append(df)
                    continue
                except Exception:
                    pass  # Fallback to pandas.read_parquet below
            df = pd.read_parquet(f, columns=list(columns) if columns else None)
            if filter_tuple:
                col_name, target_ids = filter_tuple
                if col_name in df.columns:
                    df = df[df[col_name].isin(target_ids)]
            dfs.append(df)
        
        # åˆå¹¶æ‰€æœ‰åˆ†åŒº
        if dfs:
            return pd.concat(dfs, ignore_index=True)
        
        # æ²¡æœ‰æ‰¾åˆ°ä»»ä½•parquetæ–‡ä»¶
        return pd.DataFrame()

    def _read_parquet_dataset(
        self,
        directory: Path,
        files: Optional[List[Path]] = None,
        columns: Optional[Sequence[str]] = None,
        filter_spec: Optional[FilterSpec] = None,
    ) -> Optional[pd.DataFrame]:
        """Attempt to read a parquet directory via PyArrow Dataset for fast filtering."""
        try:
            import pyarrow.dataset as ds  # type: ignore
        except ImportError:
            return None

        filter_expr = None
        if filter_spec is not None:
            filter_expr = self._build_dataset_filter(filter_spec)
        try:
            # ä¿®å¤ï¼šä½¿ç”¨ä¼ å…¥çš„æ–‡ä»¶åˆ—è¡¨åˆ›å»ºdatasetï¼Œé¿å…æ··åˆæ ¼å¼é—®é¢˜
            if files is not None:
                # ä½¿ç”¨å…·ä½“çš„parquetæ–‡ä»¶åˆ—è¡¨
                dataset = ds.dataset(files, format="parquet")
            else:
                # å›é€€åˆ°åŸå§‹é€»è¾‘ï¼ˆä»…åŒ…å«parquetæ–‡ä»¶çš„ç›®å½•ï¼‰
                try:
                    dataset = ds.dataset(directory, format="parquet", partitioning="hive")
                except (ValueError, TypeError):
                    dataset = ds.dataset(directory, format="parquet")

            table = dataset.to_table(columns=columns, filter=filter_expr)
            return table.to_pandas()
        except (OSError, ValueError, TypeError) as exc:
            if DEBUG_MODE:
                logger.debug("PyArrow dataset read failed for %s: %s", directory, exc)
            return None

    @staticmethod
    def _build_dataset_filter(filter_spec: FilterSpec):
        """Convert FilterSpec to a PyArrow Dataset expression."""
        try:
            import pyarrow.dataset as ds  # type: ignore
        except ImportError:
            return None

        field = ds.field(filter_spec.column)
        if filter_spec.op == FilterOp.EQ:
            return field == filter_spec.value
        if filter_spec.op == FilterOp.IN:
            values = _ensure_sequence(filter_spec.value)
            return field.isin(values)
        if filter_spec.op == FilterOp.BETWEEN:
            lower, upper = filter_spec.value
            return (field >= lower) & (field <= upper)
        return None

    def _read_dataset(
        self,
        table_name: str,
        dataset_cfg: DatasetOptions,
        columns: Optional[Iterable[str]],
        patient_ids_filter: Optional[FilterSpec],
    ) -> pd.DataFrame:
        """Read a table via explicit PyArrow Dataset configuration."""
        try:
            import pyarrow.dataset as ds  # type: ignore
        except ImportError as exc:  # pragma: no cover - optional dependency
            raise ImportError(
                "PyArrow is required for dataset-backed tables. "
                "Install pyarrow or remove the dataset configuration."
            ) from exc

        root = dataset_cfg.path or table_name
        root_path = Path(root)
        if not root_path.is_absolute():
            if self.base_path is None:
                raise ValueError(
                    f"Dataset path '{root}' for table '{table_name}' is relative, "
                    "but data source has no base_path."
                )
            root_path = self.base_path / root_path

        partitioning = dataset_cfg.partitioning or "hive"
        format_name = dataset_cfg.format or "parquet"
        options = dataset_cfg.options or {}

        try:
            dataset = ds.dataset(
                root_path,
                format=format_name,
                partitioning=partitioning,
                **options,
            )
        except (OSError, ValueError, TypeError) as exc:
            raise RuntimeError(
                f"Failed to initialise dataset for table '{table_name}' at {root_path}: {exc}"
            ) from exc

        filter_expr = self._build_dataset_filter(patient_ids_filter) if patient_ids_filter else None
        logger.info("ğŸ“ Using PyArrow dataset for %s (%s)", table_name, root_path)

        requested_columns = list(columns) if columns is not None else None
        effective_columns: Optional[List[str]] = None
        missing_columns: List[str] = []
        if requested_columns:
            available = set(dataset.schema.names)
            missing_columns = [col for col in requested_columns if col not in available]
            effective_columns = [col for col in requested_columns if col in available]
            if not effective_columns:
                effective_columns = None

        table = dataset.to_table(columns=effective_columns, filter=filter_expr)
        frame = table.to_pandas()

        if requested_columns:
            frame = frame.reindex(columns=requested_columns)
        if missing_columns:
            logger.warning(
                "Dataset %s missing columns %s; filled with NA values", table_name, ", ".join(missing_columns)
            )
        return frame
    

def load_table(
    data_source: ICUDataSource,
    table_name: str,
    *,
    columns: Optional[Iterable[str]] = None,
    filters: Optional[Iterable[FilterSpec]] = None,
) -> ICUTable:
    """Functional faÃ§ade delegating to :meth:`ICUDataSource.load_table`."""

    return data_source.load_table(table_name, columns=columns, filters=filters)

def _ensure_sequence(value: Any) -> List[Any]:
    """Normalise scalars/iterables for filter construction."""
    if value is None:
        return []
    if isinstance(value, Sequence) and not isinstance(value, (str, bytes)):
        return list(value)
    try:
        return list(value)
    except TypeError:
        return [value]

def _coerce_datetime(series: pd.Series) -> pd.Series:
    """Coerce a Series to datetime type, handling various edge cases.
    
    âš¡ æ€§èƒ½ä¼˜åŒ–: å‡å°‘é‡å¤çš„ç±»å‹æ£€æŸ¥å’Œè½¬æ¢
    """
    # âš¡ å¿«é€Ÿè·¯å¾„1: å·²ç»æ˜¯datetimeä¸”æ— æ—¶åŒº
    if pd.api.types.is_datetime64_any_dtype(series):
        if hasattr(series.dt, 'tz') and series.dt.tz is not None:
            return series.dt.tz_localize(None)
        return series
    
    # âš¡ å¿«é€Ÿè·¯å¾„2: æ•°å€¼å‹ä¸è½¬æ¢
    if pd.api.types.is_numeric_dtype(series):
        return series
    
    # âš¡ ä¼˜åŒ–: ä¸€æ¬¡æ€§æ£€æŸ¥å’Œreset index
    has_dup_idx = series.index.duplicated().any()
    if has_dup_idx:
        series = series.reset_index(drop=True)
    
    # âš¡ ä¼˜åŒ–: ç»Ÿä¸€ä½¿ç”¨coerceæ¨¡å¼ï¼Œé¿å…try-exceptå¼€é”€
    try:
        converted = pd.to_datetime(series, errors="coerce", utc=True)
        # åªåœ¨è½¬æ¢æˆåŠŸæ—¶ç§»é™¤æ—¶åŒº
        if converted is not None and hasattr(converted, 'dt'):
            return converted.dt.tz_localize(None)
        return series
    except Exception:
        # æç«¯æƒ…å†µï¼šè¿”å›åŸå€¼
        return series
