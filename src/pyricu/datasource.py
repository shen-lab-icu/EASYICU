"""Data loading utilities for ICU datasets."""

from __future__ import annotations

import enum
from dataclasses import dataclass, field
from pathlib import Path
import logging
from typing import Any, Callable, Dict, Iterable, List, Mapping, MutableMapping, Optional, Sequence, Tuple
from threading import RLock

import pandas as pd

from .config import DataSourceConfig, DataSourceRegistry, DatasetOptions
from .table import ICUTable

# å…¨å±€è°ƒè¯•å¼€å…³ - è®¾ç½®ä¸º False å¯ä»¥å‡å°‘è¾“å‡º
DEBUG_MODE = False
logger = logging.getLogger(__name__)

# ğŸš€ AUMC numericitems ä¼˜åŒ–ï¼šåªåŠ è½½ SOFA ç›¸å…³çš„ itemids
# åŸå§‹è¡¨ 80GBï¼Œè¿‡æ»¤åçº¦ 5GBï¼Œæ€§èƒ½æå‡çº¦ 15 å€
# è¿™äº› itemids æ¥è‡ª concept-dict.json å’Œ sofa2-dict.json ä¸­ AUMC numericitems æº
AUMC_NUMERICITEMS_ITEMIDS = {
    6640, 6641, 6642, 6643, 6684, 6707, 6709, 6773, 6774, 6776, 6777, 6778, 6779, 
    6786, 6789, 6796, 6797, 6800, 6801, 6803, 6806, 6807, 6808, 6810, 6812, 6813, 
    6815, 6817, 6822, 6824, 6825, 6828, 6833, 6835, 6836, 6837, 6839, 6840, 6846, 
    6848, 6850, 7433, 8658, 8794, 8874, 8884, 8885, 8903, 8915, 9553, 9555, 9556, 
    9557, 9560, 9561, 9580, 9658, 9924, 9927, 9930, 9933, 9935, 9937, 9941, 9943, 
    9945, 9947, 9952, 9960, 9962, 9964, 9965, 9967, 9968, 9989, 9990, 9992, 9994, 
    9996, 10053, 10079, 10175, 10267, 10284, 10285, 10286, 10407, 10409, 11423, 
    11545, 11586, 11679, 11690, 11692, 11710, 11812, 11846, 11856, 11893, 11902, 
    11978, 11984, 11990, 11998, 12266, 12279, 12310, 12311, 12356, 12805, 13076, 
    13952, 14216, 14252, 14254, 14256, 14258, 16110, 16166, 17982, 18666, 18952, 
    19703, 20656, 21213, 21214,
    # ğŸ†• SOFA-2 adv_resp (é«˜çº§å‘¼å¸æ”¯æŒ) itemids - ç”¨äº PEEP æ£€æµ‹
    # 6694=Eind exp. druk/PEEP (15.6M rows), 12284=PEEP Set (15.6M rows), 8862=PEEP/CPAP (85K rows)
    6694, 12284, 8862,
    # ğŸ†• SOFA-2 rrt (è‚¾è„æ›¿ä»£æ²»ç–—) itemids
    # 7666, 7667, 7668=é€æç›¸å…³, 8805=CRRT, 10736=è¡€æ¶²é€æ, 12444=è…¹è†œé€æ
    7666, 7667, 7668, 8805, 10736, 12444,
}

# ğŸš€ MIIV chartevents ä¼˜åŒ–ï¼šåªåŠ è½½ SOFA ç›¸å…³çš„ 93 ä¸ª itemids
# åŸå§‹è¡¨ 11GBï¼Œè¿‡æ»¤åå¤§å¹…å‡å°‘
MIIV_CHARTEVENTS_ITEMIDS = {
    467, 469, 220045, 220050, 220051, 220052, 220128, 220179, 220180, 220181, 
    220210, 220227, 220277, 220339, 220739, 223761, 223762, 223835, 223848, 223849, 
    223900, 223901, 224027, 224309, 224310, 224311, 224322, 224419, 224652, 224654, 
    224660, 224684, 224685, 224686, 224687, 224688, 224689, 224690, 224695, 224696, 
    224697, 224700, 224701, 224702, 224703, 224704, 224705, 224706, 224707, 224709, 
    224738, 224746, 224747, 224750, 225312, 225436, 225949, 225979, 226253, 226512, 
    226707, 226732, 226873, 227187, 227290, 227577, 227578, 227579, 227580, 227583, 
    227980, 228096, 228151, 228154, 228156, 228158, 228193, 228198, 228300, 228332, 
    228337, 228640, 228866, 229254, 229266, 229268, 229270, 229274, 229277, 229278, 
    229280, 229314, 229326,
}

# ğŸš€ MIIV labevents ä¼˜åŒ–ï¼šåªåŠ è½½ SOFA ç›¸å…³çš„ 53 ä¸ª itemids
# åŸå§‹è¡¨ 8GBï¼Œè¿‡æ»¤åå¤§å¹…å‡å°‘
MIIV_LABEVENTS_ITEMIDS = {
    50802, 50804, 50808, 50809, 50813, 50814, 50816, 50817, 50818, 50820, 50821, 
    50822, 50852, 50861, 50862, 50863, 50878, 50882, 50883, 50885, 50889, 50893, 
    50902, 50910, 50911, 50912, 50931, 50960, 50970, 50971, 50983, 51002, 51003, 
    51006, 51144, 51146, 51200, 51214, 51221, 51222, 51237, 51244, 51248, 51249, 
    51250, 51256, 51265, 51274, 51275, 51277, 51279, 51288, 51301,
}

# ğŸš€ eICU nursecharting ä¼˜åŒ–ï¼šåªåŠ è½½ SOFA ç›¸å…³çš„å­—ç¬¦ä¸² IDs
# åŸå§‹è¡¨ 4.3GBï¼Œè¿‡æ»¤åå¤§å¹…å‡å°‘
# æ³¨æ„ï¼šeICU nursecharting ä½¿ç”¨ nursingchartcelltypevalname åˆ—è¿›è¡Œè¿‡æ»¤
EICU_NURSECHARTING_IDS = {
    # GCS ç›¸å…³
    'GCS Total', 'Eyes', 'Sedation Score', 'Motor', 'Verbal',
    # è°µå¦„ç›¸å…³ (SOFA-2 delirium_positive) - ä½¿ç”¨ nursingchartcelltypevalname çš„å€¼
    'Value', 'Delirium Score', 'Delirium Scale',
    # ECMO ç›¸å…³ (é€šè¿‡ O2 Admin Device è®°å½•)
    'O2 Admin Device',
}

# ğŸš€ VALUE-TO-ITEMID æ˜ å°„è¡¨ï¼šç”¨äºä¼˜åŒ– sub_var: value ç±»å‹çš„æ¦‚å¿µåŠ è½½
# å½“æ¦‚å¿µå®šä¹‰ä½¿ç”¨ sub_var: value æ—¶ï¼Œéœ€è¦æ‰«æå…¨è¡¨æ¥åŒ¹é… value
# ä½†å¦‚æœæˆ‘ä»¬çŸ¥é“å“ªäº› itemid åŒ…å«ç›®æ ‡ valueï¼Œå°±å¯ä»¥ä½¿ç”¨ bucket ä¼˜åŒ–
# ç»“æ„: {db_name: {table_name: {value_col: {value: [itemids]}}}}
# ä¾‹å¦‚: ett_gcs (miiv) ä½¿ç”¨ value='No Response-ETT'ï¼Œå¯¹åº” itemid=223900
VALUE_TO_ITEMID_MAPPING = {
    'miiv': {
        'chartevents': {
            'value': {
                # ett_gcs: ç”¨äºè¯†åˆ«æ°”ç®¡æ’ç®¡çŠ¶æ€
                # value='No Response-ETT' åªå‡ºç°åœ¨ itemid=223900 (GCS - Verbal Response)
                'No Response-ETT': {223900},
                '1.0 ET/Trach': {223900},  # åŒæ ·æ˜¯æ’ç®¡çŠ¶æ€
            }
        }
    },
    'mimic': {
        'chartevents': {
            'value': {
                'No Response-ETT': {223900},
                '1.0 ET/Trach': {223900},
            }
        }
    },
    'mimic_demo': {
        'chartevents': {
            'value': {
                'No Response-ETT': {223900},
                '1.0 ET/Trach': {223900},
            }
        }
    },
}

# ğŸš€ HiRID observations ä¼˜åŒ–ï¼šåªåŠ è½½æ¦‚å¿µå­—å…¸ä¸­å®šä¹‰çš„ 198 ä¸ª variableids
# åŸå§‹è¡¨ 7.77 äº¿è¡Œï¼ˆ~72GBå†…å­˜ï¼‰ï¼Œè¿‡æ»¤åå¤§å¹…å‡å°‘
# è¿™äº› variableids æ¥è‡ª concept-dict.json å’Œ sofa2-dict.json ä¸­ HiRID observations æº
HIRID_OBSERVATIONS_VARIABLEIDS = {
    15, 71, 100, 110, 112, 113, 120, 146, 151, 163, 176, 181, 186, 189, 200, 239, 
    300, 310, 326, 331, 351, 400, 405, 410, 426, 610, 2010, 2200, 3845, 4000, 7100, 
    8280, 8290, 1000022, 1000060, 1000234, 1000272, 1000273, 1000274, 1000284, 
    1000299, 1000300, 1000302, 1000304, 1000305, 1000306, 1000315, 1000317, 1000318, 
    1000320, 1000321, 1000322, 1000325, 1000335, 1000348, 1000352, 1000363, 1000365, 
    1000383, 1000390, 1000407, 1000408, 1000424, 1000425, 1000426, 1000431, 1000432, 
    1000433, 1000434, 1000435, 1000437, 1000462, 1000483, 1000486, 1000487, 1000488, 
    1000507, 1000508, 1000518, 1000519, 1000544, 1000545, 1000549, 1000567, 1000601, 
    1000648, 1000649, 1000650, 1000655, 1000656, 1000657, 1000658, 1000666, 1000670, 
    1000671, 1000689, 1000690, 1000724, 1000746, 1000750, 1000760, 1000769, 1000770, 
    1000781, 1000791, 1000797, 1000812, 1000825, 1000829, 1000830, 1000835, 1000837, 
    1000838, 1000854, 1000855, 1000893, 1000894, 1000929, 1001005, 1001068, 1001075, 
    1001079, 1001084, 1001086, 1001095, 1001096, 1001097, 1001098, 1001168, 1001169, 
    1001170, 1001171, 1001173, 1001193, 1001198, 10000100, 10000200, 10000300, 
    10000400, 10000450, 15001552, 15001565, 20000110, 20000200, 20000300, 20000400, 
    20000500, 20000600, 20000700, 20000800, 20000900, 20001200, 20001300, 20002200, 
    20002500, 20002600, 20002700, 20004100, 20004200, 20004300, 20004410, 20005100, 
    20005110, 24000150, 24000160, 24000170, 24000210, 24000220, 24000230, 24000330, 
    24000439, 24000480, 24000519, 24000520, 24000521, 24000522, 24000523, 24000524, 
    24000526, 24000536, 24000548, 24000549, 24000550, 24000557, 24000560, 24000567, 
    24000585, 24000605, 24000658, 24000668, 24000806, 24000833, 24000835, 24000836, 
    24000866, 24000867, 30005110, 30010009,
}

# ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šæœ€å°å¿…è¦åˆ—é›†ï¼ˆè‡ªåŠ¨åº”ç”¨ï¼‰
MINIMAL_COLUMNS = {
    'chartevents': ['stay_id', 'charttime', 'itemid', 'valuenum', 'valueuom', 'value'],
    'labevents': ['subject_id', 'hadm_id', 'charttime', 'itemid', 'valuenum', 'valueuom'],
    'outputevents': ['stay_id', 'charttime', 'itemid', 'value'],
    'procedureevents': ['stay_id', 'starttime', 'endtime', 'itemid', 'value'],  # æ·»åŠ endtimeåˆ—ç”¨äºWinTbl
    'datetimeevents': ['stay_id', 'charttime', 'itemid', 'value'],
    'inputevents': ['stay_id', 'starttime', 'endtime', 'itemid', 'amount', 'amountuom'],
    'icustays': ['stay_id', 'subject_id', 'hadm_id', 'intime', 'outtime', 'los'],
    'd_items': ['itemid', 'label', 'category'],
}

class FilterOp(str, enum.Enum):
    """Supported filter operations for table loading."""

    EQ = "=="
    IN = "in"
    BETWEEN = "between"
    REGEX = "regex"

@dataclass
class FilterSpec:
    """Declarative filter specification for table loading."""

    column: str
    op: FilterOp
    value: Any
    metadata: Optional[Dict[str, Any]] = field(default=None)  # âœ… å­˜å‚¨é¢å¤–ä¿¡æ¯ï¼Œå¦‚åŸå§‹ stay_id
    _value_set: Optional[set] = field(default=None, init=False, repr=False)  # âš¡ ç¼“å­˜setç‰ˆæœ¬çš„value

    def __post_init__(self):
        """âš¡ æ€§èƒ½ä¼˜åŒ–: é¢„è®¡ç®—valueçš„setå½¢å¼ç”¨äºisinæ“ä½œ"""
        if self.op == FilterOp.IN:
            if isinstance(self.value, str):
                self._value_set = {self.value}
            elif hasattr(self.value, '__iter__'):
                self._value_set = set(self.value)
            else:
                self._value_set = {self.value}

    def apply(self, frame: pd.DataFrame) -> pd.DataFrame:
        # âš¡ æ€§èƒ½ä¼˜åŒ–: è¿”å›è§†å›¾è€Œéå‰¯æœ¬ï¼Œç”±è°ƒç”¨è€…å†³å®šæ˜¯å¦éœ€è¦copy
        if self.op == FilterOp.EQ:
            mask = frame[self.column] == self.value
            return frame.loc[mask]
        if self.op == FilterOp.IN:
            # âš¡ ä½¿ç”¨é¢„è®¡ç®—çš„setï¼Œé¿å…æ¯æ¬¡éƒ½listè½¬æ¢
            mask = frame[self.column].isin(self._value_set)
            return frame.loc[mask]
        if self.op == FilterOp.BETWEEN:
            lower, upper = self.value
            mask = frame[self.column].between(lower, upper)
            return frame.loc[mask]
        if self.op == FilterOp.REGEX:
            # Regex filtering for rgx_itm concepts (e.g., drug names)
            mask = frame[self.column].str.contains(self.value, case=False, na=False, regex=True)
            return frame.loc[mask]
        raise ValueError(f"Unsupported filter operation: {self.op}")

class ICUDataSource:
    """Lightweight facade that loads tables for a concrete dataset instance."""

    # å…¨å±€æ ¼å¼ä¼˜å…ˆçº§é…ç½®
    _global_format_priority: Optional[List[str]] = None

    @classmethod
    def set_format_priority(cls, priority: List[str]) -> None:
        """è®¾ç½®å…¨å±€æ–‡ä»¶æ ¼å¼ä¼˜å…ˆçº§
        
        Args:
            priority: æ ¼å¼åˆ—è¡¨ï¼ˆå½“å‰åªæ”¯æŒ ['parquet']ï¼‰
        
        Examples:
            >>> # åªä½¿ç”¨ Parquet æ ¼å¼ï¼ˆçº¯ Pythonï¼Œæ— éœ€ Rï¼‰
            >>> ICUDataSource.set_format_priority(['parquet'])
        """
        cls._global_format_priority = priority

    @classmethod
    def get_format_priority(cls) -> List[str]:
        """è·å–å½“å‰çš„æ ¼å¼ä¼˜å…ˆçº§é…ç½®
        
        Returns:
            æ ¼å¼åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
        """
        # 1. å¦‚æœè®¾ç½®äº†å…¨å±€ä¼˜å…ˆçº§ï¼Œä½¿ç”¨å…¨å±€é…ç½®
        if cls._global_format_priority is not None:
            return cls._global_format_priority
        
        # 2. æ£€æŸ¥ç¯å¢ƒå˜é‡
        import os
        env_priority = os.environ.get('PYRICU_FORMAT_PRIORITY')
        if env_priority:
            return [fmt.strip() for fmt in env_priority.split(',')]
        
        # åªæ”¯æŒ Parquet æ ¼å¼
        return ['parquet']

    def __init__(
        self,
        config: DataSourceConfig,
        *,
        base_path: str | Path | None = None,
        table_sources: Optional[Mapping[str, Any]] = None,
        registry: Optional[DataSourceRegistry] = None,
        default_format: str = "parquet",
        enable_cache: bool = True,
        format_priority: Optional[List[str]] = None,
    ) -> None:
        """åˆå§‹åŒ–æ•°æ®æº
        
        Args:
            config: æ•°æ®æºé…ç½®
            base_path: æ•°æ®æ–‡ä»¶åŸºç¡€è·¯å¾„
            table_sources: è¡¨æ•°æ®æºæ˜ å°„ï¼ˆå¯é€‰ï¼‰
            registry: æ•°æ®æºæ³¨å†Œè¡¨ï¼ˆå¯é€‰ï¼‰
            default_format: é»˜è®¤æ ¼å¼ï¼ˆå·²åºŸå¼ƒï¼Œä½¿ç”¨ format_priorityï¼‰
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
            format_priority: æ–‡ä»¶æ ¼å¼ä¼˜å…ˆçº§ï¼ˆå¯é€‰ï¼‰ï¼Œä¾‹å¦‚ ['parquet', 'fst', 'csv']
                           å¦‚æœæœªæŒ‡å®šï¼Œä½¿ç”¨å…¨å±€é…ç½®æˆ–ç¯å¢ƒå˜é‡
        """
        self.config = config
        self.base_path = Path(base_path) if base_path else None
        self._table_sources: MutableMapping[str, Any] = dict(table_sources or {})
        self.default_format = default_format
        self.registry = registry
        self._dataset_sources: Dict[str, DatasetOptions] = {
            name: table.dataset
            for name, table in self.config.tables.items()
            if table.dataset is not None
        }
        self.enable_cache = enable_cache
        self._table_cache: dict = {}  # ç¼“å­˜å·²åŠ è½½çš„åŸå§‹è¡¨æ•°æ®
        self._preloaded_tables: dict = {}  # ğŸš€ é¢„åŠ è½½çš„å®Œæ•´è¡¨ï¼ˆç”¨äºå¤šæ‚£è€…æ‰¹å¤„ç†ï¼‰
        self._bucket_dir_logged: set = set()  # ğŸ”§ å·²æ‰“å°æ—¥å¿—çš„åˆ†æ¡¶ç›®å½•ï¼ˆé¿å…é‡å¤æ—¥å¿—ï¼‰
        self.format_priority = format_priority or self.get_format_priority()
        self._lock = RLock()

    def register_table_source(self, table: str, source: Any) -> None:
        """Register a callable/file path used to load ``table``."""
        self._table_sources[table] = source
    
    def clear_cache(self) -> None:
        """æ¸…é™¤è¡¨ç¼“å­˜,é‡Šæ”¾å†…å­˜ã€‚"""
        with self._lock:
            self._table_cache.clear()
            self._preloaded_tables.clear()
    
    def preload_tables(self, table_names: List[str], patient_ids: Optional[List[int]] = None) -> None:
        """
        ğŸš€ é¢„åŠ è½½å¤§è¡¨åˆ°å†…å­˜ï¼Œé¿å…é‡å¤I/O
        
        Args:
            table_names: è¦é¢„åŠ è½½çš„è¡¨ååˆ—è¡¨
            patient_ids: å¯é€‰çš„æ‚£è€…IDåˆ—è¡¨ï¼Œç”¨äºé¢„è¿‡æ»¤
        """
        base_patient_ids = list(patient_ids) if patient_ids is not None else None
        for table_name in table_names:
            with self._lock:
                if table_name in self._preloaded_tables:
                    continue
                
            # åŠ è½½å®Œæ•´è¡¨ï¼ˆä½¿ç”¨æœ€å°åˆ—é›†ï¼‰
            columns = MINIMAL_COLUMNS.get(table_name)
            
            # ä¸ä½¿ç”¨filtersï¼Œç›´æ¥åŠ è½½å®Œæ•´è¡¨
            table = self.load_table(table_name, columns=columns, verbose=False)
            df = table.dataframe()  # ä¿®æ­£ï¼šè¿™æ˜¯ä¸ªæ–¹æ³•
            
            # å¦‚æœæä¾›äº†patient_idsï¼Œé¢„è¿‡æ»¤
            if base_patient_ids is not None:
                id_col = None
                filter_ids = base_patient_ids
                if 'stay_id' in df.columns:
                    id_col = 'stay_id'
                elif 'subject_id' in df.columns:
                    # éœ€è¦ä»icustaysè·å–subject_idæ˜ å°„
                    if table_name != 'icustays':
                        icustays = self.load_table('icustays', columns=['stay_id', 'subject_id'], verbose=False)
                        icustays_df = icustays.dataframe()
                        subject_ids = icustays_df[icustays_df['stay_id'].isin(base_patient_ids)]['subject_id'].dropna().astype(int).unique()
                        id_col = 'subject_id'
                        filter_ids = subject_ids.tolist()
                
                if id_col and filter_ids is not None:
                    df = df[df[id_col].isin(filter_ids)]
            
            with self._lock:
                self._preloaded_tables[table_name] = df
            logger.info(f"ğŸ“¦ é¢„åŠ è½½è¡¨ {table_name}: {len(df):,}è¡Œ")
    
    def get_preloaded_table(self, table_name: str) -> Optional[pd.DataFrame]:
        """è·å–é¢„åŠ è½½çš„è¡¨"""
        with self._lock:
            table = self._preloaded_tables.get(table_name)
        return table
    
    def get_cache_info(self) -> dict:
        """è·å–ç¼“å­˜ä¿¡æ¯ã€‚"""
        with self._lock:
            total_size = sum(df.memory_usage(deep=True).sum() for df in self._table_cache.values())
            cached_tables = len(self._table_cache)
        return {
            'cached_tables': cached_tables,
            'memory_mb': total_size / (1024 * 1024)
        }

    def load_table(
        self,
        table_name: str,
        *,
        columns: Optional[Iterable[str]] = None,
        filters: Optional[Iterable[FilterSpec]] = None,
        verbose: bool = False,
    ) -> ICUTable:
        """Load and wrap a table according to the stored configuration."""
        
        table_cfg = self.config.get_table(table_name)

        # âœ… å…³é”®ä¿®å¤ï¼šæå‰ä¿å­˜åŸå§‹ stay_id è¿‡æ»¤å™¨å€¼
        # å› ä¸ºåç»­å¯¹äº hospital tables (labeventsç­‰) ä¼šå°† stay_id è½¬æ¢æˆ subject_id/hadm_id
        # ä½†è½¬æ¢åæ— æ³•æ¢å¤åŸå§‹ stay_idï¼Œå¯¼è‡´ join åå¼•å…¥é¢å¤–æ‚£è€…
        hospital_tables = ['prescriptions', 'labevents', 'microbiologyevents', 'emar', 'pharmacy']
        original_stay_ids = None
        if table_name in hospital_tables and self.config.name in ['miiv', 'mimic_demo']:
            if filters:
                for spec in filters:
                    if spec.column == 'stay_id' and spec.op == FilterOp.IN:
                        original_stay_ids = set(spec.value)  # ä¿å­˜åŸå§‹ç›®æ ‡ stay_ids
                        print(f"ğŸ’¾ [{table_name}] ä¿å­˜åŸå§‹ stay_id è¿‡æ»¤å™¨: {len(original_stay_ids)} ä¸ªæ‚£è€…")
                        break

        # ğŸš€ ä¼˜åŒ–1ï¼šä¼˜å…ˆä½¿ç”¨é¢„åŠ è½½çš„è¡¨
        preloaded_frame = None
        with self._lock:
            if table_name in self._preloaded_tables:
                preloaded_frame = self._preloaded_tables[table_name]

        if preloaded_frame is not None:
            frame_view = preloaded_frame

            # åº”ç”¨åˆ—è¿‡æ»¤ï¼ˆé¿å…æå‰å¤åˆ¶æ•´å¼ è¡¨ï¼‰
            if columns is not None:
                available_cols = [c for c in columns if c in frame_view.columns]
                frame_view = frame_view.loc[ :, available_cols]

            # åº”ç”¨è¡Œè¿‡æ»¤
            if filters:
                frame_filtered = frame_view
                for spec in filters:
                    frame_filtered = spec.apply(frame_filtered)
            else:
                frame_filtered = frame_view

            frame = frame_filtered.copy()
        else:
            # ğŸš€ ä¼˜åŒ–2ï¼šåˆ—é€‰æ‹©ç­–ç•¥
            # - å¯¹äºå®½è¡¨ï¼ˆvitalperiodicç­‰ï¼‰ï¼šåªåŠ è½½ IDåˆ— + æ—¶é—´åˆ— + ä¼ å…¥çš„ value_var
            # - å¯¹äºé•¿è¡¨ï¼ˆcharteventsç­‰ï¼‰ï¼šä½¿ç”¨ MINIMAL_COLUMNS_MAP é¢„å®šä¹‰åˆ—é›†
            from .load_concepts import MINIMAL_COLUMNS_MAP, USE_MINIMAL_COLUMNS
            
            # ğŸ¯ å®½è¡¨åˆ—è¡¨ï¼šè¿™äº›è¡¨çš„å€¼ç›´æ¥å­˜å‚¨åœ¨åˆ—åä¸­ï¼ˆå¦‚ heartrate, temperatureï¼‰
            # ä¸ä½¿ç”¨ itemid è¿‡æ»¤ï¼Œåº”è¯¥åªåŠ è½½æ¦‚å¿µæ‰€éœ€çš„å€¼åˆ—
            WIDE_TABLES = {'vitalperiodic', 'vitalaperiodic'}
            
            # ğŸš€ å®½è¡¨é¢„åŠ è½½ä¼˜åŒ–ï¼šç¬¬ä¸€æ¬¡åŠ è½½æ—¶é¢„åŠ è½½æ‰€æœ‰å¸¸ç”¨valueåˆ—
            # è¿™æ ·åç»­æ¦‚å¿µå¯ä»¥ç›´æ¥ä»ç¼“å­˜å–ï¼Œé¿å…é‡å¤è¯»å–parquet
            
            # ğŸš€ å®½è¡¨ä¼˜åŒ–ï¼šè¯†åˆ«valueåˆ—ç”¨äºNULLè¿‡æ»¤
            # å®½è¡¨çš„valueåˆ—å°±æ˜¯ä¼ å…¥çš„columnsä¸­é™¤äº†IDåˆ—å’Œæ—¶é—´åˆ—ä»¥å¤–çš„åˆ—
            wide_table_value_columns = None  # ç”¨äºDuckDB WHERE value IS NOT NULLä¼˜åŒ–
            
            if table_name in WIDE_TABLES and columns is not None:
                # å¯¹äºå®½è¡¨ï¼Œä½¿ç”¨åŠ¨æ€åˆ—é€‰æ‹©ï¼šIDåˆ— + æ—¶é—´åˆ— + ä¼ å…¥çš„å€¼åˆ—
                table_cfg = self.config.get_table(table_name)
                base_cols = set()
                id_and_time_cols = set()  # è®°å½•IDåˆ—å’Œæ—¶é—´åˆ—
                
                # æ·»åŠ  ID åˆ—ï¼ˆä¼˜å…ˆä½¿ç”¨è¡¨é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨ icustay çº§åˆ«çš„ IDï¼‰
                if table_cfg.defaults.id_var:
                    base_cols.add(table_cfg.defaults.id_var)
                    id_and_time_cols.add(table_cfg.defaults.id_var)
                else:
                    # ä»æ•°æ®åº“ id_cfg è·å– icustay çº§åˆ« ID
                    # eICU: patientunitstayid, MIIV: stay_id, AUMC: admissionid
                    icustay_cfg = self.config.id_configs.get('icustay')
                    if icustay_cfg:
                        base_cols.add(icustay_cfg.id)
                        id_and_time_cols.add(icustay_cfg.id)
                    else:
                        # å›é€€åˆ°é»˜è®¤ ID
                        default_id = self.config.get_default_id()
                        if default_id:
                            base_cols.add(default_id)
                            id_and_time_cols.add(default_id)
                    
                # æ·»åŠ æ—¶é—´åˆ—
                if table_cfg.defaults.index_var:
                    base_cols.add(table_cfg.defaults.index_var)
                    id_and_time_cols.add(table_cfg.defaults.index_var)
                    
                # åˆå¹¶ä¼ å…¥çš„å€¼åˆ—ï¼ˆå¦‚ heartrateï¼‰
                for col in columns:
                    base_cols.add(col)
                
                # ğŸš€ æå–valueåˆ—ï¼ˆç”¨äºNULLè¿‡æ»¤ï¼‰= ä¼ å…¥çš„columns - IDåˆ— - æ—¶é—´åˆ—
                value_cols = [c for c in columns if c not in id_and_time_cols]
                if value_cols:
                    wide_table_value_columns = value_cols
                    
                columns = list(base_cols)
                if DEBUG_MODE:
                    logger.debug(f"ğŸ¯ å®½è¡¨åŠ¨æ€åˆ—é€‰æ‹©: {table_name} -> {columns}")
                    logger.debug(f"ğŸ¯ å®½è¡¨valueåˆ—(ç”¨äºNULLè¿‡æ»¤): {wide_table_value_columns}")
                    
            elif USE_MINIMAL_COLUMNS and table_name in MINIMAL_COLUMNS_MAP:
                base_columns = list(MINIMAL_COLUMNS_MAP[table_name])
                
                # ğŸ”§ FIX 2026-01-26: MIMIC-III ä½¿ç”¨ icustay_id è€Œé stay_id
                # å°† stay_id æ›¿æ¢ä¸º icustay_idï¼ˆå¯¹äº MIMIC-IIIï¼‰
                db_name = self.config.name if hasattr(self, 'config') and hasattr(self.config, 'name') else ''
                if db_name == 'mimic' and 'stay_id' in base_columns:
                    base_columns = [c if c != 'stay_id' else 'icustay_id' for c in base_columns]
                    if DEBUG_MODE:
                        logger.debug("ğŸ”„ MIMIC-III åˆ—æ˜ å°„: stay_id -> icustay_id")
                
                if columns is not None:
                    # åˆå¹¶æœ€å°åˆ—é›†å’Œä¼ å…¥çš„é¢å¤–åˆ—ï¼ˆå»é‡ï¼‰
                    extra_cols = [c for c in columns if c not in base_columns]
                    columns = base_columns + extra_cols
                    if DEBUG_MODE and extra_cols:
                        logger.debug(f"æ‰©å±•æœ€å°åˆ—é›†: {table_name} + {extra_cols} -> {len(columns)}åˆ—")
                else:
                    columns = base_columns
                    if DEBUG_MODE:
                        logger.debug(f"åº”ç”¨æœ€å°åˆ—é›†ä¼˜åŒ–: {table_name} -> {len(columns)}åˆ—")
            else:
                # å¯¹äºä¸åœ¨ MINIMAL_COLUMNS_MAP ä¸­çš„è¡¨ï¼ŒåŠ è½½æ‰€æœ‰åˆ—
                # è¿™ç¡®ä¿ AUMC/HiRID ç­‰æ•°æ®åº“çš„è¡¨èƒ½æ­£ç¡®åŠ è½½å¿…è¦çš„ ID å’Œå€¼åˆ—
                columns = None

            # æå– patient_ids è¿‡æ»¤å™¨ç”¨äºåˆ†åŒºé¢„è¿‡æ»¤
            patient_ids_filter = None
            # ğŸš€ HiRID å¤§è¡¨ä¼˜åŒ–ï¼šæå– sub_var/ids è¿‡æ»¤å™¨ç”¨äº DuckDB ç²¾ç¡®è¿‡æ»¤
            # è¿™ç¡®ä¿åŠ è½½ hr æ—¶åªæŸ¥è¯¢ variableid=200ï¼Œè€Œä¸æ˜¯å…¨å±€ç™½åå•çš„ 198 ä¸ª ID
            concept_itemid_filter = None  # (column_name, set_of_ids)
            
            # ğŸš€ ä¼˜åŒ–ï¼šå¯¹äºç¼ºå°‘ stay_id çš„è¡¨ï¼ˆå¦‚ labeventsï¼‰ï¼Œå¦‚æœè¿‡æ»¤æ¡ä»¶æ˜¯ stay_idï¼Œ
            # éœ€è¦å…ˆæŸ¥ icustays è½¬æ¢æˆ hadm_id æˆ– subject_idï¼Œä»¥ä¾¿åœ¨è¯»å– parquet æ—¶å°±èƒ½è¿‡æ»¤
            hospital_tables = ['prescriptions', 'labevents', 'microbiologyevents', 'emar', 'pharmacy']
            mapped_filter = None
            
            if filters:
                for spec in filters:
                    # æ”¯æŒå„æ•°æ®åº“çš„IDåˆ—å
                    id_columns = ['subject_id', 'icustay_id', 'hadm_id', 'stay_id',  # MIMIC
                                 'admissionid', 'patientid',  # AUMC
                                 'patientunitstayid',  # eICU
                                 'patientid',  # HiRID
                                 'CaseID', 'caseid']  # ğŸ”§ FIX 2026-01-26: æ·»åŠ  SICdb CaseID
                    
                    # ğŸš€ æ£€æµ‹ sub_var/ids è¿‡æ»¤å™¨ï¼ˆç”¨äº HiRID observations ç­‰å¤§è¡¨ï¼‰
                    # è¿™äº›è¿‡æ»¤å™¨åº”è¯¥åœ¨ DuckDB å±‚åº”ç”¨ï¼Œè€Œä¸æ˜¯å†…å­˜ä¸­åº”ç”¨
                    # ğŸ”§ FIX 2026-01-26: æ·»åŠ  SICdb DataID, LaboratoryID, DrugID
                    sub_var_columns = ['variableid', 'itemid', 'nursingchartcelltypevalname',
                                       'DataID', 'LaboratoryID', 'DrugID']
                    if spec.op == FilterOp.IN and spec.column in sub_var_columns:
                        # æå–æ¦‚å¿µç‰¹å®šçš„ itemid è¿‡æ»¤å™¨
                        ids = spec.value
                        if isinstance(ids, (list, tuple)):
                            ids = set(ids)
                        elif not isinstance(ids, set):
                            ids = {ids}
                        concept_itemid_filter = (spec.column, ids)
                        if DEBUG_MODE:
                            logger.info(f"ğŸ¯ æ¦‚å¿µç‰¹å®šè¿‡æ»¤å™¨: {spec.column} IN {len(ids)} ä¸ª ID")
                        continue  # ç»§ç»­å¤„ç†ï¼Œæ‰¾ patient_id è¿‡æ»¤å™¨
                    
                    # ğŸš€ VALUE-TO-ITEMID æ˜ å°„ä¼˜åŒ–ï¼šå¤„ç† sub_var: value ç±»å‹çš„æ¦‚å¿µ
                    # ä¾‹å¦‚ ett_gcs ä½¿ç”¨ value='No Response-ETT'ï¼Œæˆ‘ä»¬å°†å…¶è½¬æ¢ä¸º itemid=223900
                    # è¿™æ ·å¯ä»¥ä½¿ç”¨ bucket ä¼˜åŒ–ï¼Œè€Œä¸æ˜¯æ‰«æå…¨è¡¨
                    if spec.op == FilterOp.IN and spec.column == 'value':
                        db_name = self.config.name
                        value_mapping = VALUE_TO_ITEMID_MAPPING.get(db_name, {}).get(table_name, {}).get('value', {})
                        if value_mapping:
                            # æ”¶é›†æ‰€æœ‰åŒ¹é…çš„ itemid
                            mapped_itemids = set()
                            filter_values = spec.value
                            if isinstance(filter_values, str):
                                filter_values = [filter_values]
                            for val in filter_values:
                                if val in value_mapping:
                                    mapped_itemids.update(value_mapping[val])
                            
                            if mapped_itemids:
                                # ä½¿ç”¨ itemid è¿›è¡Œ bucket è¿‡æ»¤ï¼ˆå¿« 50xï¼‰
                                concept_itemid_filter = ('itemid', mapped_itemids)
                                logger.debug(f"ğŸ”„ VALUE-TO-ITEMIDæ˜ å°„: value IN {filter_values} -> itemid IN {mapped_itemids}")
                                # æ³¨æ„ï¼šä»éœ€åœ¨å†…å­˜ä¸­åº”ç”¨ value è¿‡æ»¤ï¼ˆå› ä¸º itemid å¯èƒ½åŒ…å«å¤šç§ valueï¼‰
                                # è¿™ä¸ªè¿‡æ»¤ä¼šåœ¨åé¢çš„ filters å¾ªç¯ä¸­åº”ç”¨
                                continue
                    
                    if spec.op == FilterOp.IN and spec.column in id_columns:
                        patient_ids_filter = spec
                        
                        # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœè¡¨æ˜¯ hospital table ä¸”è¿‡æ»¤å™¨æ˜¯ stay_id æˆ– icustay_id
                        # ğŸ”§ FIX 2026-01-26: æ·»åŠ  mimic (MIMIC-III) æ”¯æŒï¼Œä½¿ç”¨ icustay_id
                        is_mimic_db = self.config.name in ['miiv', 'mimic_demo', 'mimic']
                        is_id_filter = spec.column in ['stay_id', 'icustay_id']
                        if table_name in hospital_tables and is_mimic_db and is_id_filter:
                            try:
                                if verbose:
                                    logger.info(f"ğŸ”„ [{table_name}] å°† {spec.column} è¿‡æ»¤å™¨è½¬æ¢ä¸º hadm_id ä»¥ä¼˜åŒ–è¯»å–...")
                                
                                # åŠ è½½ icustays è·å–æ˜ å°„
                                # MIMIC-III ä½¿ç”¨ icustay_idï¼ŒMIMIC-IV ä½¿ç”¨ stay_id
                                id_col = 'icustay_id' if self.config.name == 'mimic' else 'stay_id'
                                icustays_map = self.load_table(
                                    'icustays', 
                                    columns=[id_col, 'hadm_id'], 
                                    filters=[spec],
                                    verbose=False
                                )
                                icustays_df = icustays_map.dataframe()
                                
                                # è·å–å¯¹åº”çš„ hadm_id åˆ—è¡¨
                                valid_hadm_ids = icustays_df['hadm_id'].dropna().unique()
                                
                                if len(valid_hadm_ids) > 0:
                                    # åˆ›å»ºæ–°çš„è¿‡æ»¤å™¨
                                    mapped_filter = FilterSpec(column='hadm_id', op=FilterOp.IN, value=valid_hadm_ids)
                                    patient_ids_filter = mapped_filter
                                    if verbose:
                                        logger.info(f"âœ… [{table_name}] æ˜ å°„æˆåŠŸ: {len(spec.value)} stay_ids -> {len(valid_hadm_ids)} hadm_ids")
                            except Exception as e:
                                logger.warning(f"âš ï¸ [{table_name}] è¿‡æ»¤å™¨æ˜ å°„å¤±è´¥: {e}")
                        
                        # åªåœ¨verboseæ¨¡å¼ä¸‹è¾“å‡ºï¼Œä¸”åªè¾“å‡ºä¸€æ¬¡
                        if verbose:
                            cache_key = f"_filter_logged_{table_name}"
                            if not hasattr(self, cache_key) or not getattr(self, cache_key, False):
                                if DEBUG_MODE:
                                    logger.debug(f"æ£€æµ‹åˆ°æ‚£è€…IDè¿‡æ»¤å™¨: {len(spec.value)} ä¸ªæ‚£è€…, åˆ—={spec.column}")
                                setattr(self, cache_key, True)
                        break

            frame = self._load_raw_frame(
                table_name, columns, 
                patient_ids_filter=patient_ids_filter,
                concept_itemid_filter=concept_itemid_filter,
                wide_table_value_columns=wide_table_value_columns  # ğŸš€ ä¼ é€’å®½è¡¨valueåˆ—ç”¨äºNULLè¿‡æ»¤
            )

            # åº”ç”¨è¿‡æ»¤å™¨ï¼Œä½†è·³è¿‡å·²ç»è¢« patient_ids_filter å¤„ç†çš„è¿‡æ»¤å™¨
            # å…³é”®ä¿®å¤ï¼šå¦‚æœ patient_ids_filter è¢«è½¬æ¢è¿‡ï¼ˆä¾‹å¦‚ stay_id â†’ hadm_idï¼‰ï¼Œ
            # ä¸åº”è¯¥å†åº”ç”¨åŸå§‹çš„ stay_id è¿‡æ»¤å™¨ï¼ˆå› ä¸ºè¡¨æ²¡æœ‰ stay_id åˆ—ï¼‰
            if filters:
                for spec in filters:
                    # è·³è¿‡å·²ç»ä½œä¸º patient_ids_filter å¤„ç†çš„è¿‡æ»¤å™¨
                    if patient_ids_filter is not None:
                        # å¦‚æœåŸå§‹è¿‡æ»¤å™¨çš„åˆ—åå’Œ patient_ids_filter çš„åˆ—åä¸åŒï¼Œ
                        # è¯´æ˜è¿‡æ»¤å™¨è¢«è½¬æ¢è¿‡ï¼ˆä¾‹å¦‚ stay_id â†’ hadm_idï¼‰
                        if spec.column != patient_ids_filter.column and spec.op == FilterOp.IN:
                            # æ£€æŸ¥æ˜¯å¦æ˜¯åŒç±»å‹çš„ ID è¿‡æ»¤å™¨ï¼ˆéƒ½æ˜¯ patient ID ç±»å‹ï¼‰
                            id_columns_set = {'subject_id', 'icustay_id', 'hadm_id', 'stay_id',
                                             'admissionid', 'patientid', 'patientunitstayid',
                                             'CaseID', 'caseid'}  # ğŸ”§ FIX 2026-01-26: æ·»åŠ  SICdb CaseID
                            if spec.column in id_columns_set and patient_ids_filter.column in id_columns_set:
                                # è¿™ä¸ªè¿‡æ»¤å™¨å·²ç»è¢«è½¬æ¢å¤„ç†äº†ï¼Œè·³è¿‡
                                continue
                        elif spec.column == patient_ids_filter.column and spec.op == patient_ids_filter.op:
                            # å®Œå…¨ç›¸åŒçš„è¿‡æ»¤å™¨ï¼Œå·²ç»åœ¨ _load_raw_frame ä¸­å¤„ç†ï¼Œè·³è¿‡
                            continue
                    
                    # å®‰å…¨æ£€æŸ¥ï¼šåªåº”ç”¨åˆ—å­˜åœ¨çš„è¿‡æ»¤å™¨
                    if spec.column in frame.columns:
                        frame = spec.apply(frame)
                    elif spec.op == FilterOp.IN and spec.column not in frame.columns:
                        # ID åˆ—ä¸å­˜åœ¨ï¼Œä½†å¯èƒ½é€šè¿‡åç»­çš„ join è¡¥å…¨ï¼Œæš‚æ—¶è·³è¿‡
                        pass
            else:
                frame = frame.copy()

        defaults = table_cfg.defaults
        id_columns = (
            [defaults.id_var]
            if defaults.id_var and defaults.id_var in frame.columns
            else []
        )
        index_column = defaults.index_var if defaults.index_var in frame.columns else None
        time_columns = [
            column for column in defaults.time_vars if column in frame.columns
        ]
        value_column = defaults.val_var if defaults.val_var in frame.columns else None
        unit_column = defaults.unit_var if defaults.unit_var in frame.columns else None

        time_like_cols = set(time_columns)
        if index_column:
            time_like_cols.add(index_column)
        
        # AUMCç‰¹æ®Šå¤„ç†ï¼šæ—¶é—´åˆ—æ˜¯æ¯«ç§’,éœ€è¦è½¬æ¢ä¸ºåˆ†é’Ÿ (å‚è€ƒR ricuçš„ms_as_mins)
        # R ricu: ms_as_mins <- function(x) min_as_mins(as.integer(x / 6e4))
        # å…³é”®: as.integer() ä¼š floor åˆ°æ•´æ•°åˆ†é’Ÿ!
        # è¿™æ ·å¤„ç†å,AUMCçš„æ—¶é—´å•ä½ä¸å…¶ä»–æ•°æ®åº“ä¸€è‡´(éƒ½æ˜¯åˆ†é’Ÿ)
        if self.config.name == 'aumc':
            for column in time_like_cols:
                if column in frame.columns and pd.api.types.is_numeric_dtype(frame[column]):
                    # å°†æ¯«ç§’è½¬æ¢ä¸ºæ•´æ•°åˆ†é’Ÿ: floor(ms / 60000) - åŒ¹é… R ricu çš„ as.integer()
                    # ğŸ”§ PERFORMANCE FIX: Use numpy floor instead of slow apply+lambda
                    # ğŸ”§ FIX: Handle pd.NA values properly by converting to numpy float array first
                    import numpy as np
                    values = np.array(frame[column], dtype=float) / 60000.0
                    frame[column] = np.where(np.isnan(values), np.nan, np.floor(values))
        
        for column in time_like_cols:
            # åªæœ‰å½“åˆ—å­˜åœ¨ä¸”ä¸æ˜¯numericç±»å‹æ—¶æ‰è½¬æ¢
            # å¦‚æœå·²ç»æ˜¯numericï¼Œå¯èƒ½æ˜¯å·²ç»å¯¹é½è¿‡çš„å°æ—¶æ•°
            if column in frame.columns:
                frame[column] = _coerce_datetime(frame[column])

        # ğŸ”§ FIX 2026-01-26: æ”¯æŒ MIMIC-III çš„ icustay_id
        # MIMIC-III çš„ id åˆ—æ˜¯ icustay_idï¼Œéœ€è¦è¡¥å…¨
        target_id_col = 'icustay_id' if self.config.name == 'mimic' else 'stay_id'
        has_target_id = target_id_col in frame.columns and not frame[target_id_col].isna().all()
        
        if not has_target_id and 'hadm_id' in frame.columns:
            # âš ï¸ é—®é¢˜ï¼šå¯¹äº hospital tables (å¦‚ labevents), åŸè¡¨æ²¡æœ‰ stay_id/icustay_idï¼Œéœ€è¦é€šè¿‡ hadm_id join icustays è¡¥å…¨
            # ä½† join ä¼šå¼•å…¥è¯¥ hadm_id çš„æ‰€æœ‰ stay_id (åŒä¸€ä½é™¢å¯èƒ½å¤šæ¬¡ICUå…¥ä½)
            # è§£å†³æ–¹æ¡ˆï¼šåœ¨å‡½æ•°å¼€å§‹æ—¶å·²ä¿å­˜ original_stay_idsï¼Œjoin åå†è¿‡æ»¤
            hospital_tables = ['prescriptions', 'labevents', 'microbiologyevents', 'emar', 'pharmacy']
            is_mimic_db = self.config.name in ['miiv', 'mimic_demo', 'mimic']
            if table_name in hospital_tables and is_mimic_db:
                try:
                    # ğŸ” æå–å½“å‰çš„æ‚£è€…IDè¿‡æ»¤å™¨ï¼ˆstay_id/icustay_id æˆ– subject_idï¼‰
                    # è¿™æ · icustays åªåŠ è½½æˆ‘ä»¬éœ€è¦çš„æ‚£è€…ï¼Œé¿å… join æ—¶äº§ç”Ÿé¢å¤–çš„åŒ¹é…
                    icustays_filters = []
                    if filters:
                        for spec in filters:
                            # stay_id/icustay_id æˆ– subject_id è¿‡æ»¤å™¨éƒ½å¯ä»¥ç”¨äºè¿‡æ»¤ icustays
                            if spec.column in ['stay_id', 'icustay_id', 'subject_id'] and spec.op == FilterOp.IN:
                                icustays_filters.append(spec)
                                if verbose:
                                    logger.debug(f"[{table_name}] æå–æ‚£è€…IDè¿‡æ»¤å™¨: {spec.column} IN ({len(spec.value)} ä¸ªå€¼)")
                                # ä¸è¦ breakï¼Œå¯èƒ½æœ‰å¤šä¸ªè¿‡æ»¤å™¨
                    
                    # åŠ è½½ icustays æ˜ å°„ï¼ˆéœ€è¦ hadm_id, stay_id/icustay_id, subject_idï¼‰
                    # å¦‚æœæœ‰æ‚£è€…IDè¿‡æ»¤å™¨ï¼Œä¼ é€’ç»™ icustays ä»¥é¿å…åŠ è½½å…¨è¡¨
                    if verbose:
                        logger.debug(f"[{table_name}] åŠ è½½ icustaysï¼Œfilters={len(icustays_filters)}ä¸ª")
                    icustays_map = self.load_table(
                        'icustays', 
                        columns=['hadm_id', target_id_col, 'subject_id', 'intime', 'outtime'],  # éœ€è¦ intime å’Œ outtime ç”¨äº rolling join
                        filters=icustays_filters if icustays_filters else None,
                        verbose=False
                    )
                    icustays_df = icustays_map.data if hasattr(icustays_map, 'data') else icustays_map
                    if verbose:
                        logger.debug(f"[{table_name}] icustays åŠ è½½å®Œæˆ: {len(icustays_df)} è¡Œ")
                    
                    # ğŸ”¥ CRITICAL FIX: ä¸ºäº†æ­£ç¡®å®ç° rolling joinï¼Œéœ€è¦åŠ è½½åŒä¸€ hadm_id ä¸‹çš„æ‰€æœ‰ stays
                    # å½“è¯·æ±‚å•ä¸ª stay æ—¶ï¼Œå¯èƒ½åŒä¸€ hadm_id æœ‰å¤šä¸ª ICU stays
                    # ricu çš„ rolling join éœ€è¦çŸ¥é“æ‰€æœ‰ stays çš„ intime æ¥æ­£ç¡®åˆ†é…æ•°æ®
                    requested_hadm_ids = icustays_df['hadm_id'].unique().tolist()
                    if requested_hadm_ids and len(icustays_df) > 0:
                        # åŠ è½½è¿™äº› hadm_ids å¯¹åº”çš„æ‰€æœ‰ staysï¼ˆå¯èƒ½æ¯”è¯·æ±‚çš„æ›´å¤šï¼‰
                        all_stays_for_hadms = self.load_table(
                            'icustays',
                            columns=['hadm_id', target_id_col, 'subject_id', 'intime', 'outtime'],
                            filters=[FilterSpec(column='hadm_id', op=FilterOp.IN, value=requested_hadm_ids)],
                            verbose=False
                        )
                        all_stays_df = all_stays_for_hadms.data if hasattr(all_stays_for_hadms, 'data') else all_stays_for_hadms
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°å¢çš„ staysï¼ˆåŒä¸€ hadm_id ä¸‹çš„å…¶ä»– staysï¼‰
                        if len(all_stays_df) > len(icustays_df):
                            if verbose:
                                logger.debug(f"[{table_name}] å‘ç°åŒä¸€ hadm_id ä¸‹æœ‰é¢å¤–çš„ stays: {len(icustays_df)} â†’ {len(all_stays_df)}")
                            # ä½¿ç”¨å®Œæ•´çš„ stays åˆ—è¡¨è¿›è¡Œ join
                            icustays_df = all_stays_df
                    
                    # ä¿å­˜åŸå§‹è¡Œæ•°ç”¨äºæ—¥å¿—
                    before_rows = len(frame)
                    
                    # JOIN è¡¥å…¨ stay_id/icustay_idï¼ˆåŒ…å« intime å’Œ outtime ç”¨äº rolling joinï¼‰
                    # æ³¨æ„ï¼šåŒä¸€ hadm_id å¯èƒ½å¯¹åº”å¤šä¸ª stay_idï¼ˆå¤šæ¬¡ ICU å…¥ä½ï¼‰
                    frame = frame.merge(
                        icustays_df[['hadm_id', target_id_col, 'intime', 'outtime']],
                        on='hadm_id',
                        how='inner',  # åªä¿ç•™æœ‰ ICU ä½é™¢çš„è®°å½•
                        suffixes=('', '_icu')
                    )
                    
                    # æ¸…ç†å¯èƒ½çš„é‡å¤åˆ—
                    icu_col_name = f'{target_id_col}_icu'
                    if icu_col_name in frame.columns:
                        # å¦‚æœåŸæ¥æœ‰ id åˆ—ä½†æ˜¯å…¨ NaNï¼Œç”¨æ–°çš„æ›¿æ¢
                        if target_id_col not in frame.columns or frame[target_id_col].isna().all():
                            frame[target_id_col] = frame[icu_col_name]
                        frame = frame.drop(columns=[icu_col_name], errors='ignore')
                    
                    after_join_rows = len(frame)
                    
                    # ğŸ”¥ CRITICAL FIX: å®ç° ricu çš„ rolling join é€»è¾‘
                    # 
                    # ricu ä½¿ç”¨ roll = -Inf, rollends = TRUEï¼š
                    # - å…³é”®å‘ç°ï¼šricu ä½¿ç”¨ **ICU outtime** ä½œä¸º rolling join çš„ keyï¼
                    # - roll = -Infï¼šå‘æœªæ¥æ»šåŠ¨ï¼Œæ‰¾ outtime >= charttime çš„æœ€è¿‘ stay
                    # - rollends = TRUEï¼šè¾¹ç•Œå¤–çš„æ•°æ®ä¹Ÿä¼šè¢«åˆ†é…ç»™æœ€è¿‘çš„è¾¹ç•Œ stay
                    #
                    # è¿™æ„å‘³ç€ï¼š
                    # - å¦‚æœ charttime < ç¬¬ä¸€ä¸ª stay çš„ outtimeï¼Œåˆ†é…ç»™ç¬¬ä¸€ä¸ª stay
                    # - å¦‚æœ charttime >= ç¬¬ä¸€ä¸ª stay çš„ outtime ä½† < ç¬¬äºŒä¸ª stay çš„ outtimeï¼Œ
                    #   åˆ†é…ç»™ç¬¬äºŒä¸ª stay
                    # - ä»¥æ­¤ç±»æ¨
                    #
                    # å½“åŒä¸€ hadm_id æœ‰å¤šä¸ª stay_id æ—¶ï¼Œéœ€è¦ä½¿ç”¨çœŸæ­£çš„ rolling join
                    time_col = None
                    for cand in ['charttime', 'storetime', 'starttime', 'specimen_time']:
                        if cand in frame.columns:
                            time_col = cand
                            break
                    
                    if time_col and target_id_col in frame.columns and 'outtime' in frame.columns:
                        # æ£€æŸ¥æ˜¯å¦æœ‰åŒä¸€ hadm_id ä¸‹çš„å¤šä¸ª stay_id/icustay_id
                        stays_per_hadm = frame.groupby('hadm_id')[target_id_col].nunique()
                        multi_stay_hadms = stays_per_hadm[stays_per_hadm > 1].index.tolist()
                        
                        if multi_stay_hadms:
                            if verbose:
                                logger.debug(f"[{table_name}] æ£€æµ‹åˆ° {len(multi_stay_hadms)} ä¸ª hadm_id æœ‰å¤šä¸ª {target_id_col}ï¼Œæ‰§è¡Œ rolling join (ä½¿ç”¨ outtime)")
                            
                            # è§„èŒƒåŒ–æ—¶é—´åˆ— - ç»Ÿä¸€ä¸º datetime64[ns] ä»¥å…¼å®¹ merge_asof
                            def _normalize_datetime_ns(series: pd.Series) -> pd.Series:
                                """è§„èŒƒåŒ–datetimeä¸ºnsç²¾åº¦ï¼Œå»æ—¶åŒº"""
                                dt = pd.to_datetime(series, errors='coerce', utc=True)
                                if dt.dt.tz is not None:
                                    dt = dt.dt.tz_localize(None)
                                # ğŸ”§ FIX: ç»Ÿä¸€è½¬æ¢ä¸º datetime64[ns] ç¡®ä¿ merge_asof å…¼å®¹
                                return dt.astype('datetime64[ns]')
                            
                            frame[time_col] = _normalize_datetime_ns(frame[time_col])
                            if 'intime' in frame.columns:
                                frame['intime'] = _normalize_datetime_ns(frame['intime'])
                            frame['outtime'] = _normalize_datetime_ns(frame['outtime'])
                            
                            # åˆ†ç¦»éœ€è¦ rolling join çš„æ•°æ®å’Œä¸éœ€è¦çš„æ•°æ®
                            single_stay_mask = ~frame['hadm_id'].isin(multi_stay_hadms)
                            single_stay_data = frame[single_stay_mask].copy()
                            multi_stay_data = frame[~single_stay_mask].copy()
                            
                            # ğŸ”¥ ä½¿ç”¨ pd.merge_asof å®ç°çœŸæ­£çš„ rolling join
                            # é¦–å…ˆï¼Œè·å–å”¯ä¸€çš„æ•°æ®è®°å½•ï¼ˆå»é™¤ join å¯¼è‡´çš„é‡å¤ï¼‰
                            data_cols = [c for c in multi_stay_data.columns 
                                        if c not in [target_id_col, 'intime', 'outtime']]
                            unique_data = multi_stay_data[data_cols].drop_duplicates()
                            
                            # è·å–æ¯ä¸ª hadm_id çš„ stay ä¿¡æ¯ï¼ŒæŒ‰ outtime æ’åº
                            stay_cols = ['hadm_id', target_id_col, 'outtime']
                            if 'intime' in multi_stay_data.columns:
                                stay_cols.append('intime')
                            stay_info = multi_stay_data[stay_cols].drop_duplicates()
                            stay_info = stay_info.sort_values(['hadm_id', 'outtime'])
                            
                            # å¯¹æ¯ä¸ª hadm_id åˆ†åˆ«åš merge_asof
                            result_frames = [single_stay_data]
                            
                            for hadm_id in multi_stay_hadms:
                                # è·å–è¯¥ hadm_id çš„æ•°æ®
                                hadm_unique = unique_data[unique_data['hadm_id'] == hadm_id].copy()
                                if hadm_unique.empty:
                                    continue
                                    
                                # è·å–è¯¥ hadm_id çš„ stay ä¿¡æ¯ï¼ŒæŒ‰ outtime æ’åº
                                hadm_stays = stay_info[stay_info['hadm_id'] == hadm_id].copy()
                                # ğŸ”§ FIX: è¿‡æ»¤æ‰ outtime ä¸ºç©ºçš„è¡Œï¼Œé¿å… merge_asof æŠ¥é”™
                                # "Merge keys contain null values on right side"
                                hadm_stays = hadm_stays.dropna(subset=['outtime'])
                                if hadm_stays.empty:
                                    continue
                                hadm_stays = hadm_stays.sort_values('outtime')
                                stays_list = hadm_stays[target_id_col].tolist()
                                outtimes_list = hadm_stays['outtime'].tolist()
                                
                                # ğŸ”§ FIX: è¿‡æ»¤æ‰æ—¶é—´åˆ—ä¸ºç©ºçš„è¡Œï¼Œé¿å… merge_asof æŠ¥é”™
                                # "Merge keys contain null values on left side"
                                hadm_unique = hadm_unique.dropna(subset=[time_col])
                                if hadm_unique.empty:
                                    continue
                                
                                # ç¡®ä¿æ•°æ®æŒ‰æ—¶é—´æ’åº
                                hadm_unique = hadm_unique.sort_values(time_col)
                                
                                # ğŸ”¥ å…³é”®ä¿®æ­£ï¼šä½¿ç”¨ outtime è€Œä¸æ˜¯ intime åš rolling join
                                # direction='forward' ç­‰ä»·äº roll = -Infï¼ˆå‘æœªæ¥æ»šåŠ¨ï¼‰
                                # æ‰¾ outtime >= charttime çš„æœ€è¿‘ stay
                                merge_cols = [target_id_col, 'outtime']
                                if 'intime' in hadm_stays.columns:
                                    merge_cols.append('intime')
                                merged = pd.merge_asof(
                                    hadm_unique,
                                    hadm_stays[merge_cols],
                                    left_on=time_col,
                                    right_on='outtime',
                                    direction='forward',  # å‘æœªæ¥æ»šåŠ¨ï¼šæ‰¾ outtime >= charttime
                                    allow_exact_matches=True
                                )
                                
                                # å¤„ç† rollends = TRUE: 
                                # å¦‚æœ charttime > æœ€åä¸€ä¸ª outtimeï¼Œåˆ†é…ç»™æœ€åä¸€ä¸ª stay
                                last_stay = stays_list[-1]
                                last_outtime = outtimes_list[-1]
                                merged.loc[merged[target_id_col].isna(), target_id_col] = last_stay
                                merged.loc[merged['outtime'].isna(), 'outtime'] = last_outtime
                                
                                # ç¡®ä¿ id æ˜¯æ•´æ•°
                                merged[target_id_col] = merged[target_id_col].astype(int)
                                
                                result_frames.append(merged)
                            
                            frame = pd.concat(result_frames, ignore_index=True)
                            
                            if verbose:
                                logger.debug(f"[{table_name}] rolling join å®Œæˆ: {after_join_rows} â†’ {len(frame)} è¡Œ")
                    
                    # æ¸…ç†ä¸´æ—¶çš„ intime å’Œ outtime åˆ—
                    for col in ['intime', 'outtime']:
                        if col in frame.columns:
                            frame = frame.drop(columns=[col], errors='ignore')
                    
                    after_rows = len(frame)
                    
                    # âœ… å…³é”®ä¿®å¤ï¼šjoin åå¿…é¡»å†æ¬¡åº”ç”¨åŸå§‹ stay_id/icustay_id è¿‡æ»¤
                    # å› ä¸º join å¯èƒ½äº§ç”Ÿäº†é¢å¤–çš„ stay_ids (åŒä¸€ subject æˆ– hadm_id çš„å¤šä¸ª ICU stays)
                    # 
                    # ä¸‰ç§æƒ…å†µï¼š
                    # 1. å¦‚æœåŸå§‹è¿‡æ»¤å™¨æ˜¯ stay_id/icustay_idï¼Œä½¿ç”¨ä¿å­˜çš„ original_stay_ids
                    # 2. å¦‚æœåŸå§‹è¿‡æ»¤å™¨æ˜¯ subject_idï¼Œä» FilterSpec.metadata ä¸­æå–åŸå§‹ stay_id
                    # 3. ä» icustays_filters ä¸­æŸ¥æ‰¾
                    target_stay_ids = original_stay_ids
                    
                    if not target_stay_ids and icustays_filters:
                        for spec in icustays_filters:
                            if spec.column in ['stay_id', 'icustay_id'] and spec.op == FilterOp.IN:
                                target_stay_ids = set(spec.value)
                                if verbose:
                                    logger.debug(f"[{table_name}] ä» {spec.column} è¿‡æ»¤å™¨è·å–: {len(target_stay_ids)} stays")
                                break
                            elif spec.column == 'subject_id' and spec.op == FilterOp.IN:
                                # ä» metadata ä¸­æå–åŸå§‹ stay_ids
                                if spec.metadata and 'original_stay_ids' in spec.metadata:
                                    target_stay_ids = set(spec.metadata['original_stay_ids'])
                                    if verbose:
                                        logger.debug(f"[{table_name}] ä» subject_id è¿‡æ»¤å™¨çš„ metadata è·å–åŸå§‹ {target_id_col}: {len(target_stay_ids)} stays")
                                    break
                    
                    if target_stay_ids:
                        before_filter = len(frame)
                        if target_id_col in frame.columns:
                            frame = frame[frame[target_id_col].isin(target_stay_ids)]
                            if verbose:
                                logger.debug(
                                    f"[{table_name}] åº”ç”¨ {target_id_col} è¿‡æ»¤: {before_filter}è¡Œ â†’ {len(frame)}è¡Œ "
                                    f"(ä¿ç•™ {len(target_stay_ids)} ä¸ªç›®æ ‡ {target_id_col})"
                                )
                        else:
                            if verbose:
                                logger.warning(f"[{table_name}] join åä»æ—  {target_id_col} åˆ—ï¼Œæ— æ³•åº”ç”¨è¿‡æ»¤")
                    
                    # è®°å½•è¡¥å…¨æ“ä½œ
                    if verbose and before_rows != after_rows:
                        logger.info(
                            "è¡¨ %s: é€šè¿‡ hadm_id è¡¥å…¨ %s (%d â†’ %d è¡Œ)",
                            table_name,
                            target_id_col,
                            before_rows,
                            after_rows
                        )
                    
                    # âœ… å…³é”®ä¿®å¤ï¼šè¡¥å…¨ stay_id åï¼Œæ›´æ–° id_columns
                    # è¿™æ ·ä¸‹æ¸¸ concept.py ä¼šä¿ç•™ stay_id åˆ—è€Œä¸æ˜¯åªä¿ç•™ subject_id
                    if 'stay_id' in frame.columns:
                        id_columns = ['stay_id']
                        if verbose:
                            logger.debug(f"[{table_name}] è¡¥å…¨ {target_id_col} åæ›´æ–° id_columns: subject_id â†’ {target_id_col}")
                        
                except Exception as e:
                    # å¦‚æœè¡¥å…¨å¤±è´¥ï¼Œè®°å½•è­¦å‘Šä½†ä¸ä¸­æ–­æµç¨‹
                    logger.warning(
                        "âš ï¸  è¡¨ %s: æ— æ³•è¡¥å…¨ %s: %s",
                        table_name,
                        target_id_col,
                        str(e)
                    )

        if verbose and logger.isEnabledFor(logging.INFO):
            id_label = id_columns[0] if id_columns else defaults.id_var or "N/A"
            (
                frame[id_label].nunique()
                if id_label in frame.columns
                else "N/A"
            )
            # å‡å°‘æ—¥å¿—è¾“å‡ºï¼Œåªåœ¨ DEBUG æ¨¡å¼ä¸‹æ˜¾ç¤º
            if DEBUG_MODE:
                logger.debug(
                    "è¡¨ %s: %d è¡Œ, %d ä¸ª %s",
                    table_name,
                    len(frame),
                    frame[id_label].nunique() if id_label in frame.columns else 0,
                    id_label,
                )

        return ICUTable(
            data=frame,
            id_columns=id_columns,
            index_column=index_column,
            value_column=value_column,
            unit_column=unit_column,
            time_columns=time_columns,
        )

    def _load_raw_frame(
        self,
        table_name: str,
        columns: Optional[Iterable[str]],
        patient_ids_filter: Optional[FilterSpec] = None,
        concept_itemid_filter: Optional[Tuple[str, set]] = None,  # ğŸš€ æ¦‚å¿µç‰¹å®š itemid è¿‡æ»¤å™¨
        wide_table_value_columns: Optional[List[str]] = None,  # ğŸš€ å®½è¡¨valueåˆ—ç”¨äºNULLè¿‡æ»¤
    ) -> pd.DataFrame:
        # ğŸ” è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºè¯·æ±‚çš„åˆ—ï¼ˆä»…åœ¨DEBUGçº§åˆ«æ˜¾ç¤ºï¼‰
        if columns:
            logger.debug(f"_load_raw_frame: table={table_name}, columns={list(columns)}")
        
        # ğŸš€ OPTIMIZATION: ç¼“å­˜é”®ä¸åŒ…å«patient_ids_filterä»¥å®ç°è·¨æ¦‚å¿µå…±äº«
        # å¯¹äºåŒä¸€æ‰¹æ‚£è€…çš„å¤šä¸ªæ¦‚å¿µåŠ è½½,åªåœ¨ç¬¬ä¸€æ¬¡è¯»å–è¡¨,åç»­ä»ç¼“å­˜ä¸­è¿‡æ»¤
        # è¿™å°†charteventsç­‰å¤§è¡¨çš„åŠ è½½ä»Næ¬¡(æ¯æ¦‚å¿µä¸€æ¬¡)å‡å°‘åˆ°1æ¬¡
        # ğŸ”§ FIX: inputevents ç°åœ¨ä¹Ÿå¯ä»¥ç¼“å­˜ï¼Œå› ä¸º key ä¸­åŒ…å«äº† filter ä¿¡æ¯
        # ä¹‹å‰æ’é™¤ inputevents æ˜¯å› ä¸ºæ‹…å¿ƒ subject_idâ†’stay_id æ˜ å°„é—®é¢˜
        # ä½†å®é™…ä¸Š inputevents è¡¨æœ‰ stay_id åˆ—ï¼Œå¯ä»¥ç›´æ¥è¿‡æ»¤
        # ğŸ”§ HiRID observations: ç”±äºæ¦‚å¿µç‰¹å®šçš„ itemid è¿‡æ»¤ï¼Œä¸åŒæ¦‚å¿µæœ‰ä¸åŒæ•°æ®ï¼Œç¦ç”¨ç¼“å­˜
        # ğŸ”§ FIX: åˆ†æ¡¶ç›®å½•(numericitems_bucketç­‰)ä¹Ÿéœ€è¦ç¦ç”¨ç¼“å­˜æˆ–åŒ…å«itemidåœ¨keyä¸­
        skip_cache_tables = ['microbiologyevents', 'admissions', 'observations']  # æ·»åŠ  observations
        enable_caching = self.enable_cache and table_name not in skip_cache_tables
        
        # ğŸ”§ FIX: å¦‚æœè¡¨æ˜¯ç»è¿‡è¿‡æ»¤åŠ è½½çš„ï¼Œå¿…é¡»å°†filteråŒ…å«åœ¨cache keyä¸­
        # å¦åˆ™ä¸åŒæ‰¹æ¬¡çš„åŠ è½½ä¼šæ··æ·†
        filter_key = None
        if patient_ids_filter:
            val = patient_ids_filter.value
            if isinstance(val, (list, tuple)):
                val = tuple(val)
            elif isinstance(val, set):
                val = tuple(sorted(val))
            elif hasattr(val, 'tolist'):  # numpy array
                val = tuple(val.tolist())
            # åŒ…å«åˆ—åå’Œæ“ä½œç¬¦ï¼Œç¡®ä¿å”¯ä¸€æ€§
            filter_key = (patient_ids_filter.column, patient_ids_filter.op, val)

        # ğŸ”§ FIX: åˆ†æ¡¶è¯»å–æ—¶ï¼Œconcept_itemid_filter ä¹Ÿéœ€è¦åŠ å…¥ç¼“å­˜key
        # å¦åˆ™ä¸åŒæ¦‚å¿µï¼ˆä¸åŒitemidï¼‰ä¼šé”™è¯¯å…±äº«ç¼“å­˜
        itemid_filter_key = None
        if concept_itemid_filter:
            col, ids = concept_itemid_filter
            itemid_filter_key = (col, tuple(sorted(ids)))

        cache_key = (table_name, tuple(sorted(columns)) if columns else None, filter_key, itemid_filter_key)
        
        # æ£€æŸ¥ç¼“å­˜
        cached_frame = None
        if enable_caching:
            with self._lock:
                cached_frame = self._table_cache.get(cache_key)
        
        if cached_frame is not None:
            # ğŸš€ OPTIMIZATION: ä»ç¼“å­˜ä¸­å–æ•°æ®åå†åº”ç”¨patientè¿‡æ»¤
            # è¿™æ ·å¤šä¸ªæ¦‚å¿µå¯ä»¥å…±äº«åŒä¸€ä¸ªç¼“å­˜çš„è¡¨å‰¯æœ¬
            # âš¡ æ€§èƒ½ä¼˜åŒ–: é¿å…copy(),ç›´æ¥è¿”å›è¿‡æ»¤åçš„è§†å›¾
            logger.debug(f"ä»ç¼“å­˜åŠ è½½: table={table_name}, cached_columns={list(cached_frame.columns)}")
            if patient_ids_filter:
                # å¦‚æœç¼“å­˜çš„keyå·²ç»åŒ…å«äº†filterï¼Œé‚£ä¹ˆcached_frameå·²ç»æ˜¯è¿‡æ»¤è¿‡çš„äº†
                # ä½†ä¸ºäº†å®‰å…¨èµ·è§ï¼ˆæˆ–è€…å¦‚æœfilter_keyé€»è¾‘æœ‰å˜ï¼‰ï¼Œå†æ¬¡æ£€æŸ¥
                # å¦‚æœfilter_keyå­˜åœ¨ï¼Œè¯´æ˜cached_frameå·²ç»æ˜¯é’ˆå¯¹è¯¥filterçš„å­é›†
                # æ­¤æ—¶å†æ¬¡åº”ç”¨filteråº”è¯¥æ˜¯å®‰å…¨çš„ï¼ˆno-opï¼‰
                return patient_ids_filter.apply(cached_frame)
            # å¦‚æœä¸éœ€è¦è¿‡æ»¤ï¼Œè¿”å›åˆ‡ç‰‡è§†å›¾è€Œéå‰¯æœ¬
            return cached_frame[:]
        
        loader = self._table_sources.get(table_name)
        dataset_cfg = self._dataset_sources.get(table_name)
        if loader is None and dataset_cfg is not None:
            frame = self._read_dataset(table_name, dataset_cfg, columns, patient_ids_filter)
        elif loader is None:
            # ğŸš€ ä¼˜å…ˆæ£€æŸ¥åˆ†æ¡¶ç›®å½•ï¼ˆæ— è®ºæ˜¯å•æ–‡ä»¶è¿˜æ˜¯å¤šæ–‡ä»¶é…ç½®ï¼‰
            # åˆ†æ¡¶ç›®å½•æ€§èƒ½è¿œä¼˜äºæ™®é€šç›®å½•æˆ–å•ä¸ªparquet
            bucket_loader = self._resolve_bucket_directory(table_name)
            if bucket_loader is not None:
                loader = bucket_loader
            else:
                # ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦ä¸ºå¤šæ–‡ä»¶é…ç½®ï¼Œå¦‚æœæ˜¯ï¼Œä½¿ç”¨ç›®å½•è·¯å¾„
                table_cfg = self.config.get_table(table_name)
                if len(table_cfg.files) > 1:
                    # å¤šæ–‡ä»¶é…ç½®ï¼šä½¿ç”¨ç›®å½•è·¯å¾„ä»¥å¯ç”¨å¤šæ–‡ä»¶è¯»å–
                    base_path = self.base_path or Path.cwd()
                    if table_cfg.files:
                        # HiRIDç‰¹æ®Šå¤„ç†ï¼šé…ç½®ä¸­çš„CSVè·¯å¾„ä¸å®é™…parquetç›®å½•ä¸åŒ
                        # observation_tables/csv/ -> observations/
                        # pharma_records/csv/ -> pharma/
                        if self.config.name == 'hirid':
                            hirid_table_dir_mapping = {
                                'observations': 'observations',
                                'pharma': 'pharma',
                            }
                            if table_name in hirid_table_dir_mapping:
                                mapped_dir = base_path / hirid_table_dir_mapping[table_name]
                                if mapped_dir.is_dir():
                                    parquet_files = list(mapped_dir.glob("*.parquet")) + list(mapped_dir.glob("*.pq"))
                                    if parquet_files:
                                        loader = mapped_dir
                        
                        # å¦‚æœHiRIDæ˜ å°„æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤é€»è¾‘
                        if loader is None:
                            # è·å–ç›®å½•è·¯å¾„ï¼ˆä»ç¬¬ä¸€ä¸ªæ–‡ä»¶è·¯å¾„ä¸­æå–ï¼‰
                            first_file = table_cfg.files[0]
                            # å¤„ç†å­—ç¬¦ä¸²æˆ–å­—å…¸æ ¼å¼
                            if isinstance(first_file, dict):
                                first_path = Path(first_file.get('path', first_file.get('name', '')))
                            else:
                                first_path = Path(first_file)
                            
                            multi_file_dir = base_path / first_path.parent
                            if multi_file_dir.is_dir():
                                loader = multi_file_dir
                            else:
                                # å›é€€åˆ°å•ä¸ªæ–‡ä»¶è§£æ
                                loader = self._resolve_loader_from_disk(table_name)
                    else:
                        # å›é€€åˆ°å•ä¸ªæ–‡ä»¶è§£æ
                        loader = self._resolve_loader_from_disk(table_name)
                else:
                    loader = self._resolve_loader_from_disk(table_name)
            
            # ğŸš€ MIMIC-III chartevents CSV fallback
            # When bucket directory is not available (due to memory constraints during conversion),
            # fall back to reading directly from CSV.gz with proper VALUE type handling
            if loader is None and self.config.name == 'mimic' and table_name == 'chartevents':
                csv_path = self._resolve_mimic3_chartevents_csv()
                if csv_path is not None and concept_itemid_filter is not None:
                    # Use CSV fallback only when we have itemid filter (for performance)
                    logger.info(f"ğŸ”„ MIMIC-III chartevents: åˆ†æ¡¶ç›®å½•ä¸å­˜åœ¨ï¼Œä½¿ç”¨ CSV å›é€€æ¨¡å¼")
                    frame = self._read_mimic3_csv_fallback(
                        csv_path=csv_path,
                        columns=columns,
                        itemid_filter=concept_itemid_filter,
                        patient_ids_filter=patient_ids_filter,
                    )
                    
                    if not frame.empty:
                        # Cache and return
                        if enable_caching:
                            with self._lock:
                                self._table_cache[cache_key] = frame
                        return frame
                    # If CSV fallback returns empty, continue to normal error handling
            
            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›ç©º DataFrameï¼ˆå…¼å®¹æ€§å¤„ç†ï¼Œé¿å…é˜»æ–­æ•´ä¸ªæµç¨‹ï¼‰
            if loader is None:
                # å¯¹äºmiivæ•°æ®æºï¼Œå¦‚æœè¡¨åœ¨é…ç½®ä¸­å®šä¹‰äº†ä½†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºDataFrame
                # è¿™å…è®¸åœ¨demoæ•°æ®ä¸­ç¼ºå°‘æŸäº›è¡¨æ—¶ç»§ç»­è¿è¡Œ
                if self.config.name == 'miiv' and table_name in self.config.tables:
                    logger.warning(f"Table {table_name} not found on disk, returning empty DataFrame")
                    # è¿”å›ç©ºDataFrameï¼Œä¿æŒä¸é…ç½®ä¸­è¡¨ç»“æ„ä¸€è‡´çš„åˆ—
                    table_cfg = self.config.get_table(table_name)
                    defaults = table_cfg.defaults
                    # å°è¯•ä»é…ç½®ä¸­è·å–é¢„æœŸçš„åˆ—
                    expected_cols = []
                    if defaults.id_var:
                        expected_cols.append(defaults.id_var)
                    if defaults.index_var:
                        expected_cols.append(defaults.index_var)
                    if defaults.val_var:
                        expected_cols.append(defaults.val_var)
                    if defaults.unit_var:
                        expected_cols.append(defaults.unit_var)
                    if defaults.time_vars:
                        expected_cols.extend(defaults.time_vars)
                    
                    # è¿”å›ç©ºDataFrameï¼Œé¿å…æŠ›å‡ºé”™è¯¯
                    return pd.DataFrame(columns=expected_cols if expected_cols else ['index'])
                
                raise KeyError(
                    f"No table source registered for '{table_name}' "
                    f"in data source '{self.config.name}'"
                )
        if callable(loader):
            frame = loader()
        else:
            frame = self._read_file(
                Path(loader), columns, 
                patient_ids_filter=patient_ids_filter, 
                table_name=table_name,
                concept_itemid_filter=concept_itemid_filter,  # ğŸš€ ä¼ é€’æ¦‚å¿µç‰¹å®šè¿‡æ»¤å™¨
                wide_table_value_columns=wide_table_value_columns  # ğŸš€ ä¼ é€’å®½è¡¨valueåˆ—ç”¨äºNULLè¿‡æ»¤
            )

        if columns is not None:
            missing = set(columns) - set(frame.columns)
            if missing:
                raise KeyError(
                    f"Columns {sorted(missing)} not found in table '{table_name}'"
                )
            frame = frame[list(columns)]
        
        # ğŸš€ OPTIMIZATION: ç¼“å­˜å®Œæ•´è¡¨(æœªç»patientè¿‡æ»¤)ä»¥å®ç°è·¨æ¦‚å¿µå…±äº«
        # patientè¿‡æ»¤åœ¨ä»ç¼“å­˜è¯»å–æ—¶åº”ç”¨(è§ä¸Šé¢cached_frameåˆ†æ”¯)
        # âš¡ æ€§èƒ½ä¼˜åŒ–: ç¼“å­˜åŸå§‹frameï¼Œè¿”å›è¿‡æ»¤åçš„ç»“æœ
        # ä¸ç¼“å­˜éœ€è¦ç‰¹æ®Šå¤„ç†çš„è¡¨ï¼ˆlabevents/admissionsç­‰ï¼‰
        if enable_caching:
            with self._lock:
                # ç¼“å­˜åŸå§‹æœªè¿‡æ»¤çš„è¡¨
                self._table_cache[cache_key] = frame
        
        # åº”ç”¨patientè¿‡æ»¤(å¦‚æœæœ‰)
        if patient_ids_filter:
            return patient_ids_filter.apply(frame)
        
        # æœªè¿‡æ»¤ä¸”æœªç¼“å­˜æ—¶è¿”å›åˆ‡ç‰‡
        return frame[:] if self.enable_cache else frame

    def _resolve_bucket_directory(self, table_name: str) -> Optional[Path]:
        """
        ğŸš€ ä¼˜å…ˆæ£€æŸ¥åˆ†æ¡¶ç›®å½•ï¼ˆæ€§èƒ½æœ€ä¼˜ï¼‰
        
        åˆ†æ¡¶ç›®å½•ä½¿ç”¨ bucket_id=* å­ç›®å½•ç»“æ„ï¼Œé€šè¿‡ hash(itemid) % num_buckets å®ç°
        è¯»å–æ—¶åªéœ€æ‰«æç›¸å…³æ¡¶ï¼Œè·³è¿‡ 99% æ— å…³æ•°æ®
        
        æ£€æŸ¥ä½ç½®ï¼š
        - base_path / {table_name}_bucket
        - base_path / icu / {table_name}_bucket  (MIIV)
        - base_path / hosp / {table_name}_bucket  (MIIV)
        
        Returns:
            åˆ†æ¡¶ç›®å½•è·¯å¾„ï¼ˆå¦‚æœå­˜åœ¨ä¸”æœ‰æ•ˆï¼‰ï¼Œå¦åˆ™ None
        """
        if not self.base_path:
            return None
        
        # å¯èƒ½çš„è¡¨åå˜ä½“
        name_variants = [table_name, table_name.lower()]
        
        # å¯èƒ½çš„åˆ†æ¡¶ç›®å½•ä½ç½®
        for name in name_variants:
            possible_bucket_dirs = [
                self.base_path / f"{name}_bucket",  # ç›´æ¥åœ¨ base_path ä¸‹
                self.base_path / "icu" / f"{name}_bucket",  # MIIV icu å­ç›®å½•
                self.base_path / "hosp" / f"{name}_bucket",  # MIIV hosp å­ç›®å½•
            ]
            for bucket_dir in possible_bucket_dirs:
                if bucket_dir.is_dir():
                    # æ£€æŸ¥æ˜¯å¦æœ‰ bucket_id=* å­ç›®å½•ï¼ˆåˆ†æ¡¶æ ¼å¼æ ‡è¯†ï¼‰
                    bucket_subdirs = list(bucket_dir.glob("bucket_id=*"))
                    if bucket_subdirs:
                        # ğŸ”§ é¿å…é‡å¤æ—¥å¿—ï¼šåªåœ¨é¦–æ¬¡å‘ç°æ—¶æ‰“å°info
                        bucket_key = str(bucket_dir)
                        if bucket_key not in self._bucket_dir_logged:
                            self._bucket_dir_logged.add(bucket_key)
                            logger.info(f"ğŸª£ ä½¿ç”¨åˆ†æ¡¶ç›®å½•: {bucket_dir} ({len(bucket_subdirs)} ä¸ªæ¡¶)")
                        return bucket_dir
        
        return None

    def _resolve_loader_from_disk(self, table_name: str) -> Optional[Callable[[], pd.DataFrame] | Path]:
        if not self.base_path:
            return None
        
        # ğŸš€ ä¼˜å…ˆçº§æœ€é«˜ï¼šæ£€æŸ¥åˆ†æ¡¶ç›®å½•ï¼ˆæ€§èƒ½æœ€ä¼˜ï¼‰
        # åˆ†æ¡¶ç›®å½•å‘½åè§„åˆ™ï¼š{table_name}_bucket
        # å¿…é¡»åœ¨æ£€æŸ¥é…ç½®æ–‡ä»¶ä¹‹å‰ï¼Œå› ä¸ºåˆ†æ¡¶ç›®å½•æ˜¯æ€§èƒ½ä¼˜åŒ–çš„å…³é”®
        possible_bucket_dirs = [
            self.base_path / f"{table_name}_bucket",  # ç›´æ¥åœ¨ base_path ä¸‹
            self.base_path / "icu" / f"{table_name}_bucket",  # MIIV icu å­ç›®å½•
            self.base_path / "hosp" / f"{table_name}_bucket",  # MIIV hosp å­ç›®å½•
        ]
        for bucket_dir in possible_bucket_dirs:
            if bucket_dir.is_dir():
                bucket_subdirs = list(bucket_dir.glob("bucket_id=*"))
                if bucket_subdirs:
                    # ğŸ”§ é¿å…é‡å¤æ—¥å¿—ï¼šåªåœ¨é¦–æ¬¡å‘ç°æ—¶æ‰“å°info
                    bucket_key = str(bucket_dir)
                    if bucket_key not in self._bucket_dir_logged:
                        self._bucket_dir_logged.add(bucket_key)
                        logger.info(f"ğŸª£ ä½¿ç”¨åˆ†æ¡¶ç›®å½•: {bucket_dir} ({len(bucket_subdirs)} ä¸ªæ¡¶)")
                    return bucket_dir
        
        table_cfg = self.config.get_table(table_name)
        explicit = table_cfg.first_file()
        
        if explicit:
            explicit_path = self.base_path / explicit
            if explicit_path.exists():
                # Accept directories (partitioned datasets) and Parquet files immediately
                if explicit_path.is_dir():
                    return explicit_path
                if explicit_path.suffix.lower() in {".parquet", ".pq"}:
                    return explicit_path
                # Otherwise continue searching for a Parquet counterpart below
        
        # MIMIC-IV æ–‡ä»¶åæ˜ å°„: é…ç½®ä¸­çš„è¡¨å -> æ–‡ä»¶ç³»ç»Ÿä¸­çš„å®é™…æ–‡ä»¶å
        # å› ä¸º MIMIC-IV æ”¹äº†è¡¨å,ä½†é…ç½®æ–‡ä»¶è¿˜æ˜¯ç”¨çš„æ—§å
        # MIMIC-IV å°† MIMIC-III çš„ä¸¤ä¸ªè¡¨åˆå¹¶äº†:
        # - procedureevents_mv -> procedureevents
        # - inputevents_cv å’Œ inputevents_mv -> inputevents
        # æ³¨æ„ï¼šå¯¹äºmiivæ•°æ®æºï¼Œæ¦‚å¿µå­—å…¸ä¸­ç›´æ¥ä½¿ç”¨procedureeventså’Œinputevents
        # ä½†å¦‚æœæ•°æ®æºé…ç½®ä¸­æ²¡æœ‰å®šä¹‰è¿™äº›è¡¨ï¼Œéœ€è¦ä»æ—§è¡¨åæ˜ å°„

        config_to_file_mappings = {
            'procedureevents_mv': 'procedureevents',  # é…ç½®å -> æ–‡ä»¶å
            'inputevents_mv': 'inputevents',
            'inputevents_cv': 'inputevents',  # MIMIC-IV åˆå¹¶äº†è¿™ä¸¤ä¸ªè¡¨
        }
        
        # å¯¹äºmiivæ•°æ®æºï¼Œå¦‚æœè¯·æ±‚çš„è¡¨åœ¨é…ç½®ä¸­ä¸å­˜åœ¨ï¼Œå°è¯•ä»æ˜ å°„ä¸­æŸ¥æ‰¾
        # ä¾‹å¦‚ï¼šæ¦‚å¿µå­—å…¸è¯·æ±‚procedureeventsï¼Œä½†é…ç½®ä¸­å¯èƒ½åªæœ‰procedureevents_mvçš„å®šä¹‰
        if self.config.name == 'miiv':
            # åå‘æ˜ å°„ï¼šå¦‚æœè¯·æ±‚çš„è¡¨åä¸åœ¨é…ç½®ä¸­ï¼Œå°è¯•æŸ¥æ‰¾å¯¹åº”çš„æ—§è¡¨å
            reverse_mapping = {
                'procedureevents': 'procedureevents_mv',
                'inputevents': 'inputevents_mv',  # ä¼˜å…ˆä½¿ç”¨inputevents_mv
            }
            if table_name not in self.config.tables and table_name in reverse_mapping:
                # ä½¿ç”¨æ˜ å°„åçš„è¡¨åæŸ¥æ‰¾æ–‡ä»¶
                mapped_table = reverse_mapping[table_name]
                file_base_name = config_to_file_mappings.get(mapped_table, table_name)
            else:
                file_base_name = config_to_file_mappings.get(table_name, table_name)
        else:
            # è·å–å®é™…è¦æŸ¥æ‰¾çš„æ–‡ä»¶å
            file_base_name = config_to_file_mappings.get(table_name, table_name)

            # ä»è¡¨é…ç½®ä¸­è·å–å®é™…çš„æ–‡ä»¶åï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if table_name in self.config.tables:
                table_config = self.config.tables[table_name]
                if hasattr(table_config, 'files') and table_config.files:
                    # è·å–ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„è·¯å¾„ï¼Œå»æ‰æ‰©å±•å
                    file_info = table_config.files[0]
                    if isinstance(file_info, dict) and 'path' in file_info:
                        file_path = file_info['path']
                    elif hasattr(file_info, 'path'):
                        file_path = file_info.path
                    else:
                        file_path = str(file_info)
                    # å»æ‰æ‰©å±•åï¼Œè·å–åŸºç¡€æ–‡ä»¶åï¼ˆå¤„ç†å¤åˆæ‰©å±•åå¦‚.csv.gzï¼‰
                    parts = file_path.split('.')
                    if len(parts) >= 2:
                        # å¤„ç†å¤åˆæ‰©å±•åå¦‚ .csv.gz
                        if parts[-1] == 'gz' and len(parts) >= 3 and parts[-2] == 'csv':
                            file_base_name = '.'.join(parts[:-2])
                        else:
                            file_base_name = '.'.join(parts[:-1])
                    else:
                        file_base_name = file_path
        
        # Only support Parquet format - try different name variations
        for name in [file_base_name, file_base_name.lower(), table_name, table_name.lower()]:
            # ğŸš€ ä¼˜å…ˆæ£€æŸ¥ bucket ç›®å½•ï¼ˆåˆ†æ¡¶æ ¼å¼ï¼Œæ€§èƒ½æœ€ä¼˜ï¼‰
            # å¿…é¡»åœ¨æ£€æŸ¥ .parquet æ–‡ä»¶ä¹‹å‰ï¼Œå› ä¸ºåˆ†æ¡¶ç›®å½•å¯èƒ½ä¸è¡¨åŒå
            # æ£€æŸ¥å¤šä¸ªå¯èƒ½çš„ä½ç½®ï¼šbase_path å’Œå­ç›®å½•ï¼ˆå¦‚ icu/, hosp/ï¼‰
            possible_bucket_dirs = [
                self.base_path / f"{name}_bucket",  # ç›´æ¥åœ¨ base_path ä¸‹
                self.base_path / "icu" / f"{name}_bucket",  # MIIV icu å­ç›®å½•
                self.base_path / "hosp" / f"{name}_bucket",  # MIIV hosp å­ç›®å½•
            ]
            for bucket_dir in possible_bucket_dirs:
                if bucket_dir.is_dir():
                    # æ£€æŸ¥æ˜¯å¦æœ‰ bucket_id=* å­ç›®å½•
                    bucket_subdirs = list(bucket_dir.glob("bucket_id=*"))
                    if bucket_subdirs:
                        # ğŸ”§ é¿å…é‡å¤æ—¥å¿—ï¼šåªåœ¨é¦–æ¬¡å‘ç°æ—¶æ‰“å°info
                        bucket_key = str(bucket_dir)
                        if bucket_key not in self._bucket_dir_logged:
                            self._bucket_dir_logged.add(bucket_key)
                            logger.info(f"ğŸª£ ä½¿ç”¨åˆ†æ¡¶ç›®å½•: {bucket_dir} ({len(bucket_subdirs)} ä¸ªæ¡¶)")
                        return bucket_dir
            
            # Try .parquet extension - æ£€æŸ¥å¤šä¸ªå¯èƒ½çš„ä½ç½®
            # MIMIC-IV çš„è¡¨å¯èƒ½åœ¨ icu/ æˆ– hosp/ å­ç›®å½•ä¸‹
            possible_parquet_paths = [
                self.base_path / f"{name}.parquet",  # ç›´æ¥åœ¨ base_path ä¸‹
                self.base_path / "icu" / f"{name}.parquet",  # MIIV icu å­ç›®å½•
                self.base_path / "hosp" / f"{name}.parquet",  # MIIV hosp å­ç›®å½•
            ]
            for parquet_candidate in possible_parquet_paths:
                if parquet_candidate.exists():
                    return parquet_candidate
            
            # Try .pq extension (short form) - åŒæ ·æ£€æŸ¥å­ç›®å½•
            possible_pq_paths = [
                self.base_path / f"{name}.pq",
                self.base_path / "icu" / f"{name}.pq",
                self.base_path / "hosp" / f"{name}.pq",
            ]
            for pq_candidate in possible_pq_paths:
                if pq_candidate.exists():
                    return pq_candidate
        
        # Check subdirectory for partitioned parquet data (common in hirid observations)
        if self.base_path is not None:
            for name in [table_name, table_name.lower()]:
                # æ£€æŸ¥æ™®é€šåˆ†åŒºç›®å½•
                subdir = self.base_path / name
                if subdir.is_dir():
                    # Look for Parquet files
                    parquet_files = list(subdir.glob("*.parquet")) + list(subdir.glob("*.pq"))
                    if parquet_files:
                        return subdir
        
        # Fall back to explicit file if it exists (e.g., CSV) so that callers can handle it
        if explicit:
            explicit_path = self.base_path / explicit
            if explicit_path.exists():
                return explicit_path

        return None

    def _get_minimal_columns(self, table_name: str) -> Optional[List[str]]:
        """è·å–è¡¨çš„æœ€å°å¿…è¦åˆ—é›†ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰"""
        return MINIMAL_COLUMNS.get(table_name)
    
    def _read_file(
        self, path: Path, columns: Optional[Iterable[str]], 
        patient_ids_filter: Optional[FilterSpec] = None, 
        table_name: Optional[str] = None,
        concept_itemid_filter: Optional[Tuple[str, set]] = None,  # ğŸš€ æ¦‚å¿µç‰¹å®š itemid è¿‡æ»¤å™¨
        wide_table_value_columns: Optional[List[str]] = None  # ğŸš€ å®½è¡¨valueåˆ—ç”¨äºNULLè¿‡æ»¤
    ) -> pd.DataFrame:
        # ğŸš€ å¤§è¡¨ itemid é¢„è¿‡æ»¤é…ç½®
        # æ£€æµ‹æ˜¯å¦ä¸ºéœ€è¦ itemid è¿‡æ»¤çš„å¤§è¡¨
        # ğŸ”§ 2024-12-02: é‡æ–°å¯ç”¨ç™½åå•è¿‡æ»¤ï¼Œç™½åå•å·²åŒ…å«æ‰€æœ‰ sofa2-dict.json ä¸­å®šä¹‰çš„ itemid
        itemid_filter_config = None
        db_name = self.config.name
        
        # ğŸš€ ä¼˜å…ˆä½¿ç”¨æ¦‚å¿µç‰¹å®šçš„ itemid è¿‡æ»¤å™¨ï¼ˆç²¾ç¡®è¿‡æ»¤ï¼Œæ€§èƒ½æœ€ä½³ï¼‰
        # å¦‚æœä¼ å…¥äº† concept_itemid_filterï¼Œç›´æ¥ä½¿ç”¨ï¼Œè·³è¿‡å…¨å±€ç™½åå•
        if concept_itemid_filter:
            itemid_filter_config = concept_itemid_filter
            if DEBUG_MODE:
                col, ids = concept_itemid_filter
                logger.info(f"ğŸ¯ ä½¿ç”¨æ¦‚å¿µç‰¹å®šè¿‡æ»¤: {col} IN {len(ids)} ä¸ª ID (ç²¾ç¡®æ¨¡å¼)")
        elif db_name == 'aumc' and table_name == 'numericitems':
            # AUMC numericitems: 80GB â†’ çº¦5GB
            itemid_filter_config = ('itemid', AUMC_NUMERICITEMS_ITEMIDS)
        elif db_name in ('miiv', 'mimic_demo') and table_name == 'chartevents':
            # MIIV chartevents: 11GB
            itemid_filter_config = ('itemid', MIIV_CHARTEVENTS_ITEMIDS)
        elif db_name in ('miiv', 'mimic_demo') and table_name == 'labevents':
            # MIIV labevents: 8GB
            itemid_filter_config = ('itemid', MIIV_LABEVENTS_ITEMIDS)
        elif db_name == 'eicu' and table_name == 'nursecharting':
            # eICU nursecharting: 4.3GB - ä½¿ç”¨å­—ç¬¦ä¸²åˆ—
            itemid_filter_config = ('nursingchartcelltypevalname', EICU_NURSECHARTING_IDS)
        elif db_name == 'hirid' and table_name == 'observations':
            # ğŸš€ HiRID observations: 7.77äº¿è¡Œ (~72GB) â†’ å¤§å¹…å‡å°‘
            # ä½¿ç”¨ variableid è¿‡æ»¤åªåŠ è½½æ¦‚å¿µå­—å…¸ä¸­å®šä¹‰çš„å˜é‡
            itemid_filter_config = ('variableid', HIRID_OBSERVATIONS_VARIABLEIDS)
        
        # Handle directory (partitioned data)
        if path.is_dir():
            if DEBUG_MODE:
                logger.debug(f"è¯»å–åˆ†åŒºç›®å½•: {path.name}, è¯·æ±‚åˆ—: {list(columns) if columns else 'å…¨éƒ¨åˆ—'}")
            # ğŸš€ ä¼˜å…ˆä½¿ç”¨ DuckDBï¼ˆå•æ‚£è€…/å°æ‰¹é‡æŸ¥è¯¢å¿« 5-6 å€ï¼‰
            # å¯¹äºå¤§æ‰¹é‡æ‚£è€…ï¼ˆ>100ï¼‰ï¼ŒPyArrow çš„å¹¶è¡Œè¯»å–æ›´ä¼˜
            use_duckdb = True
            if patient_ids_filter and patient_ids_filter.value:
                values = patient_ids_filter.value
                if isinstance(values, (list, tuple, set)):
                    use_duckdb = len(values) <= 100
                elif isinstance(values, pd.Series):
                    use_duckdb = len(values) <= 100
            
            if use_duckdb:
                return self._read_partitioned_data_duckdb(
                    path, columns, patient_ids_filter, 
                    itemid_filter_config=itemid_filter_config, 
                    table_name=table_name,
                    wide_table_value_columns=wide_table_value_columns  # ğŸš€ ä¼ é€’å®½è¡¨valueåˆ—ç”¨äºNULLè¿‡æ»¤
                )
            else:
                return self._read_partitioned_data_optimized(path, columns, patient_ids_filter, itemid_filter_config=itemid_filter_config)
        
        suffix = path.suffix.lower()
        [s.lower() for s in path.suffixes]
        
        # Preferred: Parquet format
        if suffix in {".parquet", ".pq"}:
            # ğŸš€ ä½¿ç”¨PyArrowè¿‡æ»¤å™¨ä¼˜åŒ–å¤§æ–‡ä»¶è¯»å–
            if patient_ids_filter and patient_ids_filter.op == FilterOp.IN:
                try:
                    import pyarrow.parquet as pq
                    import pyarrow as pa
                    # âš¡ ä½¿ç”¨é¢„è®¡ç®—çš„set
                    target_ids = list(patient_ids_filter._value_set)
                    
                    # ä½¿ç”¨PyArrowè¯»å–å¹¶è¿‡æ»¤ - ä½¿ç”¨ DNF æ ¼å¼
                    df = pq.read_table(
                        path,
                        columns=list(columns) if columns else None,
                        filters=[[(patient_ids_filter.column, 'in', target_ids)]]
                    ).to_pandas()
                except (ImportError, Exception):
                    # å¦‚æœPyArrowè¿‡æ»¤å¤±è´¥ï¼Œå›é€€åˆ°pandasåè¿‡æ»¤
                    df = pd.read_parquet(path, columns=list(columns) if columns else None, engine='pyarrow')
                    if patient_ids_filter.column in df.columns:
                        df = patient_ids_filter.apply(df)
            else:
                df = pd.read_parquet(path, columns=list(columns) if columns else None, engine='pyarrow')
            
            # å¤„ç†é‡å¤åˆ—åï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if df.columns.duplicated().any():
                import pandas.io.common
                df.columns = pandas.io.common.dedup_names(df.columns, is_potential_multiindex=False)
            return df
        
        raise ValueError(
            f"Unsupported file format '{path.suffix}' for {path.name}. Only Parquet format is supported."
        )
    
    def _compute_target_buckets(self, itemids: set, num_buckets: int, duckdb_module) -> set:
        """ä½¿ç”¨ DuckDB hash è®¡ç®— itemid å¯¹åº”çš„ç›®æ ‡æ¡¶ ID
        
        å¿…é¡»ä½¿ç”¨ DuckDB çš„ hash() å‡½æ•°ï¼Œå› ä¸ºåˆ†æ¡¶æ—¶ä½¿ç”¨çš„æ˜¯ DuckDB hashã€‚
        Python çš„ hash() å‡½æ•°ä¸ DuckDB ä¸ä¸€è‡´ï¼
        
        Args:
            itemids: itemid é›†åˆ
            num_buckets: æ€»æ¡¶æ•°
            duckdb_module: å·²å¯¼å…¥çš„ duckdb æ¨¡å—
            
        Returns:
            ç›®æ ‡æ¡¶ ID é›†åˆ
        """
        conn = duckdb_module.connect()
        itemid_list = list(itemids)
        conn.execute("CREATE TEMP TABLE items AS SELECT UNNEST(?) as itemid", [itemid_list])
        result = conn.execute(f"SELECT DISTINCT hash(itemid) % {num_buckets} FROM items").fetchall()
        conn.close()
        return {row[0] for row in result}
    
    def _read_mimic3_csv_fallback(
        self,
        csv_path: Path,
        columns: Optional[Iterable[str]],
        itemid_filter: Optional[Tuple[str, set]],
        patient_ids_filter: Optional[FilterSpec] = None,
    ) -> pd.DataFrame:
        """ğŸš€ MIMIC-III chartevents CSV fallback: read directly from CSV.gz with correct VALUE type
        
        When chartevents_bucket directory doesn't exist (due to memory constraints during conversion),
        this method reads directly from the original CSV.gz file using DuckDB with proper type hints.
        
        The key issue is that DuckDB's read_csv_auto incorrectly detects the VALUE column as DOUBLE
        when early rows have numeric values, causing text values like "4 Spontaneously" (GCS scores)
        to become NaN. We use types={'VALUE': 'VARCHAR'} to preserve these text values.
        
        Args:
            csv_path: Path to CHARTEVENTS.csv.gz
            columns: Columns to select (will be uppercased for MIMIC-III)
            itemid_filter: (column_name, set_of_itemids) for filtering
            patient_ids_filter: Optional patient ID filter
            
        Returns:
            DataFrame with the requested data
        """
        try:
            import duckdb
        except ImportError:
            logger.warning("DuckDB not installed, cannot use MIMIC-III CSV fallback")
            return pd.DataFrame()
        
        logger.info(f"ğŸ“„ MIMIC-III CSV å›é€€æ¨¡å¼: ä» {csv_path.name} è¯»å– (VALUE åˆ—ä¿æŒä¸ºå­—ç¬¦ä¸²)")
        
        # Build column selection - MIMIC-III uses UPPERCASE column names in CSV
        if columns:
            # Map common column names to MIMIC-III uppercase
            col_mapping = {
                'icustay_id': 'ICUSTAY_ID',
                'subject_id': 'SUBJECT_ID', 
                'hadm_id': 'HADM_ID',
                'charttime': 'CHARTTIME',
                'itemid': 'ITEMID',
                'value': 'VALUE',
                'valuenum': 'VALUENUM',
                'valueuom': 'VALUEUOM',
            }
            upper_cols = []
            for c in columns:
                upper_cols.append(col_mapping.get(c.lower(), c.upper()))
            columns_sql = ", ".join(upper_cols)
        else:
            columns_sql = "*"
        
        # Build WHERE conditions
        where_conditions = []
        
        # ITEMID filter (critical for performance - filters 330M rows to small subset)
        if itemid_filter:
            filter_col, filter_ids = itemid_filter
            # MIMIC-III uses uppercase ITEMID
            filter_col_upper = filter_col.upper()
            itemids_list = ", ".join(str(int(x)) for x in filter_ids)
            where_conditions.append(f"{filter_col_upper} IN ({itemids_list})")
            logger.debug(f"ğŸ¯ CSV è¿‡æ»¤: {filter_col_upper} IN ({len(filter_ids)} ä¸ª ID)")
        
        # Patient ID filter
        if patient_ids_filter and patient_ids_filter.value:
            id_col = patient_ids_filter.column.upper()  # MIMIC-III uses uppercase
            values = patient_ids_filter.value
            if isinstance(values, (list, tuple, set)):
                value_list = list(values)
            elif isinstance(values, pd.Series):
                value_list = values.tolist()
            else:
                value_list = [values]
            
            if value_list:
                if len(value_list) == 1:
                    where_conditions.append(f"{id_col} = {value_list[0]}")
                else:
                    values_str = ", ".join(map(str, value_list))
                    where_conditions.append(f"{id_col} IN ({values_str})")
        
        # Build WHERE clause
        where_clause = ""
        if where_conditions:
            where_clause = "WHERE " + " AND ".join(where_conditions)
        
        # ğŸ”‘ CRITICAL: Use types={'VALUE': 'VARCHAR'} to preserve GCS text values
        # Without this, values like "4 Spontaneously" become NaN because DuckDB
        # incorrectly detects VALUE as DOUBLE from early numeric rows
        # NOTE: Do NOT use sample_size=-1 as it causes full file scan for type detection
        query = f"""
            SELECT {columns_sql}
            FROM read_csv_auto(
                '{csv_path}',
                ignore_errors=true,
                null_padding=true,
                types={{'VALUE': 'VARCHAR'}}
            )
            {where_clause}
        """
        
        try:
            con = duckdb.connect()
            con.execute("SET timezone='UTC'")
            con.execute("SET enable_progress_bar = false")
            con.execute("SET enable_progress_bar_print = false")
            df = con.execute(query).fetchdf()
            con.close()
            
            # Normalize column names to lowercase (MIMIC-III CSV uses uppercase)
            df.columns = [c.lower() for c in df.columns]
            
            logger.info(f"âœ… CSV å›é€€æˆåŠŸ: åŠ è½½ {len(df)} è¡Œ")
            return df
            
        except Exception as e:
            logger.error(f"âŒ MIMIC-III CSV å›é€€å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _resolve_mimic3_chartevents_csv(self) -> Optional[Path]:
        """Find MIMIC-III CHARTEVENTS.csv.gz file
        
        Returns:
            Path to CSV file if found, None otherwise
        """
        if not self.base_path:
            return None
        
        # Try different possible file names (MIMIC-III uses uppercase)
        possible_names = [
            'CHARTEVENTS.csv.gz',
            'chartevents.csv.gz', 
            'CHARTEVENTS.csv',
            'chartevents.csv',
        ]
        
        for name in possible_names:
            csv_path = self.base_path / name
            if csv_path.exists():
                return csv_path
        
        return None

    def _read_partitioned_data_duckdb(self, directory: Path, columns: Optional[Iterable[str]], patient_ids_filter: Optional[FilterSpec] = None, itemid_filter_config: Optional[tuple] = None, table_name: Optional[str] = None, wide_table_value_columns: Optional[List[str]] = None) -> pd.DataFrame:
        """ä½¿ç”¨ DuckDB è¯»å–åˆ†åŒºæ•°æ®ï¼ˆé«˜æ€§èƒ½ç‰ˆæœ¬ï¼‰
        
        DuckDB å¯¹å•æ‚£è€…/å°æ‰¹é‡æ‚£è€…æŸ¥è¯¢ç‰¹åˆ«é«˜æ•ˆï¼Œæ¯” PyArrow å¿« 5-6 å€ã€‚
        æ”¯æŒä¸¤ç§ç›®å½•ç»“æ„ï¼š
        - æ™®é€šåˆ†åŒº: directory/*.parquet
        - åˆ†æ¡¶æ ¼å¼: directory/bucket_id=*/*.parquet (AUMC numericitems_bucket)
        
        Args:
            itemid_filter_config: å¯é€‰çš„ (åˆ—å, itemidé›†åˆ) å…ƒç»„ï¼Œç”¨äºå¤§è¡¨é¢„è¿‡æ»¤
            table_name: è¡¨åï¼Œç”¨äºç¡®å®šé¢„æ’åºé”®
            wide_table_value_columns: ğŸš€ å®½è¡¨valueåˆ—åˆ—è¡¨ï¼Œç”¨äºNULLè¿‡æ»¤ä¼˜åŒ–
                                      å¯¹äºvitalperiodicç­‰å®½è¡¨ï¼Œä¼ å…¥å¦‚['heartrate']
                                      ä¼šç”ŸæˆWHERE heartrate IS NOT NULLæ¡ä»¶
        """
        try:
            import duckdb
        except ImportError:
            # DuckDB æœªå®‰è£…ï¼Œå›é€€åˆ° PyArrow
            return self._read_partitioned_data_optimized(directory, columns, patient_ids_filter, itemid_filter_config=itemid_filter_config)
        
        # ğŸš€ æ£€æµ‹ç›®å½•ç»“æ„ï¼šåˆ†æ¡¶æ ¼å¼ vs æ™®é€šåˆ†åŒº
        bucket_subdirs = list(directory.glob("bucket_id=*"))
        if bucket_subdirs:
            # ğŸ”§ CRITICAL: ä½¿ç”¨æœ€å¤§ bucket_id + 1 ä½œä¸ºæ¡¶æ•°
            # ä¸èƒ½ç”¨ len(bucket_subdirs)ï¼Œå› ä¸ºæŸäº›æ¡¶å¯èƒ½æ˜¯ç©ºçš„ï¼ˆæ²¡æœ‰ç›®å½•ï¼‰
            # ä¾‹å¦‚ HiRID æœ‰ 100 ä¸ªæ¡¶ä½†åªæœ‰ 81 ä¸ªéç©ºæ¡¶
            max_bucket_id = max(int(d.name.split("=")[1]) for d in bucket_subdirs)
            num_buckets = max_bucket_id + 1
            
            # ğŸš€ å…³é”®ä¼˜åŒ–ï¼šå¦‚æœæœ‰ itemid è¿‡æ»¤æ¡ä»¶ï¼Œè®¡ç®—ç›®æ ‡æ¡¶ï¼Œåªæ‰«æè¿™äº›æ¡¶
            if itemid_filter_config:
                filter_col, filter_ids = itemid_filter_config
                # åªæœ‰æ•°å€¼å‹ itemid æ‰èƒ½ä½¿ç”¨ hash åˆ†æ¡¶
                numeric_ids = {int(x) for x in filter_ids if isinstance(x, (int, float)) and not isinstance(x, bool)}
                if numeric_ids:
                    # ä½¿ç”¨ DuckDB hash è®¡ç®—ç›®æ ‡æ¡¶ï¼ˆä¸åˆ†æ¡¶è½¬æ¢æ—¶ä¸€è‡´ï¼‰
                    target_buckets = self._compute_target_buckets(numeric_ids, num_buckets, duckdb)
                    # æ„å»ºåªåŒ…å«ç›®æ ‡æ¡¶çš„æ–‡ä»¶åˆ—è¡¨
                    target_files = []
                    for bucket_id in target_buckets:
                        bucket_dir = directory / f"bucket_id={bucket_id}"
                        if bucket_dir.exists():
                            target_files.extend(bucket_dir.glob("*.parquet"))
                    if target_files:
                        # ä½¿ç”¨ç²¾ç¡®çš„æ–‡ä»¶åˆ—è¡¨è€Œéå…¨æ‰«æ
                        file_list_str = ", ".join(f"'{f}'" for f in target_files)
                        glob_pattern = f"[{file_list_str}]"
                        logger.debug(f"ğŸª£ åˆ†æ¡¶ç²¾å‡†è¯»å–: {len(target_buckets)}/{num_buckets} ä¸ªæ¡¶, {len(target_files)} ä¸ªæ–‡ä»¶")
                    else:
                        # ç›®æ ‡æ¡¶ä¸å­˜åœ¨ï¼Œå¯èƒ½æ˜¯ç©ºæ•°æ®
                        logger.warning(f"âš ï¸ ç›®æ ‡æ¡¶ä¸å­˜åœ¨: bucket_id in {target_buckets}")
                        glob_pattern = str(directory / "**/*.parquet")
                else:
                    # å­—ç¬¦ä¸²å‹ IDï¼Œæ— æ³•ä½¿ç”¨ hash åˆ†æ¡¶ä¼˜åŒ–
                    glob_pattern = str(directory / "**/*.parquet")
                    logger.debug(f"ğŸª£ ä½¿ç”¨åˆ†æ¡¶æ¨¡å¼è¯»å–(å…¨æ‰«æ): {directory.name}")
            else:
                # æ²¡æœ‰ itemid è¿‡æ»¤ï¼Œå…¨æ‰«æ
                glob_pattern = str(directory / "**/*.parquet")
                logger.debug(f"ğŸª£ ä½¿ç”¨åˆ†æ¡¶æ¨¡å¼è¯»å–(æ— è¿‡æ»¤): {directory.name}")
        else:
            # æ™®é€šåˆ†åŒº: directory/*.parquet
            glob_pattern = str(directory / "*.parquet")
        
        # åˆ—é€‰æ‹©
        if columns:
            select_cols = ", ".join(list(columns))
        else:
            select_cols = "*"
        
        # WHERE å­å¥ï¼ˆæ”¯æŒå¤šä¸ªæ¡ä»¶ï¼‰
        where_conditions = []
        
        # æ‚£è€… ID è¿‡æ»¤
        if patient_ids_filter and patient_ids_filter.value:
            id_col = patient_ids_filter.column
            values = patient_ids_filter.value
            
            if isinstance(values, (list, tuple, set)):
                value_list = list(values)
            elif isinstance(values, pd.Series):
                value_list = values.tolist()
            else:
                try:
                    value_list = list(values)
                except TypeError:
                    value_list = [values]
            
            if value_list:
                if len(value_list) == 1:
                    where_conditions.append(f"{id_col} = {value_list[0]}")
                else:
                    values_str = ", ".join(map(str, value_list))
                    where_conditions.append(f"{id_col} IN ({values_str})")
        
        # ğŸš€ å¤§è¡¨ itemid é¢„è¿‡æ»¤ä¼˜åŒ–
        if itemid_filter_config:
            filter_col, filter_ids = itemid_filter_config
            # æ£€æŸ¥æ˜¯å¦ä¸ºå­—ç¬¦ä¸²ç±»å‹çš„ IDï¼ˆå¦‚ eICU nursechartingï¼‰
            is_string_ids = any(isinstance(x, str) for x in filter_ids)
            if is_string_ids:
                # å­—ç¬¦ä¸² ID éœ€è¦åŠ å¼•å·
                id_str = ", ".join(f"'{x}'" for x in sorted(filter_ids))
            else:
                # æ•°å­— ID
                id_str = ", ".join(map(str, sorted(filter_ids)))
            where_conditions.append(f"{filter_col} IN ({id_str})")
            if DEBUG_MODE:
                logger.info(f"ğŸš€ å¤§è¡¨ä¼˜åŒ–: {filter_col} è¿‡æ»¤ {len(filter_ids)} ä¸ª ID")
        
        # ğŸš€ å®½è¡¨NULLè¿‡æ»¤ä¼˜åŒ–ï¼šè·³è¿‡valueåˆ—ä¸ºNULLçš„è¡Œ
        # è¿™å¯¹äºeICU vitalperiodicç­‰å®½è¡¨éå¸¸é‡è¦
        # ä¾‹å¦‚ï¼šåŠ è½½hræ¦‚å¿µæ—¶ï¼Œheartrateä¸ºNULLçš„è¡Œæ²¡æœ‰æ„ä¹‰ï¼Œå¯ä»¥è·³è¿‡
        # è¿™å°†145Mè¡Œâ†’12Mè¡Œï¼Œå¤§å¹…å‡å°‘æ•°æ®ä¼ è¾“å’Œpandaså¤„ç†å¼€é”€
        if wide_table_value_columns:
            for val_col in wide_table_value_columns:
                where_conditions.append(f"{val_col} IS NOT NULL")
            if DEBUG_MODE:
                logger.info(f"ğŸš€ å®½è¡¨NULLè¿‡æ»¤: {wide_table_value_columns} IS NOT NULL")
        
        # æ„å»º WHERE å­å¥
        where_clause = ""
        if where_conditions:
            where_clause = "WHERE " + " AND ".join(where_conditions)
        
        # ğŸ”§ DuckDB é¢„æ’åºä¼˜åŒ–ï¼šé’ˆå¯¹è¶…å¤§è¡¨åœ¨æŸ¥è¯¢æ—¶ç›´æ¥æ’åº
        # pandas sort_values åœ¨ 1.46 äº¿è¡Œä¸Šéœ€è¦ 25 ç§’ï¼Œè€Œ DuckDB ORDER BY åªéœ€ 1.9 ç§’
        # å®½è¡¨ (vitalperiodic, vitalaperiodic) å¿…é¡»é¢„æ’åºï¼Œå¦åˆ™åç»­ sort_values éå¸¸æ…¢
        order_by_clause = ""
        PRESORT_TABLES = {'vitalperiodic', 'vitalaperiodic'}  # éœ€è¦é¢„æ’åºçš„å®½è¡¨
        if table_name and table_name.lower() in PRESORT_TABLES:
            # è·å–è¡¨é…ç½®ä»¥ç¡®å®šæ’åºé”®
            try:
                table_cfg = self.config.get_table(table_name)
                sort_keys = []
                
                # è·å– ID åˆ—
                if table_cfg.defaults.id_var:
                    sort_keys.append(table_cfg.defaults.id_var)
                else:
                    icustay_cfg = self.config.id_configs.get('icustay')
                    if icustay_cfg:
                        sort_keys.append(icustay_cfg.id)
                
                # è·å–æ—¶é—´åˆ—
                if table_cfg.defaults.index_var:
                    sort_keys.append(table_cfg.defaults.index_var)
                
                if sort_keys:
                    order_by_clause = f" ORDER BY {', '.join(sort_keys)}"
                    logger.debug(f"ğŸš€ å®½è¡¨é¢„æ’åº: {table_name} ORDER BY {sort_keys}")
            except Exception as e:
                logger.debug(f"æ— æ³•è·å–è¡¨é…ç½®è¿›è¡Œé¢„æ’åº: {e}")
        
        # ğŸ”§ CRITICAL FIX: ä½¿ç”¨ union_by_name=true å¤„ç†ä¸åŒåˆ†åŒºçš„ schema å·®å¼‚
        # HiRID observations çš„ä¸åŒåˆ†åŒºæœ‰ä¸åŒçš„åˆ—ç±»å‹ï¼ˆå¦‚ stringvalueï¼‰
        # æ³¨æ„ï¼šglob_pattern å¯èƒ½æ˜¯å•å¼•å·åŒ…è£¹çš„è·¯å¾„ï¼Œä¹Ÿå¯èƒ½æ˜¯åˆ—è¡¨è¯­æ³• [...]
        if glob_pattern.startswith("["):
            # åˆ—è¡¨è¯­æ³•ï¼šå¤šä¸ªæ–‡ä»¶
            query = f"SELECT {select_cols} FROM read_parquet({glob_pattern}, union_by_name=true) {where_clause}{order_by_clause}"
        else:
            # è·¯å¾„/glob è¯­æ³•
            query = f"SELECT {select_cols} FROM read_parquet('{glob_pattern}', union_by_name=true) {where_clause}{order_by_clause}"
        
        try:
            con = duckdb.connect()
            # ğŸ”§ CRITICAL FIX: è®¾ç½® DuckDB æ—¶åŒºä¸º UTC
            # DuckDB é»˜è®¤å°† UTC æ—¶é—´è½¬æ¢ä¸ºæœ¬åœ°æ—¶åŒºï¼Œè¿™ä¼šå¯¼è‡´æ—¶é—´åç§»
            # ä¾‹å¦‚ï¼šUTC 15:37 ä¼šè¢«è½¬æ¢æˆ Asia/Shanghai 23:37 (+8 å°æ—¶)
            # è®¾ç½®æ—¶åŒºä¸º UTC å¯ä»¥ä¿æŒåŸå§‹ UTC æ—¶é—´ä¸å˜
            con.execute("SET timezone='UTC'")
            # ğŸ”§ ç¦ç”¨DuckDBè¿›åº¦æ¡ï¼Œé¿å…ç»ˆç«¯è¾“å‡ºå¼€é”€
            con.execute("SET enable_progress_bar = false")
            con.execute("SET enable_progress_bar_print = false")
            df = con.execute(query).fetchdf()
            con.close()
            return df
        except Exception as e:
            logger.warning(f"DuckDB è¯»å–å¤±è´¥ï¼Œå›é€€åˆ° PyArrow: {e}")
            return self._read_partitioned_data_optimized(directory, columns, patient_ids_filter, itemid_filter_config=itemid_filter_config)
    
    def _read_partitioned_data_optimized(self, directory: Path, columns: Optional[Iterable[str]], patient_ids_filter: Optional[FilterSpec] = None, itemid_filter_config: Optional[tuple] = None) -> pd.DataFrame:
        """è¯»å–åˆ†åŒºæ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        Args:
            itemid_filter_config: å¯é€‰çš„ (åˆ—å, itemidé›†åˆ) å…ƒç»„ï¼Œç”¨äºå¤§è¡¨é¢„è¿‡æ»¤
        """
        try:
            import pyarrow.dataset as ds
            
            # ğŸš€ ä½¿ç”¨PyArrow Dataset - æœ€å¿«çš„æ–¹å¼
            dataset = ds.dataset(
                directory,
                format='parquet',
                partitioning=None,
                exclude_invalid_files=True
            )
            
            # æ„å»ºè¿‡æ»¤è¡¨è¾¾å¼ï¼ˆæ”¯æŒå¤šä¸ªæ¡ä»¶ï¼‰
            filter_exprs = []
            
            # æ‚£è€… ID è¿‡æ»¤
            if patient_ids_filter:
                id_col = patient_ids_filter.column
                values = patient_ids_filter.value
                if isinstance(values, (list, tuple, set)):
                    value_list = list(values)
                elif isinstance(values, pd.Series):
                    value_list = values.tolist()
                else:
                    try:
                        value_list = list(values)
                    except TypeError:
                        value_list = [values]

                if not value_list:
                    wanted_cols = list(columns) if columns else dataset.schema.names
                    return pd.DataFrame(columns=wanted_cols)

                try:
                    filter_exprs.append(ds.field(id_col).isin(value_list))
                except Exception:
                    pass
            
            # ğŸš€ å¤§è¡¨ä¼˜åŒ–ï¼šitemid é¢„è¿‡æ»¤ (AUMC numericitems, MIIV chartevents/labevents)
            if itemid_filter_config:
                try:
                    filter_col, filter_ids = itemid_filter_config
                    filter_exprs.append(ds.field(filter_col).isin(list(filter_ids)))
                    if DEBUG_MODE:
                        logger.info(f"ğŸš€ å¤§è¡¨ä¼˜åŒ– (PyArrow): {filter_col} è¿‡æ»¤ {len(filter_ids)} ä¸ª ID")
                except Exception:
                    pass
            
            # åˆå¹¶è¿‡æ»¤æ¡ä»¶
            filter_expr = None
            if filter_exprs:
                filter_expr = filter_exprs[0]
                for expr in filter_exprs[1:]:
                    filter_expr = filter_expr & expr

            # æ‰¹é‡è¯»å–ï¼Œå¯ç”¨å¤šçº¿ç¨‹ï¼ˆä¼˜åŒ–å¤§è§„æ¨¡æå–ï¼‰
            # ğŸš€ ä¼˜åŒ–ï¼šä¸º90000+æ‚£è€…æå–å¢åŠ çº¿ç¨‹æ± 
            thread_count = 32  # æœ€ä¼˜é…ç½®ï¼š32çº¿ç¨‹
            
            if columns:
                table = dataset.to_table(
                    columns=list(columns), 
                    filter=filter_expr,
                    use_threads=thread_count  # æ˜ç¡®çº¿ç¨‹æ•°
                )
            else:
                table = dataset.to_table(
                    filter=filter_expr,
                    use_threads=thread_count
                )

            # è½¬æ¢ä¸º pandasï¼Œä½¿ç”¨ zero-copy ä¼˜åŒ–
            return table.to_pandas(split_blocks=True, self_destruct=True)
            
        except Exception:
            # å›é€€åˆ°ç®€å•æ–¹å¼
            parquet_files = sorted(directory.glob("*.parquet"))
            if not parquet_files:
                parquet_files = sorted(directory.glob("*.pq"))
            
            if not parquet_files:
                return pd.DataFrame(columns=list(columns) if columns else [])
            
            # å‡†å¤‡è¿‡æ»¤æ¡ä»¶
            filter_ids = None
            id_column = None
            if patient_ids_filter:
                id_column = patient_ids_filter.column
                if isinstance(patient_ids_filter.value, (list, tuple, set)):
                    filter_ids = set(patient_ids_filter.value)
                else:
                    filter_ids = {patient_ids_filter.value}
            
            # å¿«é€Ÿè¯»å–+è¿‡æ»¤
            chunks = []
            for file_path in parquet_files:
                if columns:
                    df_chunk = pd.read_parquet(file_path, columns=list(columns))
                else:
                    df_chunk = pd.read_parquet(file_path)
                
                # ç«‹å³åº”ç”¨è¿‡æ»¤ï¼ˆå‡å°‘å†…å­˜å ç”¨ï¼‰
                if filter_ids and id_column and id_column in df_chunk.columns:
                    df_chunk = df_chunk[df_chunk[id_column].isin(filter_ids)]
                
                # åªä¿ç•™æœ‰æ•°æ®çš„chunk
                if len(df_chunk) > 0:
                    chunks.append(df_chunk)
            
            # åˆå¹¶æ‰€æœ‰chunks
            if chunks:
                return pd.concat(chunks, ignore_index=True)
            else:
                # è¿”å›ç©ºDataFrameï¼Œä¿æŒåˆ—ç»“æ„
                if columns:
                    return pd.DataFrame(columns=list(columns))
                else:
                    return pd.DataFrame()
            
        except Exception as e:
            # æœ€ç»ˆå›é€€åˆ°åŸå§‹å®ç°
            logger.warning(f"ä¼˜åŒ–è¯»å–å¤±è´¥: {e}ï¼Œå›é€€åˆ°fallbackæ–¹æ³•")
            return self._read_partitioned_data_fallback(directory, columns, patient_ids_filter)
    
    def _read_partitioned_data_fallback(self, directory: Path, columns: Optional[Iterable[str]], patient_ids_filter: Optional[FilterSpec] = None) -> pd.DataFrame:
        """Read partitioned data from a directory, respecting format priority."""
        
        # ğŸ” è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºåˆ†åŒºåŠ è½½è¯·æ±‚çš„åˆ—
        if DEBUG_MODE and columns:
            logger.debug(f"åˆ†åŒºè¡¨ {directory.name} è¯·æ±‚çš„åˆ—: {list(columns)}")
        
        # åªæ”¯æŒ Parquet æ ¼å¼
        files = sorted(directory.glob("*.parquet")) + sorted(directory.glob("*.pq"))
        if not files:
            # æ²¡æœ‰æ‰¾åˆ° parquet æ–‡ä»¶
            return pd.DataFrame()
        
        num_files = len(files)
        
        # å‡†å¤‡æ‚£è€…IDè¿‡æ»¤å™¨ (æ”¯æŒå¤šç§æ•°æ®åº“çš„IDåˆ—)
        filter_tuple = None
        if patient_ids_filter and patient_ids_filter.column in ['subject_id', 'hadm_id', 'icustay_id', 'stay_id', 'admissionid', 'patientid']:
            target_ids = set(patient_ids_filter.value) if not isinstance(patient_ids_filter.value, str) else {patient_ids_filter.value}
            filter_tuple = (patient_ids_filter.column, target_ids)
            if DEBUG_MODE: print(f"   ğŸ“ åŠ è½½ {directory.name} ({num_files} ä¸ª parquet åˆ†åŒº) - è¿‡æ»¤ {len(target_ids)} ä¸ªæ‚£è€…...")
        else:
            if DEBUG_MODE: print(f"   ğŸ“ åŠ è½½ {directory.name} ({num_files} ä¸ª parquet åˆ†åŒº)...")
        
        # ä¿®å¤ï¼šä¼ é€’å…·ä½“çš„parquetæ–‡ä»¶åˆ—è¡¨ï¼Œè€Œä¸æ˜¯ç›®å½•ï¼Œé¿å…æ··åˆæ ¼å¼é—®é¢˜
        dataset_df = self._read_parquet_dataset(
            directory,
            files,  # ä¼ é€’å…·ä½“çš„parquetæ–‡ä»¶åˆ—è¡¨
            columns=list(columns) if columns else None,
            filter_spec=patient_ids_filter,
        )
        if dataset_df is not None:
            return dataset_df
        # Fallback: iterate individual parquet files
        dfs = []
        arrow_filters = None
        if patient_ids_filter:
            arrow_filters = self._build_dataset_filter(patient_ids_filter)
        for f in files:
            if arrow_filters is not None or columns is not None:
                try:
                    import pyarrow.parquet as pq  # type: ignore
                    table = pq.read_table(
                        f,
                        columns=list(columns) if columns else None,
                    )
                    if arrow_filters is not None:
                        table = table.filter(arrow_filters)
                    df = table.to_pandas()
                    dfs.append(df)
                    continue
                except Exception:
                    pass  # Fallback to pandas.read_parquet below
            df = pd.read_parquet(f, columns=list(columns) if columns else None)
            if filter_tuple:
                col_name, target_ids = filter_tuple
                if col_name in df.columns:
                    df = df[df[col_name].isin(target_ids)]
            dfs.append(df)
        
        # åˆå¹¶æ‰€æœ‰åˆ†åŒº
        if dfs:
            return pd.concat(dfs, ignore_index=True)
        
        # æ²¡æœ‰æ‰¾åˆ°ä»»ä½•parquetæ–‡ä»¶
        return pd.DataFrame()

    def _read_parquet_dataset(
        self,
        directory: Path,
        files: Optional[List[Path]] = None,
        columns: Optional[Sequence[str]] = None,
        filter_spec: Optional[FilterSpec] = None,
    ) -> Optional[pd.DataFrame]:
        """Attempt to read a parquet directory via PyArrow Dataset for fast filtering."""
        try:
            import pyarrow.dataset as ds  # type: ignore
        except ImportError:
            return None

        filter_expr = None
        if filter_spec is not None:
            filter_expr = self._build_dataset_filter(filter_spec)
        try:
            # ä¿®å¤ï¼šä½¿ç”¨ä¼ å…¥çš„æ–‡ä»¶åˆ—è¡¨åˆ›å»ºdatasetï¼Œé¿å…æ··åˆæ ¼å¼é—®é¢˜
            if files is not None:
                # ä½¿ç”¨å…·ä½“çš„parquetæ–‡ä»¶åˆ—è¡¨
                dataset = ds.dataset(files, format="parquet")
            else:
                # å›é€€åˆ°åŸå§‹é€»è¾‘ï¼ˆä»…åŒ…å«parquetæ–‡ä»¶çš„ç›®å½•ï¼‰
                try:
                    dataset = ds.dataset(directory, format="parquet", partitioning="hive")
                except (ValueError, TypeError):
                    dataset = ds.dataset(directory, format="parquet")

            table = dataset.to_table(columns=columns, filter=filter_expr)
            return table.to_pandas()
        except (OSError, ValueError, TypeError) as exc:
            if DEBUG_MODE:
                logger.debug("PyArrow dataset read failed for %s: %s", directory, exc)
            return None

    @staticmethod
    def _build_dataset_filter(filter_spec: FilterSpec):
        """Convert FilterSpec to a PyArrow Dataset expression."""
        try:
            import pyarrow.dataset as ds  # type: ignore
        except ImportError:
            return None

        field = ds.field(filter_spec.column)
        if filter_spec.op == FilterOp.EQ:
            return field == filter_spec.value
        if filter_spec.op == FilterOp.IN:
            values = _ensure_sequence(filter_spec.value)
            return field.isin(values)
        if filter_spec.op == FilterOp.BETWEEN:
            lower, upper = filter_spec.value
            return (field >= lower) & (field <= upper)
        return None

    def _read_dataset(
        self,
        table_name: str,
        dataset_cfg: DatasetOptions,
        columns: Optional[Iterable[str]],
        patient_ids_filter: Optional[FilterSpec],
    ) -> pd.DataFrame:
        """Read a table via explicit PyArrow Dataset configuration."""
        try:
            import pyarrow.dataset as ds  # type: ignore
        except ImportError as exc:  # pragma: no cover - optional dependency
            raise ImportError(
                "PyArrow is required for dataset-backed tables. "
                "Install pyarrow or remove the dataset configuration."
            ) from exc

        root = dataset_cfg.path or table_name
        root_path = Path(root)
        if not root_path.is_absolute():
            if self.base_path is None:
                raise ValueError(
                    f"Dataset path '{root}' for table '{table_name}' is relative, "
                    "but data source has no base_path."
                )
            root_path = self.base_path / root_path

        partitioning = dataset_cfg.partitioning or "hive"
        format_name = dataset_cfg.format or "parquet"
        options = dataset_cfg.options or {}

        try:
            dataset = ds.dataset(
                root_path,
                format=format_name,
                partitioning=partitioning,
                **options,
            )
        except (OSError, ValueError, TypeError) as exc:
            raise RuntimeError(
                f"Failed to initialise dataset for table '{table_name}' at {root_path}: {exc}"
            ) from exc

        filter_expr = self._build_dataset_filter(patient_ids_filter) if patient_ids_filter else None
        logger.info("ğŸ“ Using PyArrow dataset for %s (%s)", table_name, root_path)

        requested_columns = list(columns) if columns is not None else None
        effective_columns: Optional[List[str]] = None
        missing_columns: List[str] = []
        if requested_columns:
            available = set(dataset.schema.names)
            missing_columns = [col for col in requested_columns if col not in available]
            effective_columns = [col for col in requested_columns if col in available]
            if not effective_columns:
                effective_columns = None

        table = dataset.to_table(columns=effective_columns, filter=filter_expr)
        frame = table.to_pandas()

        if requested_columns:
            frame = frame.reindex(columns=requested_columns)
        if missing_columns:
            logger.warning(
                "Dataset %s missing columns %s; filled with NA values", table_name, ", ".join(missing_columns)
            )
        return frame
    

def load_table(
    data_source: ICUDataSource,
    table_name: str,
    *,
    columns: Optional[Iterable[str]] = None,
    filters: Optional[Iterable[FilterSpec]] = None,
) -> ICUTable:
    """Functional faÃ§ade delegating to :meth:`ICUDataSource.load_table`."""

    return data_source.load_table(table_name, columns=columns, filters=filters)

def _ensure_sequence(value: Any) -> List[Any]:
    """Normalise scalars/iterables for filter construction."""
    if value is None:
        return []
    if isinstance(value, Sequence) and not isinstance(value, (str, bytes)):
        return list(value)
    try:
        # ğŸ”§ FIX: å­—ç¬¦ä¸²ä¸åº”è¯¥è¢«è½¬æ¢ä¸ºå­—ç¬¦åˆ—è¡¨
        if isinstance(value, (str, bytes)):
            return [value]
        return list(value)
    except TypeError:
        return [value]

def _coerce_datetime(series: pd.Series) -> pd.Series:
    """Coerce a Series to datetime type, handling various edge cases.
    
    âš¡ æ€§èƒ½ä¼˜åŒ–: å‡å°‘é‡å¤çš„ç±»å‹æ£€æŸ¥å’Œè½¬æ¢
    """
    # âš¡ å¿«é€Ÿè·¯å¾„1: å·²ç»æ˜¯datetimeä¸”æ— æ—¶åŒº
    if pd.api.types.is_datetime64_any_dtype(series):
        if hasattr(series.dt, 'tz') and series.dt.tz is not None:
            return series.dt.tz_localize(None)
        return series
    
    # âš¡ å¿«é€Ÿè·¯å¾„2: æ•°å€¼å‹ä¸è½¬æ¢
    if pd.api.types.is_numeric_dtype(series):
        return series
    
    # âš¡ ä¼˜åŒ–: ä¸€æ¬¡æ€§æ£€æŸ¥å’Œreset index
    has_dup_idx = series.index.duplicated().any()
    if has_dup_idx:
        series = series.reset_index(drop=True)
    
    # âš¡ ä¼˜åŒ–: ç»Ÿä¸€ä½¿ç”¨coerceæ¨¡å¼ï¼Œé¿å…try-exceptå¼€é”€
    try:
        converted = pd.to_datetime(series, errors="coerce", utc=True)
        # åªåœ¨è½¬æ¢æˆåŠŸæ—¶ç§»é™¤æ—¶åŒº
        if converted is not None and hasattr(converted, 'dt'):
            return converted.dt.tz_localize(None)
        return series
    except Exception:
        # æç«¯æƒ…å†µï¼šè¿”å›åŸå€¼
        return series


def load_bucketed_table_aggregated(
    data_source: "ICUDataSource",
    table_name: str,
    value_column: str,
    itemids: List[int],
    interval_minutes: float = 60.0,
    patient_ids: Optional[List] = None,
    agg_func: str = 'median',  # 'median', 'mean', 'max', 'min', 'first', 'sum'
    id_col: Optional[str] = None,
    time_col: Optional[str] = None,
) -> pd.DataFrame:
    """
    ğŸš€ é«˜æ€§èƒ½åˆ†æ¡¶è¡¨åŠ è½½ï¼šåœ¨DuckDBä¸­å®Œæˆèšåˆé™é‡‡æ ·
    
    é’ˆå¯¹AUMC numericitemsã€HiRID observationsç­‰åˆ†æ¡¶è¡¨ä¼˜åŒ–ã€‚
    ç›´æ¥åœ¨DuckDBä¸­å®Œæˆå°æ—¶èšåˆï¼Œé¿å…åŠ è½½3700ä¸‡è¡Œåˆ°Pythonå†é™é‡‡æ ·ã€‚
    
    å…³é”®ä¼˜åŒ–ï¼š
    - åªæ‰«æç›®æ ‡æ¡¶ï¼ˆè€Œéå…¨éƒ¨100ä¸ªæ¡¶ï¼‰
    - åœ¨DuckDBä¸­å®Œæˆæ—¶é—´èšåˆï¼ˆ37M â†’ 2.5Mè¡Œï¼‰
    - æ˜¾è‘—é™ä½å†…å­˜ä½¿ç”¨ï¼ˆ<500MB vs 5GB+ï¼‰
    
    Args:
        data_source: ICUæ•°æ®æº
        table_name: åŸå§‹è¡¨åï¼ˆå¦‚'numericitems'ï¼‰ï¼Œä¼šè‡ªåŠ¨æŸ¥æ‰¾å¯¹åº”çš„åˆ†æ¡¶ç›®å½•
        value_column: å€¼åˆ—åï¼ˆå¦‚'value'ï¼‰
        itemids: è¦æå–çš„itemidåˆ—è¡¨
        interval_minutes: æ—¶é—´èšåˆé—´éš”ï¼ˆåˆ†é’Ÿï¼‰ï¼Œé»˜è®¤60åˆ†é’Ÿ
        patient_ids: å¯é€‰çš„æ‚£è€…IDè¿‡æ»¤
        agg_func: èšåˆå‡½æ•°ï¼Œé»˜è®¤'median'ï¼ˆä¸R ricuä¸€è‡´ï¼‰
        id_col: IDåˆ—åï¼ˆå¯é€‰ï¼Œé»˜è®¤æ ¹æ®æ•°æ®åº“æ¨æ–­ï¼‰
        time_col: æ—¶é—´åˆ—åï¼ˆå¯é€‰ï¼Œé»˜è®¤æ ¹æ®æ•°æ®åº“æ¨æ–­ï¼‰
        
    Returns:
        èšåˆåçš„DataFrame
    """
    import duckdb
    
    # ç¡®å®šæ•°æ®åº“ç±»å‹
    db_name = data_source.config.name if hasattr(data_source.config, 'name') else 'unknown'
    
    # ğŸ”§ ç›´æ¥æŸ¥æ‰¾åˆ†æ¡¶ç›®å½•ï¼ˆä¸ä¾èµ–_resolve_loader_from_diskï¼‰
    base_path = data_source.base_path
    bucket_table_name = f"{table_name}_bucket"
    
    possible_bucket_dirs = [
        base_path / bucket_table_name,
        base_path / "icu" / bucket_table_name,
        base_path / "hosp" / bucket_table_name,
    ]
    
    bucket_dir = None
    for dir_path in possible_bucket_dirs:
        if dir_path.is_dir():
            bucket_subdirs = list(dir_path.glob("bucket_id=*"))
            if bucket_subdirs:
                bucket_dir = dir_path
                break
    
    if bucket_dir is None:
        raise ValueError(f"Cannot find bucketed directory for {table_name} (tried: {[str(p) for p in possible_bucket_dirs]})")
    
    # ç¡®å®šIDåˆ—å’Œæ—¶é—´åˆ—
    if id_col is None:
        if db_name == 'aumc':
            id_col = 'admissionid'
        elif db_name == 'hirid':
            id_col = 'patientid'
        else:
            id_col = 'stay_id'
    
    if time_col is None:
        if db_name == 'aumc':
            time_col = 'measuredat'  # AUMCä½¿ç”¨measuredatï¼ˆæ¯«ç§’æ—¶é—´æˆ³ï¼‰
        elif db_name == 'hirid':
            time_col = 'datetime'
        else:
            time_col = 'charttime'
    
    # ç¡®å®šitemidåˆ—å
    if db_name == 'aumc':
        itemid_col = 'itemid'
    elif db_name == 'hirid':
        itemid_col = 'variableid'
    else:
        itemid_col = 'itemid'
    
    # è®¡ç®—ç›®æ ‡æ¡¶
    conn = duckdb.connect()
    conn.execute("SET timezone='UTC'")
    # ğŸ”§ ç¦ç”¨DuckDBè¿›åº¦æ¡ï¼Œé¿å…16ç§’çš„ç»ˆç«¯è¾“å‡ºå¼€é”€
    conn.execute("SET enable_progress_bar = false")
    conn.execute("SET enable_progress_bar_print = false")
    
    # è·å–æ¡¶æ•°
    bucket_subdirs = list(bucket_dir.glob("bucket_id=*"))
    if not bucket_subdirs:
        conn.close()
        return pd.DataFrame()
    
    num_buckets = max(int(d.name.split('=')[1]) for d in bucket_subdirs) + 1
    
    # è®¡ç®—ç›®æ ‡æ¡¶ID
    itemid_list = list(itemids)
    conn.execute("CREATE TEMP TABLE items AS SELECT UNNEST(?) as itemid", [itemid_list])
    result = conn.execute(f"SELECT DISTINCT hash(itemid) % {num_buckets} FROM items").fetchall()
    target_buckets = [row[0] for row in result]
    
    # æ„å»ºæ–‡ä»¶åˆ—è¡¨
    target_files = []
    for bucket_id in target_buckets:
        bucket_subdir = bucket_dir / f"bucket_id={bucket_id}"
        if bucket_subdir.exists():
            target_files.extend(bucket_subdir.glob("*.parquet"))
    
    if not target_files:
        conn.close()
        return pd.DataFrame()
    
    file_list_str = ", ".join(f"'{f}'" for f in target_files)
    glob_pattern = f"[{file_list_str}]"
    
    # æ„å»ºWHEREæ¡ä»¶
    where_conditions = []
    
    # itemidè¿‡æ»¤
    ids_str = ", ".join(str(x) for x in itemids)
    where_conditions.append(f"{itemid_col} IN ({ids_str})")
    
    # æ‚£è€…è¿‡æ»¤
    if patient_ids:
        patient_str = ", ".join(str(x) for x in patient_ids)
        where_conditions.append(f"{id_col} IN ({patient_str})")
    
    where_clause = "WHERE " + " AND ".join(where_conditions)
    
    # èšåˆå‡½æ•°æ˜ å°„
    agg_map = {
        'median': 'MEDIAN',
        'mean': 'AVG',
        'max': 'MAX',
        'min': 'MIN',
        'first': 'FIRST',
        'sum': 'SUM',
    }
    duckdb_agg = agg_map.get(agg_func, 'MEDIAN')
    
    # æ„å»ºæ—¶é—´èšåˆè¡¨è¾¾å¼
    # AUMC: measuredatæ˜¯æ¯«ç§’æ—¶é—´æˆ³ï¼Œéœ€è¦è½¬æ¢åå†èšåˆ
    if db_name == 'aumc':
        # AUMC measuredatæ˜¯Unixæ¯«ç§’æ—¶é—´æˆ³ï¼Œè½¬æ¢ä¸ºåˆ†é’Ÿåå†å–æ•´
        time_round_expr = f"FLOOR(({time_col} / 60000.0) / {interval_minutes}) * {interval_minutes}"
        # è¾“å‡ºæ—¶é—´åˆ—ä¸ºåˆ†é’Ÿåç§»é‡ï¼ˆç›¸å¯¹äºadmittedatï¼‰
        output_time_expr = f"{time_round_expr} as measuredat_minutes"
        # æ ‡å‡†æŸ¥è¯¢
        query = f"""
        SELECT 
            {id_col},
            {output_time_expr},
            {itemid_col},
            {duckdb_agg}({value_column}) as {value_column}
        FROM read_parquet({glob_pattern}, union_by_name=true)
        {where_clause}
        GROUP BY {id_col}, {time_round_expr}, {itemid_col}
        ORDER BY {id_col}, 2, {itemid_col}
        """
    elif db_name == 'hirid':
        # ğŸš€ HiRID ä¼˜åŒ–: åœ¨ DuckDB ä¸­ç›´æ¥å®Œæˆæ—¶é—´è½¬æ¢ï¼ˆdatetime â†’ ç›¸å¯¹å…¥é™¢å°æ—¶æ•°ï¼‰
        # è¿™æ ·é¿å…äº† Python ä¸­çš„ merge + æ—¶é—´è®¡ç®—å¼€é”€ï¼ˆä» 20s ä¼˜åŒ–åˆ° 0.6sï¼‰
        # ğŸ”§ FIX: HiRID çš„ general_table å¯èƒ½æ˜¯ CSV æˆ– Parquet æ ¼å¼
        general_path = data_source.base_path / 'general_table.parquet'
        general_read_func = 'read_parquet'
        if not general_path.exists():
            general_csv = data_source.base_path / 'general_table.csv'
            if general_csv.exists():
                general_path = general_csv
                general_read_func = 'read_csv'
        
        # HiRID: ä½¿ç”¨ general è¡¨çš„ admissiontime è®¡ç®—ç›¸å¯¹å°æ—¶æ•°
        time_round_expr = f"FLOOR(EPOCH(o.{time_col} - CAST(a.admissiontime AS TIMESTAMP)) / 3600.0 / {interval_minutes / 60}) * {interval_minutes / 60}"
        output_time_expr = f"{time_round_expr} as charttime"
        
        # ğŸ”§ ä¿®å¤: ä¸º HiRID çš„ JOIN æŸ¥è¯¢æ·»åŠ è¡¨åˆ«åå‰ç¼€
        # å› ä¸ºä½¿ç”¨äº† JOINï¼Œåˆ—åéœ€è¦æ˜ç¡®æ¥è‡ªå“ªä¸ªè¡¨
        hirid_where_clause = where_clause.replace(f'{itemid_col}', f'o.{itemid_col}')
        hirid_where_clause = hirid_where_clause.replace(f'{id_col} IN', f'o.{id_col} IN')
        
        query = f"""
        WITH adm AS (
            SELECT patientid, CAST(admissiontime AS TIMESTAMP) as admissiontime 
            FROM {general_read_func}('{general_path}')
        )
        SELECT 
            o.{id_col},
            {output_time_expr},
            o.{itemid_col},
            {duckdb_agg}(o.{value_column}) as {value_column}
        FROM read_parquet({glob_pattern}, union_by_name=true) o
        JOIN adm a ON o.{id_col} = a.patientid
        {hirid_where_clause}
        GROUP BY o.{id_col}, {time_round_expr}, o.{itemid_col}
        ORDER BY o.{id_col}, 2, o.{itemid_col}
        """
    else:
        time_round_expr = f"FLOOR({time_col} / {interval_minutes}) * {interval_minutes}"
        output_time_expr = f"{time_round_expr} as charttime"
        # æ ‡å‡†æŸ¥è¯¢
        query = f"""
        SELECT 
            {id_col},
            {output_time_expr},
            {itemid_col},
            {duckdb_agg}({value_column}) as {value_column}
        FROM read_parquet({glob_pattern}, union_by_name=true)
        {where_clause}
        GROUP BY {id_col}, {time_round_expr}, {itemid_col}
        ORDER BY {id_col}, 2, {itemid_col}
        """
    
    try:
        df = conn.execute(query).fetchdf()
        logger.info(f"ğŸš€ åˆ†æ¡¶è¡¨DuckDBèšåˆå®Œæˆ: {table_name} itemids={len(itemids)} -> {len(df):,} è¡Œ")
        return df
    except Exception as e:
        logger.warning(f"DuckDBèšåˆå¤±è´¥: {e}")
        raise
    finally:
        conn.close()


def load_wide_table_aggregated(
    data_source: "ICUDataSource",
    table_name: str,
    value_columns: List[str],
    interval_hours: float = 1.0,
    patient_ids: Optional[List] = None,
    agg_func: str = 'median',  # 'median' (default, matches R ricu), 'first', 'mean', 'max', 'min'
) -> pd.DataFrame:
    """
    ğŸš€ é«˜æ€§èƒ½å®½è¡¨æ‰¹é‡åŠ è½½ï¼šåœ¨DuckDBä¸­å®Œæˆèšåˆå’Œå»é‡
    
    é’ˆå¯¹eICU vitalperiodicç­‰å®½è¡¨ä¼˜åŒ–ï¼Œä¸€æ¬¡åŠ è½½å¤šä¸ªæ¦‚å¿µåˆ—ï¼Œ
    ç›´æ¥åœ¨DuckDBä¸­å®Œæˆå°æ—¶èšåˆï¼Œé¿å…pandasåå¤„ç†å¼€é”€ã€‚
    
    Args:
        data_source: ICUæ•°æ®æº
        table_name: è¡¨åï¼ˆå¦‚'vitalperiodic'ï¼‰
        value_columns: éœ€è¦åŠ è½½çš„å€¼åˆ—åˆ—è¡¨ï¼ˆå¦‚['heartrate', 'respiration']ï¼‰
        interval_hours: æ—¶é—´èšåˆé—´éš”ï¼ˆå°æ—¶ï¼‰
        patient_ids: å¯é€‰çš„æ‚£è€…IDè¿‡æ»¤
        agg_func: èšåˆå‡½æ•°ï¼ˆ'first', 'mean', 'max', 'min'ï¼‰
        
    Returns:
        èšåˆåçš„DataFrameï¼ŒåŒ…å«idåˆ—ã€æ—¶é—´åˆ—å’Œæ‰€æœ‰å€¼åˆ—
        
    Example:
        >>> df = load_wide_table_aggregated(
        ...     data_source, 'vitalperiodic', 
        ...     ['heartrate', 'respiration', 'sao2'], 
        ...     interval_hours=1.0
        ... )
        >>> # è¿”å›: patientunitstayid | charttime | heartrate | respiration | sao2
    """
    import duckdb
    
    # è·å–è¡¨é…ç½®
    table_cfg = data_source.config.get_table(table_name)
    
    # ç¡®å®šIDåˆ—å’Œæ—¶é—´åˆ—
    id_col = table_cfg.defaults.id_var
    if not id_col:
        icustay_cfg = data_source.config.id_configs.get('icustay')
        id_col = icustay_cfg.id if icustay_cfg else 'patientunitstayid'
    
    time_col = table_cfg.defaults.index_var or 'observationoffset'
    
    # ç¡®å®šæ•°æ®ç›®å½•
    table_path = data_source._resolve_loader_from_disk(table_name)
    if table_path is None:
        raise ValueError(f"Cannot find data for table {table_name}")
    
    # table_path è¿”å› Path å¯¹è±¡
    directory = table_path if isinstance(table_path, Path) else Path(table_path)
    
    # æ„å»ºglob pattern
    if directory.is_dir():
        glob_pattern = str(directory / "*.parquet")
    else:
        glob_pattern = str(directory)
    
    # æ„å»ºDuckDBèšåˆå‡½æ•°æ˜ å°„ (medianä¸ºR ricué»˜è®¤)
    agg_map = {
        'median': 'MEDIAN',  # R ricu default
        'first': 'FIRST',
        'mean': 'AVG',
        'max': 'MAX', 
        'min': 'MIN',
    }
    duckdb_agg = agg_map.get(agg_func, 'MEDIAN')
    
    # æ„å»ºCTEï¼šæ¯ä¸ªå€¼åˆ—å•ç‹¬èšåˆï¼ˆå¤„ç†NULLï¼‰
    cte_parts = []
    for i, val_col in enumerate(value_columns):
        cte_name = f"agg_{i}"
        cte_sql = f"""
        {cte_name} AS (
            SELECT 
                {id_col},
                FLOOR({time_col} / {interval_hours * 60.0}) as charttime,
                {duckdb_agg}({val_col}) as {val_col}
            FROM raw_data
            WHERE {val_col} IS NOT NULL
            GROUP BY {id_col}, FLOOR({time_col} / {interval_hours * 60.0})
        )"""
        cte_parts.append(cte_sql)
    
    # æ„å»ºWHEREæ¡ä»¶
    where_conditions = []
    if patient_ids:
        ids_str = ", ".join(str(x) for x in patient_ids)
        where_conditions.append(f"{id_col} IN ({ids_str})")
    
    where_clause = ""
    if where_conditions:
        where_clause = "WHERE " + " AND ".join(where_conditions)
    
    # æ„å»ºæœ€ç»ˆåˆå¹¶æŸ¥è¯¢ï¼ˆFULL OUTER JOINæ‰€æœ‰CTEï¼‰
    if len(value_columns) == 1:
        # å•åˆ—ç®€å•å¤„ç†
        query = f"""
        WITH raw_data AS (
            SELECT {id_col}, {time_col}, {value_columns[0]}
            FROM read_parquet('{glob_pattern}', union_by_name=true)
            {where_clause}
        ),
        {cte_parts[0]}
        SELECT {id_col}, charttime, {value_columns[0]}
        FROM agg_0
        ORDER BY {id_col}, charttime
        """
    else:
        # å¤šåˆ—åˆå¹¶
        # ä½¿ç”¨COALESCEé€æ­¥åˆå¹¶æ‰€æœ‰CTE
        coalesce_id = f"COALESCE(agg_0.{id_col}"
        coalesce_time = "COALESCE(agg_0.charttime"
        
        for i in range(1, len(value_columns)):
            coalesce_id += f", agg_{i}.{id_col}"
            coalesce_time += f", agg_{i}.charttime"
        
        coalesce_id += f") as {id_col}"
        coalesce_time += ") as charttime"
        
        # æ„å»ºJOINé“¾
        join_sql = "agg_0"
        for i in range(1, len(value_columns)):
            prev_id = ", ".join(f"agg_{j}.{id_col}" for j in range(i))
            prev_time = ", ".join(f"agg_{j}.charttime" for j in range(i))
            join_sql += f"""
            FULL OUTER JOIN agg_{i} 
                ON COALESCE({prev_id}) = agg_{i}.{id_col} 
                AND COALESCE({prev_time}) = agg_{i}.charttime"""
        
        select_cols = [coalesce_id, coalesce_time]
        select_cols += [f"agg_{i}.{col}" for i, col in enumerate(value_columns)]
        
        query = f"""
        WITH raw_data AS (
            SELECT {id_col}, {time_col}, {', '.join(value_columns)}
            FROM read_parquet('{glob_pattern}', union_by_name=true)
            {where_clause}
        ),
        {','.join(cte_parts)}
        SELECT {', '.join(select_cols)}
        FROM {join_sql}
        ORDER BY 1, 2
        """
    
    # æ‰§è¡ŒæŸ¥è¯¢
    conn = duckdb.connect()
    conn.execute("SET timezone='UTC'")
    # ğŸ”§ ç¦ç”¨DuckDBè¿›åº¦æ¡ï¼Œé¿å…ç»ˆç«¯è¾“å‡ºå¼€é”€
    conn.execute("SET enable_progress_bar = false")
    conn.execute("SET enable_progress_bar_print = false")
    
    try:
        df = conn.execute(query).fetchdf()
        logger.info(f"ğŸš€ å®½è¡¨æ‰¹é‡åŠ è½½å®Œæˆ: {table_name} {value_columns} -> {len(df):,} è¡Œ")
        return df
    finally:
        conn.close()
