"""Data loading utilities for ICU datasets."""

from __future__ import annotations

import enum
from dataclasses import dataclass, field
from pathlib import Path
import logging
from typing import Any, Callable, Dict, Iterable, List, Mapping, MutableMapping, Optional, Sequence
from threading import RLock

import pandas as pd

from .config import DataSourceConfig, DataSourceRegistry, DatasetOptions, TableConfig
from .table import ICUTable

# å…¨å±€è°ƒè¯•å¼€å…³ - è®¾ç½®ä¸º False å¯ä»¥å‡å°‘è¾“å‡º
logger = logging.getLogger(__name__)

# ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šæœ€å°å¿…è¦åˆ—é›†ï¼ˆè‡ªåŠ¨åº”ç”¨ï¼‰
MINIMAL_COLUMNS = {
    'chartevents': ['stay_id', 'charttime', 'itemid', 'valuenum', 'valueuom', 'value'],
    'labevents': ['subject_id', 'hadm_id', 'charttime', 'itemid', 'valuenum', 'valueuom'],
    'outputevents': ['stay_id', 'charttime', 'itemid', 'value'],
    'procedureevents': ['stay_id', 'starttime', 'itemid', 'value'],
    'datetimeevents': ['stay_id', 'charttime', 'itemid', 'value'],
    'inputevents': ['stay_id', 'starttime', 'endtime', 'itemid', 'amount', 'amountuom'],
    'icustays': ['stay_id', 'subject_id', 'hadm_id', 'intime', 'outtime', 'los'],
    'd_items': ['itemid', 'label', 'category'],
}


class FilterOp(str, enum.Enum):
    """Supported filter operations for table loading."""

    EQ = "=="
    IN = "in"
    BETWEEN = "between"
    REGEX = "regex"


@dataclass
class FilterSpec:
    """Declarative filter specification for table loading."""

    column: str
    op: FilterOp
    value: Any
    _value_set: Optional[set] = field(default=None, init=False, repr=False)  # âš¡ ç¼“å­˜setç‰ˆæœ¬çš„value

    def __post_init__(self):
        """âš¡ æ€§èƒ½ä¼˜åŒ–: é¢„è®¡ç®—valueçš„setå½¢å¼ç”¨äºisinæ“ä½œ"""
        if self.op == FilterOp.IN:
            if isinstance(self.value, str):
                self._value_set = {self.value}
            elif hasattr(self.value, '__iter__'):
                self._value_set = set(self.value)
            else:
                self._value_set = {self.value}

    def apply(self, frame: pd.DataFrame) -> pd.DataFrame:
        # âš¡ æ€§èƒ½ä¼˜åŒ–: è¿”å›è§†å›¾è€Œéå‰¯æœ¬ï¼Œç”±è°ƒç”¨è€…å†³å®šæ˜¯å¦éœ€è¦copy
        if self.op == FilterOp.EQ:
            mask = frame[self.column] == self.value
            return frame.loc[mask]
        if self.op == FilterOp.IN:
            # âš¡ ä½¿ç”¨é¢„è®¡ç®—çš„setï¼Œé¿å…æ¯æ¬¡éƒ½listè½¬æ¢
            mask = frame[self.column].isin(self._value_set)
            return frame.loc[mask]
        if self.op == FilterOp.BETWEEN:
            lower, upper = self.value
            mask = frame[self.column].between(lower, upper)
            return frame.loc[mask]
        if self.op == FilterOp.REGEX:
            # Regex filtering for rgx_itm concepts (e.g., drug names)
            mask = frame[self.column].str.contains(self.value, case=False, na=False, regex=True)
            return frame.loc[mask]
        raise ValueError(f"Unsupported filter operation: {self.op}")


class ICUDataSource:
    """Lightweight facade that loads tables for a concrete dataset instance."""

    # å…¨å±€æ ¼å¼ä¼˜å…ˆçº§é…ç½®
    _global_format_priority: Optional[List[str]] = None

    @classmethod
    def set_format_priority(cls, priority: List[str]) -> None:
        """è®¾ç½®å…¨å±€æ–‡ä»¶æ ¼å¼ä¼˜å…ˆçº§
        
        Args:
            priority: æ ¼å¼åˆ—è¡¨ï¼ˆå½“å‰åªæ”¯æŒ ['parquet']ï¼‰
        
        Examples:
            >>> # åªä½¿ç”¨ Parquet æ ¼å¼ï¼ˆçº¯ Pythonï¼Œæ— éœ€ Rï¼‰
            >>> ICUDataSource.set_format_priority(['parquet'])
        """
        cls._global_format_priority = priority

    @classmethod
    def get_format_priority(cls) -> List[str]:
        """è·å–å½“å‰çš„æ ¼å¼ä¼˜å…ˆçº§é…ç½®
        
        Returns:
            æ ¼å¼åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
        """
        # 1. å¦‚æœè®¾ç½®äº†å…¨å±€ä¼˜å…ˆçº§ï¼Œä½¿ç”¨å…¨å±€é…ç½®
        if cls._global_format_priority is not None:
            return cls._global_format_priority
        
        # 2. æ£€æŸ¥ç¯å¢ƒå˜é‡
        import os
        env_priority = os.environ.get('PYRICU_FORMAT_PRIORITY')
        if env_priority:
            return [fmt.strip() for fmt in env_priority.split(',')]
        
        # åªæ”¯æŒ Parquet æ ¼å¼
        return ['parquet']

    def __init__(
        self,
        config: DataSourceConfig,
        *,
        base_path: str | Path | None = None,
        table_sources: Optional[Mapping[str, Any]] = None,
        registry: Optional[DataSourceRegistry] = None,
        default_format: str = "parquet",
        enable_cache: bool = True,
        format_priority: Optional[List[str]] = None,
    ) -> None:
        """åˆå§‹åŒ–æ•°æ®æº
        
        Args:
            config: æ•°æ®æºé…ç½®
            base_path: æ•°æ®æ–‡ä»¶åŸºç¡€è·¯å¾„
            table_sources: è¡¨æ•°æ®æºæ˜ å°„ï¼ˆå¯é€‰ï¼‰
            registry: æ•°æ®æºæ³¨å†Œè¡¨ï¼ˆå¯é€‰ï¼‰
            default_format: é»˜è®¤æ ¼å¼ï¼ˆå·²åºŸå¼ƒï¼Œä½¿ç”¨ format_priorityï¼‰
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
            format_priority: æ–‡ä»¶æ ¼å¼ä¼˜å…ˆçº§ï¼ˆå¯é€‰ï¼‰ï¼Œä¾‹å¦‚ ['parquet', 'fst', 'csv']
                           å¦‚æœæœªæŒ‡å®šï¼Œä½¿ç”¨å…¨å±€é…ç½®æˆ–ç¯å¢ƒå˜é‡
        """
        self.config = config
        self.base_path = Path(base_path) if base_path else None
        self._table_sources: MutableMapping[str, Any] = dict(table_sources or {})
        self.default_format = default_format
        self.registry = registry
        self._dataset_sources: Dict[str, DatasetOptions] = {
            name: table.dataset
            for name, table in self.config.tables.items()
            if table.dataset is not None
        }
        self.enable_cache = enable_cache
        self._table_cache: dict = {}  # ç¼“å­˜å·²åŠ è½½çš„åŸå§‹è¡¨æ•°æ®
        self._preloaded_tables: dict = {}  # ğŸš€ é¢„åŠ è½½çš„å®Œæ•´è¡¨ï¼ˆç”¨äºå¤šæ‚£è€…æ‰¹å¤„ç†ï¼‰
        self.format_priority = format_priority or self.get_format_priority()
        self._lock = RLock()

    def register_table_source(self, table: str, source: Any) -> None:
        """Register a callable/file path used to load ``table``."""
        self._table_sources[table] = source
    
    def clear_cache(self) -> None:
        """æ¸…é™¤è¡¨ç¼“å­˜,é‡Šæ”¾å†…å­˜ã€‚"""
        with self._lock:
            self._table_cache.clear()
            self._preloaded_tables.clear()
    
    def preload_tables(self, table_names: List[str], patient_ids: Optional[List[int]] = None) -> None:
        """
        ğŸš€ é¢„åŠ è½½å¤§è¡¨åˆ°å†…å­˜ï¼Œé¿å…é‡å¤I/O
        
        Args:
            table_names: è¦é¢„åŠ è½½çš„è¡¨ååˆ—è¡¨
            patient_ids: å¯é€‰çš„æ‚£è€…IDåˆ—è¡¨ï¼Œç”¨äºé¢„è¿‡æ»¤
        """
        base_patient_ids = list(patient_ids) if patient_ids is not None else None
        for table_name in table_names:
            with self._lock:
                if table_name in self._preloaded_tables:
                    continue
                
            # åŠ è½½å®Œæ•´è¡¨ï¼ˆä½¿ç”¨æœ€å°åˆ—é›†ï¼‰
            columns = MINIMAL_COLUMNS.get(table_name)
            
            # ä¸ä½¿ç”¨filtersï¼Œç›´æ¥åŠ è½½å®Œæ•´è¡¨
            table = self.load_table(table_name, columns=columns, verbose=False)
            df = table.dataframe()  # ä¿®æ­£ï¼šè¿™æ˜¯ä¸ªæ–¹æ³•
            
            # å¦‚æœæä¾›äº†patient_idsï¼Œé¢„è¿‡æ»¤
            if base_patient_ids is not None:
                id_col = None
                filter_ids = base_patient_ids
                if 'stay_id' in df.columns:
                    id_col = 'stay_id'
                elif 'subject_id' in df.columns:
                    # éœ€è¦ä»icustaysè·å–subject_idæ˜ å°„
                    if table_name != 'icustays':
                        icustays = self.load_table('icustays', columns=['stay_id', 'subject_id'], verbose=False)
                        icustays_df = icustays.dataframe()
                        subject_ids = icustays_df[icustays_df['stay_id'].isin(base_patient_ids)]['subject_id'].dropna().astype(int).unique()
                        id_col = 'subject_id'
                        filter_ids = subject_ids.tolist()
                
                if id_col and filter_ids is not None:
                    df = df[df[id_col].isin(filter_ids)]
            
            with self._lock:
                self._preloaded_tables[table_name] = df
            logger.info(f"ğŸ“¦ é¢„åŠ è½½è¡¨ {table_name}: {len(df):,}è¡Œ")
    
    def get_preloaded_table(self, table_name: str) -> Optional[pd.DataFrame]:
        """è·å–é¢„åŠ è½½çš„è¡¨"""
        with self._lock:
            table = self._preloaded_tables.get(table_name)
        return table
    
    def get_cache_info(self) -> dict:
        """è·å–ç¼“å­˜ä¿¡æ¯ã€‚"""
        with self._lock:
            total_size = sum(df.memory_usage(deep=True).sum() for df in self._table_cache.values())
            cached_tables = len(self._table_cache)
        return {
            'cached_tables': cached_tables,
            'memory_mb': total_size / (1024 * 1024)
        }

    def load_table(
        self,
        table_name: str,
        *,
        columns: Optional[Iterable[str]] = None,
        filters: Optional[Iterable[FilterSpec]] = None,
        verbose: bool = False,
    ) -> ICUTable:
        """Load and wrap a table according to the stored configuration."""
        
        table_cfg = self.config.get_table(table_name)

        # ğŸš€ ä¼˜åŒ–1ï¼šä¼˜å…ˆä½¿ç”¨é¢„åŠ è½½çš„è¡¨
        preloaded_frame = None
        with self._lock:
            if table_name in self._preloaded_tables:
                preloaded_frame = self._preloaded_tables[table_name]

        if preloaded_frame is not None:
            frame_view = preloaded_frame

            # åº”ç”¨åˆ—è¿‡æ»¤ï¼ˆé¿å…æå‰å¤åˆ¶æ•´å¼ è¡¨ï¼‰
            if columns is not None:
                available_cols = [c for c in columns if c in frame_view.columns]
                frame_view = frame_view.loc[:, available_cols]

            # åº”ç”¨è¡Œè¿‡æ»¤
            if filters:
                frame_filtered = frame_view
                for spec in filters:
                    frame_filtered = spec.apply(frame_filtered)
            else:
                frame_filtered = frame_view

            frame = frame_filtered.copy()
        else:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šcolumnsï¼Œä½¿ç”¨æœ€å°åˆ—é›†
            if columns is None:
                from .load_concepts import MINIMAL_COLUMNS_MAP, USE_MINIMAL_COLUMNS
                if USE_MINIMAL_COLUMNS and table_name in MINIMAL_COLUMNS_MAP:
                    columns = MINIMAL_COLUMNS_MAP[table_name]

            # æå– patient_ids è¿‡æ»¤å™¨ç”¨äºåˆ†åŒºé¢„è¿‡æ»¤
            patient_ids_filter = None
            if filters:
                for spec in filters:
                    # æ”¯æŒå„æ•°æ®åº“çš„IDåˆ—å
                    id_columns = ['subject_id', 'icustay_id', 'hadm_id', 'stay_id',  # MIMIC
                                 'admissionid', 'patientid',  # AUMC
                                 'patientunitstayid',  # eICU
                                 'patientid']  # HiRID
                    if spec.op == FilterOp.IN and spec.column in id_columns:
                        patient_ids_filter = spec
                        break

            frame = self._load_raw_frame(table_name, columns, patient_ids_filter=patient_ids_filter)

            if filters:
                for spec in filters:
                    frame = spec.apply(frame)
            else:
                frame = frame.copy()

        defaults = table_cfg.defaults
        id_columns = (
            [defaults.id_var]
            if defaults.id_var and defaults.id_var in frame.columns
            else []
        )
        index_column = defaults.index_var if defaults.index_var in frame.columns else None
        time_columns = [
            column for column in defaults.time_vars if column in frame.columns
        ]
        value_column = defaults.val_var if defaults.val_var in frame.columns else None
        unit_column = defaults.unit_var if defaults.unit_var in frame.columns else None

        time_like_cols = set(time_columns)
        if index_column:
            time_like_cols.add(index_column)
        
        # ğŸ”§ AUMCç‰¹æ®Šå¤„ç†ï¼šæ—¶é—´åˆ—æ˜¯æ¯«ç§’,éœ€è¦è½¬æ¢ä¸ºåˆ†é’Ÿ (å‚è€ƒR ricuçš„ms_as_mins)
        # R ricu: ms_as_mins <- function(x) min_as_mins(as.integer(x / 6e4))
        # è¿™æ ·å¤„ç†å,AUMCçš„æ—¶é—´å•ä½ä¸å…¶ä»–æ•°æ®åº“ä¸€è‡´(éƒ½æ˜¯åˆ†é’Ÿ)
        if self.config.name == 'aumc':
            for column in time_like_cols:
                if column in frame.columns and pd.api.types.is_numeric_dtype(frame[column]):
                    # å°†æ¯«ç§’è½¬æ¢ä¸ºåˆ†é’Ÿ: ms / 60000
                    frame[column] = (frame[column] / 60000.0).astype('float64')
        
        for column in time_like_cols:
            # åªæœ‰å½“åˆ—å­˜åœ¨ä¸”ä¸æ˜¯numericç±»å‹æ—¶æ‰è½¬æ¢
            # å¦‚æœå·²ç»æ˜¯numericï¼Œå¯èƒ½æ˜¯å·²ç»å¯¹é½è¿‡çš„å°æ—¶æ•°
            if column in frame.columns:
                frame[column] = _coerce_datetime(frame[column])

        if verbose and logger.isEnabledFor(logging.INFO):
            id_label = id_columns[0] if id_columns else defaults.id_var or "N/A"
            unique_count = (
                frame[id_label].nunique()
                if id_label in frame.columns
                else "N/A"
            )
            logger.info(
                "ğŸ” è¡¨ %s åŠ è½½å: %d è¡Œ, å”¯ä¸€%s: %s",
                table_name,
                len(frame),
                id_label,
                unique_count,
            )

        return ICUTable(
            data=frame,
            id_columns=id_columns,
            index_column=index_column,
            value_column=value_column,
            unit_column=unit_column,
            time_columns=time_columns,
        )

    def _load_raw_frame(
        self,
        table_name: str,
        columns: Optional[Iterable[str]],
        patient_ids_filter: Optional[FilterSpec] = None,
    ) -> pd.DataFrame:
        # è¶…é›†ç¼“å­˜ç­–ç•¥ï¼šæ£€æŸ¥æ˜¯å¦æœ‰åŒ…å«æ‰€éœ€åˆ—çš„ç¼“å­˜
        if self.enable_cache and columns:
            requested_cols = set(columns)
            with self._lock:
                # æŸ¥æ‰¾åŒ…å«æ‰€æœ‰è¯·æ±‚åˆ—çš„ç¼“å­˜
                for cache_key, cached_frame in self._table_cache.items():
                    if cache_key[0] == table_name:  # è¡¨ååŒ¹é…
                        cached_cols = set(cached_frame.columns)
                        if requested_cols.issubset(cached_cols):
                            # æ‰¾åˆ°åŒ…å«æ‰€éœ€åˆ—çš„ç¼“å­˜ï¼Œè¿”å›å­é›†
                            result_frame = cached_frame[list(columns)]
                            if patient_ids_filter:
                                return patient_ids_filter.apply(result_frame)
                            return result_frame
        
        # ğŸš€ OPTIMIZATION: ç¼“å­˜é”®ä¸åŒ…å«patient_ids_filterä»¥å®ç°è·¨æ¦‚å¿µå…±äº«
        # å¯¹äºåŒä¸€æ‰¹æ‚£è€…çš„å¤šä¸ªæ¦‚å¿µåŠ è½½,åªåœ¨ç¬¬ä¸€æ¬¡è¯»å–è¡¨,åç»­ä»ç¼“å­˜ä¸­è¿‡æ»¤
        # è¿™å°†charteventsç­‰å¤§è¡¨çš„åŠ è½½ä»Næ¬¡(æ¯æ¦‚å¿µä¸€æ¬¡)å‡å°‘åˆ°1æ¬¡
        cache_key = (table_name, tuple(sorted(columns)) if columns else None)
        
        # æ£€æŸ¥ç²¾ç¡®åŒ¹é…çš„ç¼“å­˜
        cached_frame = None
        if self.enable_cache:
            with self._lock:
                cached_frame = self._table_cache.get(cache_key)
        
        if cached_frame is not None:
            # ä»ç¼“å­˜ä¸­å–æ•°æ®åå†åº”ç”¨patientè¿‡æ»¤
            if patient_ids_filter:
                # è¿”å›è¿‡æ»¤åçš„è§†å›¾ï¼Œé¿å…æ‹·è´æ•´ä¸ªç¼“å­˜è¡¨
                return patient_ids_filter.apply(cached_frame)
            # å¦‚æœä¸éœ€è¦è¿‡æ»¤ï¼Œè¿”å›åˆ‡ç‰‡è§†å›¾è€Œéå‰¯æœ¬
            return cached_frame[:]
        
        loader = self._table_sources.get(table_name)
        dataset_cfg = self._dataset_sources.get(table_name)
        if loader is None and dataset_cfg is not None:
            frame = self._read_dataset(table_name, dataset_cfg, columns, patient_ids_filter)
        elif loader is None:
            # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦ä¸ºå¤šæ–‡ä»¶é…ç½®ï¼Œå¦‚æœæ˜¯ï¼Œä½¿ç”¨ç›®å½•è·¯å¾„
            table_cfg = self.config.get_table(table_name)
            if len(table_cfg.files) > 1:
                # å¤šæ–‡ä»¶é…ç½®ï¼šä½¿ç”¨ç›®å½•è·¯å¾„ä»¥å¯ç”¨å¤šæ–‡ä»¶è¯»å–
                base_path = self.base_path or Path.cwd()
                if table_cfg.files and table_cfg.files[0].get('path'):
                    # è·å–ç›®å½•è·¯å¾„
                    first_path = Path(table_cfg.files[0]['path'])
                    multi_file_dir = base_path / first_path.parent
                    if multi_file_dir.is_dir():
                        loader = multi_file_dir
                    else:
                        # å›é€€åˆ°å•ä¸ªæ–‡ä»¶è§£æ
                        loader = self._resolve_loader_from_disk(table_name)
                else:
                    # å›é€€åˆ°å•ä¸ªæ–‡ä»¶è§£æ
                    loader = self._resolve_loader_from_disk(table_name)
            else:
                loader = self._resolve_loader_from_disk(table_name)
            if loader is None:
                # å¯¹äºmiivæ•°æ®æºï¼Œå¦‚æœè¡¨åœ¨é…ç½®ä¸­å®šä¹‰äº†ä½†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºDataFrame
                # è¿™å…è®¸åœ¨demoæ•°æ®ä¸­ç¼ºå°‘æŸäº›è¡¨æ—¶ç»§ç»­è¿è¡Œ
                if self.config.name == 'miiv' and table_name in self.config.tables:
                    # è¿”å›ç©ºDataFrameï¼Œä¿æŒä¸é…ç½®ä¸­è¡¨ç»“æ„ä¸€è‡´çš„åˆ—
                    table_cfg = self.config.get_table(table_name)
                    defaults = table_cfg.defaults
                    # å°è¯•ä»é…ç½®ä¸­è·å–é¢„æœŸçš„åˆ—
                    expected_cols = []
                    if defaults.id_var:
                        expected_cols.append(defaults.id_var)
                    if defaults.index_var:
                        expected_cols.append(defaults.index_var)
                    if defaults.val_var:
                        expected_cols.append(defaults.val_var)
                    if defaults.unit_var:
                        expected_cols.append(defaults.unit_var)
                    if defaults.time_vars:
                        expected_cols.extend(defaults.time_vars)
                    
                    # è¿”å›ç©ºDataFrameï¼Œé¿å…æŠ›å‡ºé”™è¯¯
                    return pd.DataFrame(columns=expected_cols if expected_cols else ['index'])
                
                raise KeyError(
                    f"No table source registered for '{table_name}' "
                    f"in data source '{self.config.name}'"
                )
        if callable(loader):
            frame = loader()
        else:
            frame = self._read_file(Path(loader), columns, patient_ids_filter=patient_ids_filter)

        if columns is not None:
            missing = set(columns) - set(frame.columns)
            if missing:
                raise KeyError(
                    f"Columns {sorted(missing)} not found in table '{table_name}'"
                )
            frame = frame[list(columns)]
        
        # ğŸš€ OPTIMIZATION: ç¼“å­˜å®Œæ•´è¡¨(æœªç»patientè¿‡æ»¤)ä»¥å®ç°è·¨æ¦‚å¿µå…±äº«
        # patientè¿‡æ»¤åœ¨ä»ç¼“å­˜è¯»å–æ—¶åº”ç”¨(è§ä¸Šé¢cached_frameåˆ†æ”¯)
        # âš¡ æ€§èƒ½ä¼˜åŒ–: ç¼“å­˜åŸå§‹frameï¼Œè¿”å›è¿‡æ»¤åçš„ç»“æœ
        if self.enable_cache:
            with self._lock:
                # ç¼“å­˜åŸå§‹æœªè¿‡æ»¤çš„è¡¨
                self._table_cache[cache_key] = frame
        
        # åº”ç”¨patientè¿‡æ»¤(å¦‚æœæœ‰)
        if patient_ids_filter:
            return patient_ids_filter.apply(frame)
        
        # æœªè¿‡æ»¤ä¸”æœªç¼“å­˜æ—¶è¿”å›åˆ‡ç‰‡
        return frame[:] if self.enable_cache else frame

    def _resolve_loader_from_disk(self, table_name: str) -> Optional[Callable[[], pd.DataFrame] | Path]:
        if not self.base_path:
            return None
        
        table_cfg = self.config.get_table(table_name)
        explicit = table_cfg.first_file()
        if explicit:
            explicit_path = self.base_path / explicit
            if explicit_path.exists():
                # Accept directories (partitioned datasets) and Parquet files immediately
                if explicit_path.is_dir():
                    return explicit_path
                if explicit_path.suffix.lower() in {".parquet", ".pq"}:
                    return explicit_path
                # Otherwise continue searching for a Parquet counterpart below
        
        # MIMIC-IV æ–‡ä»¶åæ˜ å°„: é…ç½®ä¸­çš„è¡¨å -> æ–‡ä»¶ç³»ç»Ÿä¸­çš„å®é™…æ–‡ä»¶å
        # å› ä¸º MIMIC-IV æ”¹äº†è¡¨å,ä½†é…ç½®æ–‡ä»¶è¿˜æ˜¯ç”¨çš„æ—§å
        # MIMIC-IV å°† MIMIC-III çš„ä¸¤ä¸ªè¡¨åˆå¹¶äº†:
        # - procedureevents_mv -> procedureevents
        # - inputevents_cv å’Œ inputevents_mv -> inputevents
        # æ³¨æ„ï¼šå¯¹äºmiivæ•°æ®æºï¼Œæ¦‚å¿µå­—å…¸ä¸­ç›´æ¥ä½¿ç”¨procedureeventså’Œinputevents
        # ä½†å¦‚æœæ•°æ®æºé…ç½®ä¸­æ²¡æœ‰å®šä¹‰è¿™äº›è¡¨ï¼Œéœ€è¦ä»æ—§è¡¨åæ˜ å°„

        config_to_file_mappings = {
            'procedureevents_mv': 'procedureevents',  # é…ç½®å -> æ–‡ä»¶å
            'inputevents_mv': 'inputevents',
            'inputevents_cv': 'inputevents',  # MIMIC-IV åˆå¹¶äº†è¿™ä¸¤ä¸ªè¡¨
        }
        
        # å¯¹äºmiivæ•°æ®æºï¼Œå¦‚æœè¯·æ±‚çš„è¡¨åœ¨é…ç½®ä¸­ä¸å­˜åœ¨ï¼Œå°è¯•ä»æ˜ å°„ä¸­æŸ¥æ‰¾
        # ä¾‹å¦‚ï¼šæ¦‚å¿µå­—å…¸è¯·æ±‚procedureeventsï¼Œä½†é…ç½®ä¸­å¯èƒ½åªæœ‰procedureevents_mvçš„å®šä¹‰
        if self.config.name == 'miiv':
            # åå‘æ˜ å°„ï¼šå¦‚æœè¯·æ±‚çš„è¡¨åä¸åœ¨é…ç½®ä¸­ï¼Œå°è¯•æŸ¥æ‰¾å¯¹åº”çš„æ—§è¡¨å
            reverse_mapping = {
                'procedureevents': 'procedureevents_mv',
                'inputevents': 'inputevents_mv',  # ä¼˜å…ˆä½¿ç”¨inputevents_mv
            }
            if table_name not in self.config.tables and table_name in reverse_mapping:
                # ä½¿ç”¨æ˜ å°„åçš„è¡¨åæŸ¥æ‰¾æ–‡ä»¶
                mapped_table = reverse_mapping[table_name]
                file_base_name = config_to_file_mappings.get(mapped_table, table_name)
            else:
                file_base_name = config_to_file_mappings.get(table_name, table_name)
        else:
            # è·å–å®é™…è¦æŸ¥æ‰¾çš„æ–‡ä»¶å
            file_base_name = config_to_file_mappings.get(table_name, table_name)

            # ä»è¡¨é…ç½®ä¸­è·å–å®é™…çš„æ–‡ä»¶åï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if table_name in self.config.tables:
                table_config = self.config.tables[table_name]
                if hasattr(table_config, 'files') and table_config.files:
                    # è·å–ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„è·¯å¾„ï¼Œå»æ‰æ‰©å±•å
                    file_info = table_config.files[0]
                    if isinstance(file_info, dict) and 'path' in file_info:
                        file_path = file_info['path']
                    elif hasattr(file_info, 'path'):
                        file_path = file_info.path
                    else:
                        file_path = str(file_info)
                    # å»æ‰æ‰©å±•åï¼Œè·å–åŸºç¡€æ–‡ä»¶åï¼ˆå¤„ç†å¤åˆæ‰©å±•åå¦‚.csv.gzï¼‰
                    parts = file_path.split('.')
                    if len(parts) >= 2:
                        # å¤„ç†å¤åˆæ‰©å±•åå¦‚ .csv.gz
                        if parts[-1] == 'gz' and len(parts) >= 3 and parts[-2] == 'csv':
                            file_base_name = '.'.join(parts[:-2])
                        else:
                            file_base_name = '.'.join(parts[:-1])
                    else:
                        file_base_name = file_path
        
        # Only support Parquet format - try different name variations
        for name in [file_base_name, file_base_name.lower(), table_name, table_name.lower()]:
            # Try .parquet extension
            parquet_candidate = self.base_path / f"{name}.parquet"
            if parquet_candidate.exists():
                return parquet_candidate
            # Try .pq extension (short form)
            pq_candidate = self.base_path / f"{name}.pq"
            if pq_candidate.exists():
                return pq_candidate
        
        # Check subdirectory for partitioned parquet data (common in hirid observations)
        if self.base_path is not None:
            for name in [table_name, table_name.lower()]:
                subdir = self.base_path / name
                if subdir.is_dir():
                    # Look for Parquet files
                    parquet_files = list(subdir.glob("*.parquet")) + list(subdir.glob("*.pq"))
                    if parquet_files:
                        return subdir
        
        # Fall back to explicit file if it exists (e.g., CSV) so that callers can handle it
        if explicit:
            explicit_path = self.base_path / explicit
            if explicit_path.exists():
                return explicit_path

        return None

    def _get_minimal_columns(self, table_name: str) -> Optional[List[str]]:
        """è·å–è¡¨çš„æœ€å°å¿…è¦åˆ—é›†ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰"""
        return MINIMAL_COLUMNS.get(table_name)
    
    def _read_file(self, path: Path, columns: Optional[Iterable[str]], patient_ids_filter: Optional[FilterSpec] = None) -> pd.DataFrame:
        # Handle directory (partitioned data)
        if path.is_dir():
            return self._read_partitioned_data_optimized(path, columns, patient_ids_filter)
        
        suffix = path.suffix.lower()
        suffixes = [s.lower() for s in path.suffixes]
        
        # Preferred: Parquet format
        if suffix in {".parquet", ".pq"}:
            # ğŸš€ ä½¿ç”¨PyArrowè¿‡æ»¤å™¨ä¼˜åŒ–å¤§æ–‡ä»¶è¯»å–
            if patient_ids_filter and patient_ids_filter.op == FilterOp.IN:
                try:
                    import pyarrow.parquet as pq
                    import pyarrow as pa
                    # âš¡ ä½¿ç”¨é¢„è®¡ç®—çš„set
                    target_ids = list(patient_ids_filter._value_set)
                    
                    # ä½¿ç”¨PyArrowè¯»å–å¹¶è¿‡æ»¤ - ä½¿ç”¨ DNF æ ¼å¼
                    df = pq.read_table(
                        path,
                        columns=list(columns) if columns else None,
                        filters=[[(patient_ids_filter.column, 'in', target_ids)]]
                    ).to_pandas()
                except (ImportError, Exception) as e:
                    # å¦‚æœPyArrowè¿‡æ»¤å¤±è´¥ï¼Œå›é€€åˆ°pandasåè¿‡æ»¤
                    df = pd.read_parquet(path, columns=list(columns) if columns else None, engine='pyarrow')
                    if patient_ids_filter.column in df.columns:
                        df = patient_ids_filter.apply(df)
            else:
                df = pd.read_parquet(path, columns=list(columns) if columns else None, engine='pyarrow')
            
            # å¤„ç†é‡å¤åˆ—åï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if df.columns.duplicated().any():
                import pandas.io.common
                df.columns = pandas.io.common.dedup_names(df.columns, is_potential_multiindex=False)
            return df
        
        # Fallback: CSV (optionally compressed)
        is_csv_gz = suffix == ".gz" and len(suffixes) >= 2 and suffixes[-2] == ".csv"
        if suffix == ".csv" or is_csv_gz:
            compression = "gzip" if is_csv_gz else None
            return self._read_csv_file(path, columns, patient_ids_filter, compression=compression)

        raise ValueError(
            f"Unsupported file format '{path.suffix}' for {path.name}. Provide Parquet or CSV inputs."
        )

    def _read_csv_file(
        self,
        path: Path,
        columns: Optional[Iterable[str]],
        patient_ids_filter: Optional[FilterSpec],
        *,
        compression: Optional[str] = None,
    ) -> pd.DataFrame:
        logger.warning(
            "âš ï¸  Falling back to CSV read for %s. Consider generating Parquet files for better performance.",
            path.name,
        )
        usecols = list(columns) if columns else None
        df = pd.read_csv(path, usecols=usecols, compression=compression, low_memory=False)
        if patient_ids_filter and patient_ids_filter.column in df.columns:
            values = patient_ids_filter.value
            if not isinstance(values, (list, tuple, set, pd.Series)):
                values = [values]
            df = df[df[patient_ids_filter.column].isin(values)]
        return df
    
    def _read_partitioned_data_optimized(self, directory: Path, columns: Optional[Iterable[str]], patient_ids_filter: Optional[FilterSpec] = None) -> pd.DataFrame:
        """è¯»å–åˆ†åŒºæ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼šè‡ªåŠ¨å¿½ç•¥.fstæ–‡ä»¶ï¼Œåªè¯»å–.parquetï¼‰"""
        try:
            import pyarrow.dataset as ds
            import pyarrow.parquet as pq
            import pyarrow.compute as pc
            
            # ğŸš€ ç­–ç•¥1ï¼šå°è¯•ä½¿ç”¨PyArrow Datasetï¼ˆæœ€å¿«ï¼Œä½†éœ€è¦æ‰€æœ‰æ–‡ä»¶æ ¼å¼ä¸€è‡´ï¼‰
            try:
                dataset = ds.dataset(
                    directory,
                    format='parquet',
                    partitioning=None,
                    exclude_invalid_files=True  # å¿½ç•¥.fstç­‰éparquetæ–‡ä»¶
                )
                
                filter_expr = None
                if patient_ids_filter:
                    id_col = patient_ids_filter.column
                    values = patient_ids_filter.value
                    if isinstance(values, (list, tuple, set)):
                        value_list = list(values)
                    elif isinstance(values, pd.Series):
                        value_list = values.tolist()
                    else:
                        try:
                            value_list = list(values)
                        except TypeError:
                            value_list = [values]

                    if not value_list:
                        wanted_cols = list(columns) if columns else dataset.schema.names
                        return pd.DataFrame(columns=wanted_cols)

                    try:
                        filter_expr = ds.field(id_col).isin(value_list)
                    except Exception:
                        filter_expr = None

                if columns:
                    table = dataset.to_table(columns=list(columns), filter=filter_expr)
                else:
                    table = dataset.to_table(filter=filter_expr)

                return table.to_pandas()
            
            except Exception as e:
                # Datasetè¯»å–å¤±è´¥ï¼Œå›é€€åˆ°é€æ–‡ä»¶è¯»å–
            # ğŸš€ ç­–ç•¥2ï¼šé€æ–‡ä»¶è¯»å–å¹¶ç«‹å³è¿‡æ»¤ï¼ˆå†…å­˜å‹å¥½ï¼Œé€‚åˆå¤§æ•°æ®é›†ï¼‰
            parquet_files = sorted(directory.glob("*.parquet"))
            if not parquet_files:
                parquet_files = sorted(directory.glob("*.pq"))
            
            if not parquet_files:
                raise FileNotFoundError(f"No parquet files found in {directory}")
            
            # å‡†å¤‡è¿‡æ»¤æ¡ä»¶
            filter_ids = None
            id_column = None
            if patient_ids_filter:
                id_column = patient_ids_filter.column
                if isinstance(patient_ids_filter.value, (list, tuple, set)):
                    filter_ids = set(patient_ids_filter.value)
                else:
                    filter_ids = {patient_ids_filter.value}
            
            # é€æ–‡ä»¶è¯»å–+ç«‹å³è¿‡æ»¤
            chunks = []
            for file_path in parquet_files:
                # è¯»å–å•ä¸ªæ–‡ä»¶ï¼ˆåªè¯»å–éœ€è¦çš„åˆ—ï¼‰
                if columns:
                    df_chunk = pd.read_parquet(file_path, columns=list(columns))
                else:
                    df_chunk = pd.read_parquet(file_path)
                
                # ç«‹å³åº”ç”¨è¿‡æ»¤ï¼ˆå‡å°‘å†…å­˜å ç”¨ï¼‰
                if filter_ids and id_column and id_column in df_chunk.columns:
                    df_chunk = df_chunk[df_chunk[id_column].isin(filter_ids)]
                
                # åªä¿ç•™æœ‰æ•°æ®çš„chunk
                if len(df_chunk) > 0:
                    chunks.append(df_chunk)
            
            # åˆå¹¶æ‰€æœ‰chunks
            if chunks:
                return pd.concat(chunks, ignore_index=True)
            else:
                # è¿”å›ç©ºDataFrameï¼Œä¿æŒåˆ—ç»“æ„
                if columns:
                    return pd.DataFrame(columns=list(columns))
                else:
                    return pd.DataFrame()
            
        except Exception as e:
            # æœ€ç»ˆå›é€€åˆ°åŸå§‹å®ç°
            logger.warning(f"ä¼˜åŒ–è¯»å–å¤±è´¥: {e}ï¼Œå›é€€åˆ°fallbackæ–¹æ³•")
            return self._read_partitioned_data_fallback(directory, columns, patient_ids_filter)
    
    def _read_partitioned_data_fallback(self, directory: Path, columns: Optional[Iterable[str]], patient_ids_filter: Optional[FilterSpec] = None) -> pd.DataFrame:
        """Read partitioned data from a directory, respecting format priority."""
        
        # ğŸ” è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºåˆ†åŒºåŠ è½½è¯·æ±‚çš„åˆ—
        if DEBUG_MODE and columns:
        
        # åªæ”¯æŒ Parquet æ ¼å¼
        files = sorted(directory.glob("*.parquet")) + sorted(directory.glob("*.pq"))
        if not files:
            # æ²¡æœ‰æ‰¾åˆ° parquet æ–‡ä»¶
            return pd.DataFrame()
        
        num_files = len(files)
        
        # å‡†å¤‡æ‚£è€…IDè¿‡æ»¤å™¨ (æ”¯æŒå¤šç§æ•°æ®åº“çš„IDåˆ—)
        filter_tuple = None
        if patient_ids_filter and patient_ids_filter.column in ['subject_id', 'hadm_id', 'icustay_id', 'stay_id', 'admissionid', 'patientid']:
            target_ids = set(patient_ids_filter.value) if not isinstance(patient_ids_filter.value, str) else {patient_ids_filter.value}
            filter_tuple = (patient_ids_filter.column, target_ids)
        else:
        # ğŸ”§ ä¿®å¤ï¼šä¼ é€’å…·ä½“çš„parquetæ–‡ä»¶åˆ—è¡¨ï¼Œè€Œä¸æ˜¯ç›®å½•ï¼Œé¿å…æ··åˆæ ¼å¼é—®é¢˜
        dataset_df = self._read_parquet_dataset(
            directory,
            files,  # ä¼ é€’å…·ä½“çš„parquetæ–‡ä»¶åˆ—è¡¨
            columns=list(columns) if columns else None,
            filter_spec=patient_ids_filter,
        )
        if dataset_df is not None:
            return dataset_df
        # Fallback: iterate individual parquet files
        dfs = []
        arrow_filters = None
        if patient_ids_filter:
            arrow_filters = self._build_dataset_filter(patient_ids_filter)
        for f in files:
            if arrow_filters is not None or columns is not None:
                try:
                    import pyarrow.parquet as pq  # type: ignore
                    table = pq.read_table(
                        f,
                        columns=list(columns) if columns else None,
                    )
                    if arrow_filters is not None:
                        import pyarrow.compute as pc  # type: ignore
                        table = table.filter(arrow_filters)
                    df = table.to_pandas()
                    dfs.append(df)
                    continue
                except Exception:
                    pass  # Fallback to pandas.read_parquet below
            df = pd.read_parquet(f, columns=list(columns) if columns else None)
            if filter_tuple:
                col_name, target_ids = filter_tuple
                if col_name in df.columns:
                    df = df[df[col_name].isin(target_ids)]
            dfs.append(df)
        
        # åˆå¹¶æ‰€æœ‰åˆ†åŒº
        if dfs:
            return pd.concat(dfs, ignore_index=True)
        
        # æ²¡æœ‰æ‰¾åˆ°ä»»ä½•parquetæ–‡ä»¶
        return pd.DataFrame()

    def _read_parquet_dataset(
        self,
        directory: Path,
        files: Optional[List[Path]] = None,
        columns: Optional[Sequence[str]] = None,
        filter_spec: Optional[FilterSpec] = None,
    ) -> Optional[pd.DataFrame]:
        """Attempt to read a parquet directory via PyArrow Dataset for fast filtering."""
        try:
            import pyarrow.dataset as ds  # type: ignore
        except ImportError:
            return None

        filter_expr = None
        if filter_spec is not None:
            filter_expr = self._build_dataset_filter(filter_spec)
        try:
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¼ å…¥çš„æ–‡ä»¶åˆ—è¡¨åˆ›å»ºdatasetï¼Œé¿å…æ··åˆæ ¼å¼é—®é¢˜
            if files is not None:
                # ä½¿ç”¨å…·ä½“çš„parquetæ–‡ä»¶åˆ—è¡¨
                dataset = ds.dataset(files, format="parquet")
            else:
                # å›é€€åˆ°åŸå§‹é€»è¾‘ï¼ˆä»…åŒ…å«parquetæ–‡ä»¶çš„ç›®å½•ï¼‰
                try:
                    dataset = ds.dataset(directory, format="parquet", partitioning="hive")
                except (ValueError, TypeError):
                    dataset = ds.dataset(directory, format="parquet")

            table = dataset.to_table(columns=columns, filter=filter_expr)
            return table.to_pandas()
        except (OSError, ValueError, TypeError) as exc:
            return None

    @staticmethod
    def _build_dataset_filter(filter_spec: FilterSpec):
        """Convert FilterSpec to a PyArrow Dataset expression."""
        try:
            import pyarrow.dataset as ds  # type: ignore
        except ImportError:
            return None

        field = ds.field(filter_spec.column)
        if filter_spec.op == FilterOp.EQ:
            return field == filter_spec.value
        if filter_spec.op == FilterOp.IN:
            values = _ensure_sequence(filter_spec.value)
            return field.isin(values)
        if filter_spec.op == FilterOp.BETWEEN:
            lower, upper = filter_spec.value
            return (field >= lower) & (field <= upper)
        return None

    def _read_dataset(
        self,
        table_name: str,
        dataset_cfg: DatasetOptions,
        columns: Optional[Iterable[str]],
        patient_ids_filter: Optional[FilterSpec],
    ) -> pd.DataFrame:
        """Read a table via explicit PyArrow Dataset configuration."""
        try:
            import pyarrow.dataset as ds  # type: ignore
        except ImportError as exc:  # pragma: no cover - optional dependency
            raise ImportError(
                "PyArrow is required for dataset-backed tables. "
                "Install pyarrow or remove the dataset configuration."
            ) from exc

        root = dataset_cfg.path or table_name
        root_path = Path(root)
        if not root_path.is_absolute():
            if self.base_path is None:
                raise ValueError(
                    f"Dataset path '{root}' for table '{table_name}' is relative, "
                    "but data source has no base_path."
                )
            root_path = self.base_path / root_path

        partitioning = dataset_cfg.partitioning or "hive"
        format_name = dataset_cfg.format or "parquet"
        options = dataset_cfg.options or {}

        try:
            dataset = ds.dataset(
                root_path,
                format=format_name,
                partitioning=partitioning,
                **options,
            )
        except (OSError, ValueError, TypeError) as exc:
            raise RuntimeError(
                f"Failed to initialise dataset for table '{table_name}' at {root_path}: {exc}"
            ) from exc

        filter_expr = self._build_dataset_filter(patient_ids_filter) if patient_ids_filter else None
        logger.info("ğŸ“ Using PyArrow dataset for %s (%s)", table_name, root_path)

        requested_columns = list(columns) if columns is not None else None
        effective_columns: Optional[List[str]] = None
        missing_columns: List[str] = []
        if requested_columns:
            available = set(dataset.schema.names)
            missing_columns = [col for col in requested_columns if col not in available]
            effective_columns = [col for col in requested_columns if col in available]
            if not effective_columns:
                effective_columns = None

        table = dataset.to_table(columns=effective_columns, filter=filter_expr)
        frame = table.to_pandas()

        if requested_columns:
            frame = frame.reindex(columns=requested_columns)
        if missing_columns:
            logger.warning(
                "Dataset %s missing columns %s; filled with NA values", table_name, ", ".join(missing_columns)
            )
        return frame
    


def load_table(
    data_source: ICUDataSource,
    table_name: str,
    *,
    columns: Optional[Iterable[str]] = None,
    filters: Optional[Iterable[FilterSpec]] = None,
) -> ICUTable:
    """Functional faÃ§ade delegating to :meth:`ICUDataSource.load_table`."""

    return data_source.load_table(table_name, columns=columns, filters=filters)


def _ensure_sequence(value: Any) -> List[Any]:
    """Normalise scalars/iterables for filter construction."""
    if value is None:
        return []
    if isinstance(value, Sequence) and not isinstance(value, (str, bytes)):
        return list(value)
    try:
        return list(value)
    except TypeError:
        return [value]


def _coerce_datetime(series: pd.Series) -> pd.Series:
    """Coerce a Series to datetime type, handling various edge cases.
    
    âš¡ æ€§èƒ½ä¼˜åŒ–: å‡å°‘é‡å¤çš„ç±»å‹æ£€æŸ¥å’Œè½¬æ¢
    """
    # âš¡ å¿«é€Ÿè·¯å¾„1: å·²ç»æ˜¯datetimeä¸”æ— æ—¶åŒº
    if pd.api.types.is_datetime64_any_dtype(series):
        if hasattr(series.dt, 'tz') and series.dt.tz is not None:
            return series.dt.tz_localize(None)
        return series
    
    # âš¡ å¿«é€Ÿè·¯å¾„2: æ•°å€¼å‹ä¸è½¬æ¢
    if pd.api.types.is_numeric_dtype(series):
        return series
    
    # âš¡ ä¼˜åŒ–: ä¸€æ¬¡æ€§æ£€æŸ¥å’Œreset index
    has_dup_idx = series.index.duplicated().any()
    if has_dup_idx:
        series = series.reset_index(drop=True)
    
    # âš¡ ä¼˜åŒ–: ç»Ÿä¸€ä½¿ç”¨coerceæ¨¡å¼ï¼Œé¿å…try-exceptå¼€é”€
    try:
        converted = pd.to_datetime(series, errors="coerce", utc=True)
        # åªåœ¨è½¬æ¢æˆåŠŸæ—¶ç§»é™¤æ—¶åŒº
        if converted is not None and hasattr(converted, 'dt'):
            return converted.dt.tz_localize(None)
        return series
    except Exception:
        # æç«¯æƒ…å†µï¼šè¿”å›åŸå€¼
        return series
