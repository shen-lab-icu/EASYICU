"""
Base classes and utilities for pyricu

This module provides unified base classes that consolidate common functionality
from across the codebase, reducing code duplication.
"""

from __future__ import annotations

import os
import logging
import threading
from pathlib import Path
from typing import List, Optional, Union, Dict, Any, Sequence
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from collections import defaultdict
import pandas as pd

from .datasource import ICUDataSource
from .concept import ConceptResolver, ConceptDictionary
from .resources import load_data_sources, load_dictionary
from .cache_manager import get_cache_manager
from .table import ICUTable

logger = logging.getLogger(__name__)

class BaseICULoader:
    """
    Unified base loader class that consolidates common initialization and loading logic.

    This class replaces the multiple initialization patterns found in:
    - quickstart.py (ICUQuickLoader)
    - api.py (load_concepts function)
    - api_enhanced.py (cached loading)
    - api_unified.py (UnifiedConceptLoader)
    """

    def __init__(
        self,
        data_path: Optional[Union[str, Path]] = None,
        database: Optional[str] = None,
        dict_path: Optional[Union[str, Path, List[Union[str, Path]]]] = None,
        use_sofa2: bool = False,
        verbose: bool = False,
    ):
        """Initialize the unified loader

        Args:
            data_path: Path to ICU data (auto-detected if None)
            database: Database type ('miiv', 'mimic', 'eicu', 'hirid', 'aumc')
            dict_path: Custom concept dictionary path(s)
            use_sofa2: Whether to load SOFA2 dictionary
            verbose: Enable verbose logging
        """
        self.verbose = verbose
        # æ£€æµ‹æ•°æ®åº“ç±»åž‹ - å¦‚æžœç”¨æˆ·æ²¡æœ‰æŒ‡å®šï¼Œå…ˆå°è¯•ä»Žè·¯å¾„æŽ¨æ–­
        self.database = self._detect_database(database, data_path)
        self.data_path = self._setup_data_path(data_path, self.database)
        self._dict_path = dict_path
        self._use_sofa2 = use_sofa2

        # Check and prepare data (convert CSV to Parquet if needed)
        self._ensure_data_ready()

        # Initialize data source
        self._init_datasource()

        # Initialize concept system
        self._init_concept_system(dict_path, use_sofa2)

        # Register caches with global cache manager
        self._register_caches()

        # Thread-local storage for per-worker concept resolvers
        self._thread_local_resolver = threading.local()

    def _detect_database(self, database: Optional[str], data_path: Optional[Union[str, Path]] = None) -> str:
        """Detect database type from data_path, environment or use default
        
        ä¼˜å…ˆçº§:
        1. ç”¨æˆ·æ˜¾å¼æŒ‡å®šçš„ database å‚æ•°
        2. ä»Ž data_path è·¯å¾„æŽ¨æ–­ï¼ˆæ£€æŸ¥è·¯å¾„åå’Œæ•°æ®æ–‡ä»¶ï¼‰
        3. çŽ¯å¢ƒå˜é‡
        4. é»˜è®¤å€¼ miiv
        """
        if database:
            return database
        
        # å°è¯•ä»Ž data_path æŽ¨æ–­æ•°æ®åº“ç±»åž‹
        if data_path:
            path = Path(data_path)
            path_str = str(path).lower()
            
            # 1. æ£€æŸ¥è·¯å¾„åç§°ä¸­æ˜¯å¦åŒ…å«æ•°æ®åº“æ ‡è¯†
            db_patterns = {
                'eicu': ['eicu', 'eicu-crd'],
                'aumc': ['aumc', 'amsterdam'],
                'hirid': ['hirid'],
                'miiv': ['miiv', 'mimiciv', 'mimic-iv', 'mimic_iv'],
                'mimic': ['mimic', 'mimic-iii', 'mimic_iii', 'mimiciii'],
            }
            for db_name, patterns in db_patterns.items():
                if any(p in path_str for p in patterns):
                    if self.verbose:
                        logger.info(f"Auto-detected database: {db_name} from path: {path}")
                    return db_name
            
            # 2. æ£€æŸ¥æ•°æ®æ–‡ä»¶æ¥æŽ¨æ–­æ•°æ®åº“ç±»åž‹
            if path.is_dir():
                marker_files = {
                    'eicu': ['patient.parquet', 'patient.csv', 'patient.csv.gz', 'vitalPeriodic.parquet'],
                    'aumc': ['numericitems', 'admissions.parquet'],
                    'miiv': ['chartevents', 'icustays.parquet'],
                    'hirid': ['general.parquet', 'observations'],
                }
                for db_name, markers in marker_files.items():
                    if any((path / m).exists() for m in markers):
                        # é¢å¤–ç¡®è®¤ï¼šé¿å…è¯¯åˆ¤
                        if db_name == 'eicu' and (path / 'patient.parquet').exists():
                            # ç¡®è®¤æ˜¯ eicu è€Œä¸æ˜¯å…¶ä»–æœ‰ patient è¡¨çš„æ•°æ®åº“
                            if not (path / 'chartevents').exists():
                                if self.verbose:
                                    logger.info(f"Auto-detected database: {db_name} from data files in: {path}")
                                return db_name
                        elif db_name != 'eicu':
                            if self.verbose:
                                logger.info(f"Auto-detected database: {db_name} from data files in: {path}")
                            return db_name

        # Check environment variables
        for db_name in ['miiv', 'mimic', 'eicu', 'hirid', 'aumc']:
            env_var = f'{db_name.upper()}_PATH'
            if os.getenv(env_var):
                if self.verbose:
                    logger.info(f"Auto-detected database: {db_name} from {env_var}")
                return db_name

        # Default
        if self.verbose:
            logger.info("Using default database: miiv")
        return 'miiv'

    def _setup_data_path(self, data_path: Optional[Union[str, Path]], database: str) -> Path:
        """Setup and validate data path
        
        æ™ºèƒ½å¤„ç†æ•°æ®è·¯å¾„ï¼š
        - å¦‚æžœç”¨æˆ·ä¼ å…¥å®Œæ•´çš„æ•°æ®åº“è·¯å¾„ï¼ˆåŒ…å«æ•°æ®æ–‡ä»¶ï¼‰ï¼Œç›´æŽ¥ä½¿ç”¨
        - å¦‚æžœç”¨æˆ·ä¼ å…¥çš„æ˜¯åŸºç¡€è·¯å¾„ï¼ˆå¦‚ /home/1_publicData/icu_databasesï¼‰ï¼Œè‡ªåŠ¨æŸ¥æ‰¾æ•°æ®åº“å­ç›®å½•
        """
        if data_path:
            user_path = Path(data_path)
            
            # æ£€æŸ¥ç”¨æˆ·è·¯å¾„æ˜¯å¦ç›´æŽ¥åŒ…å«æ•°æ®æ–‡ä»¶ï¼ˆå¦‚ admissions.parquet, numericitems/ ç­‰ï¼‰
            if user_path.is_dir():
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„æ•°æ®åº“ç›®å½•ï¼ˆåŒ…å«ç‰¹å¾æ–‡ä»¶ï¼‰
                # AUMC ç‰¹å¾æ–‡ä»¶: admissions.csv/parquet, numericitems/
                # MIIV ç‰¹å¾æ–‡ä»¶: admissions.csv/parquet, chartevents/
                # eICU ç‰¹å¾æ–‡ä»¶: patient.csv, vitalPeriodic.csv
                # MIMIC-III ç‰¹å¾æ–‡ä»¶: icustays.parquet, chartevents_bucket/
                # SICdb ç‰¹å¾æ–‡ä»¶: cases.parquet, data_float_h_bucket/
                marker_files = {
                    'aumc': ['admissions.csv', 'admissions.parquet', 'numericitems'],
                    'miiv': ['admissions.csv', 'admissions.parquet', 'chartevents'],
                    'eicu': ['patient.csv', 'patient.csv.gz', 'vitalPeriodic.csv'],
                    'hirid': ['general.csv', 'observations'],
                    'mimic': ['icustays.parquet', 'chartevents_bucket', 'labevents_bucket'],  # MIMIC-III
                    'mimic_demo': ['icustays.parquet', 'chartevents'],  # MIMIC-III demo
                    'sic': ['cases.parquet', 'data_float_h_bucket', 'laboratory_bucket'],  # SICdb
                }
                
                db_markers = marker_files.get(database, [])
                is_valid_db_dir = any((user_path / marker).exists() for marker in db_markers)
                
                if is_valid_db_dir:
                    if self.verbose:
                        logger.info(f"Using user-provided database path: {user_path}")
                    return user_path
                
                # å¦‚æžœä¸æ˜¯æœ‰æ•ˆçš„æ•°æ®åº“ç›®å½•ï¼Œå°è¯•æŸ¥æ‰¾å­ç›®å½•
                # AUMC ç‰¹æ®Šå¤„ç†ï¼šé€šå¸¸åœ¨ aumc/1.0.2/ å­ç›®å½•
                # MIIV ç‰¹æ®Šå¤„ç†ï¼šé€šå¸¸åœ¨ mimiciv/3.1/ å­ç›®å½•
                # eICU ç‰¹æ®Šå¤„ç†ï¼šé€šå¸¸åœ¨ eicu/2.0.1/ å­ç›®å½•
                # HiRID ç‰¹æ®Šå¤„ç†ï¼šé€šå¸¸åœ¨ hirid/1.1.1/ å­ç›®å½•
                # MIMIC-III ç‰¹æ®Šå¤„ç†ï¼šé€šå¸¸åœ¨ mimiciii/1.4/ å­ç›®å½•
                # SICdb ç‰¹æ®Šå¤„ç†ï¼šé€šå¸¸åœ¨ sicdb/1.0.6/ å­ç›®å½•
                # å…ˆå°è¯•ç²¾ç¡®ç‰ˆæœ¬åŒ¹é…ï¼Œå†å°è¯•é€šç”¨ç›®å½•
                possible_subpaths = [
                    user_path / database,  # /base/aumc
                    user_path / database / '1.0.2',  # /base/aumc/1.0.2 (AUMC)
                    user_path / database / '3.1',    # /base/miiv/3.1 (MIIV)
                    user_path / database / '2.0.1',  # /base/eicu/2.0.1 (eICU)
                    user_path / database / '2.0',    # /base/eicu/2.0 (eICU old)
                    user_path / database / '1.1.1',  # /base/hirid/1.1.1 (HiRID)
                    # æ”¯æŒ mimiciv å‘½åå˜ä½“
                    user_path / 'mimiciv' / '3.1',
                    # MIMIC-III æ”¯æŒ
                    user_path / 'mimiciii' / '1.4',
                    user_path / 'mimic' / '1.4',
                    # SICdb æ”¯æŒ
                    user_path / 'sicdb' / '1.0.6',
                    user_path / 'sic' / '1.0.6',
                ]
                
                # å¦‚æžœæ²¡æ‰¾åˆ°ï¼Œå°è¯•åŠ¨æ€æœç´¢å­ç›®å½•
                for subpath in possible_subpaths:
                    if subpath.is_dir():
                        is_valid = any((subpath / marker).exists() for marker in db_markers)
                        if is_valid:
                            if self.verbose:
                                logger.info(f"Auto-detected database path: {subpath} (from base: {user_path})")
                            return subpath
                
                # å›žé€€ï¼šè¿”å›žç”¨æˆ·è·¯å¾„ï¼ˆå¯èƒ½å¯¼è‡´åŽç»­é”™è¯¯ï¼Œä½†ä¿æŒå‘åŽå…¼å®¹ï¼‰
                if self.verbose:
                    logger.warning(f"Could not find valid {database} data in {user_path}, using as-is")
                return user_path
            
            return user_path

        # Check environment variables
        # 1. é¦–å…ˆæ£€æŸ¥æ•°æ®åº“ä¸“ç”¨çš„çŽ¯å¢ƒå˜é‡ï¼ˆå¦‚ MIMIC_PATHï¼‰
        env_var = f'{database.upper()}_PATH'
        path = os.getenv(env_var)
        if path:
            if self.verbose:
                logger.info(f"Using path from {env_var}: {path}")
            return Path(path)
        
        # 2. æ£€æŸ¥ RICU_DATA_PATH é€šç”¨çŽ¯å¢ƒå˜é‡ï¼ˆéœ€è¦ä¸Žæ•°æ®åº“ç›®å½•æ˜ å°„ï¼‰
        ricu_data_path = os.getenv('RICU_DATA_PATH')
        if ricu_data_path:
            base_path = Path(ricu_data_path)
            # æ•°æ®åº“åç§°åˆ°ç›®å½•åçš„æ˜ å°„
            db_dir_mapping = {
                'mimic': ['mimiciii/1.4', 'mimic/1.4', 'mimiciii'],
                'mimic_demo': ['mimic_demo', 'mimiciii_demo'],
                'miiv': ['mimiciv/3.1', 'miiv/3.1', 'mimiciv'],
                'eicu': ['eicu/2.0.1', 'eicu/2.0', 'eicu'],
                'eicu_demo': ['eicu_demo'],
                'aumc': ['aumc/1.0.2', 'aumc'],
                'hirid': ['hirid/1.1.1', 'hirid'],
                'sic': ['sicdb/1.0.6', 'sic/1.0.6', 'sicdb', 'sic'],
            }
            
            for subdir in db_dir_mapping.get(database, [database]):
                candidate = base_path / subdir
                if candidate.is_dir():
                    if self.verbose:
                        logger.info(f"Using path from RICU_DATA_PATH: {candidate}")
                    return candidate

        # Check production data paths from project_config
        try:
            from .project_config import get_data_path
            prod_path = get_data_path(source='production', database=database)
            if prod_path and prod_path.exists():
                if self.verbose:
                    logger.info(f"Using production data path: {prod_path}")
                return prod_path
        except Exception as e:
            if self.verbose:
                logger.debug(f"Could not get production path from project_config: {e}")

        # Check common paths
        common_paths = [
            Path.home() / 'data' / database,
            Path('/data') / database,
            Path('.') / 'data' / database,
        ]

        for path in common_paths:
            if path.exists():
                if self.verbose:
                    logger.info(f"Found existing path: {path}")
                return path

        # Return default path (may not exist)
        default_path = Path('./data') / database
        if self.verbose:
            logger.info(f"Using default path: {default_path}")
        return default_path

    def _ensure_data_ready(self):
        """Ensure data files are ready (convert CSV to Parquet if needed)
        
        This method checks if Parquet files exist for the database's tables.
        If only CSV/CSV.GZ files exist, a warning will be logged.
        Use DataConverter or CLI to convert files before loading.
        """
        try:
            from .data_converter import DataConverter
            
            converter = DataConverter(
                data_path=self.data_path,
                database=self.database,
                verbose=False  # Suppress verbose output for status check
            )
            
            # Check status without auto-converting
            is_ready, missing = converter.is_ready()
            
            if not is_ready:
                # Log warning about missing parquet files
                logger.warning(
                    f"âš ï¸ {len(missing)} CSV files need to be converted to Parquet for optimal performance.\n"
                    f"   Run: python -m pyricu.data_converter {self.data_path}\n"
                    f"   Or use: DataConverter('{self.data_path}').ensure_parquet_ready()"
                )
                if self.verbose:
                    for msg in missing[:5]:
                        logger.warning(f"   - {msg}")
                    if len(missing) > 5:
                        logger.warning(f"   ... and {len(missing) - 5} more")
            elif self.verbose:
                logger.info(f"âœ… All data files are ready in {self.data_path}")
            
        except ImportError:
            if self.verbose:
                logger.debug("data_converter module not available, skipping data preparation check")
        except Exception as e:
            # Don't fail initialization if data check fails
            if self.verbose:
                logger.warning(f"Data preparation check failed: {e}")

    def _init_datasource(self):
        """Initialize the data source"""
        try:
            registry = load_data_sources()
            config = registry.get(self.database)
            if not config:
                raise ValueError(f"Unknown database: {self.database}")

            self.datasource = ICUDataSource(config=config, base_path=self.data_path)

            if self.verbose:
                logger.info(f"Initialized datasource for {self.database}")

        except Exception as e:
            raise RuntimeError(f"Failed to initialize datasource: {e}")

    def _init_concept_system(self, dict_path: Optional[Union[str, Path, List[Union[str, Path]]]], use_sofa2: bool):
        """Initialize the concept system"""
        try:
            if dict_path is None:
                env_override = os.getenv("PYRICU_DICT_PATH") or os.getenv("PYRICU_DICT_DIR")
                if env_override:
                    dict_path = env_override

            dicts: List[ConceptDictionary] = [load_dictionary(include_sofa2=use_sofa2)]

            if dict_path is not None:
                if isinstance(dict_path, (list, tuple)):
                    sources = list(dict_path)
                else:
                    sources = [dict_path]

                for source in sources:
                    dicts.append(self._load_dict_source(source))

            # Create merged dictionary
            if len(dicts) == 1:
                self.concept_dict = dicts[0]
            else:
                # Merge multiple dictionaries
                merged = dicts[0].copy()
                for dict_obj in dicts[1:]:
                    merged.update(dict_obj)
                self.concept_dict = merged

            # Initialize resolver
            self.concept_resolver = ConceptResolver(
                dictionary=self.concept_dict
            )

            if self.verbose:
                concept_count = len(list(self.concept_dict.keys()))
                logger.info(f"Initialized concept system with {concept_count} concepts")

        except Exception as e:
            raise RuntimeError(f"Failed to initialize concept system: {e}")

    def _register_caches(self):
        """Register caches with global cache manager"""
        try:
            cache_manager = get_cache_manager()

            # Register data source cache
            if hasattr(self, 'datasource') and self.datasource:
                cache_manager.register_memory_cache(self.datasource)

            # Register concept resolver cache
            if hasattr(self, 'concept_resolver') and self.concept_resolver:
                cache_manager.register_memory_cache(self.concept_resolver)

            if self.verbose:
                logger.info("âœ… å·²æ³¨å†Œç¼“å­˜åˆ°å…¨å±€ç¼“å­˜ç®¡ç†å™¨")

        except Exception as e:
            if self.verbose:
                logger.warning(f"âš ï¸  ç¼“å­˜æ³¨å†Œå¤±è´¥: {e}")
            # ä¸å½±å“ä¸»è¦åŠŸèƒ½ï¼Œç»§ç»­è¿è¡Œ

    def clear_cache(self):
        """Clear all caches to free memory
        
        This is useful when processing data in batches to ensure each batch
        uses fresh data and doesn't accumulate memory from previous batches.
        """
        if hasattr(self, 'concept_resolver') and self.concept_resolver:
            self.concept_resolver.clear_table_cache(keep_concept_cache=False)
        
        # Also try to clear datasource cache if it has one
        if hasattr(self, 'datasource') and self.datasource:
            if hasattr(self.datasource, '_cache'):
                self.datasource._cache.clear()

    def _create_resolver_clone(self) -> ConceptResolver:
        """Create a fresh ConceptResolver sharing the same dictionary."""
        return ConceptResolver(dictionary=self.concept_dict)

    def _get_thread_resolver(self) -> ConceptResolver:
        """Lazily create per-thread concept resolvers for parallel batches."""
        if not hasattr(self, '_thread_local_resolver'):
            self._thread_local_resolver = threading.local()
        resolver = getattr(self._thread_local_resolver, 'resolver', None)
        if resolver is None:
            resolver = self._create_resolver_clone()
            self._thread_local_resolver.resolver = resolver
        return resolver

    def _limit_blas_threads(self) -> Dict[str, Optional[str]]:
        """Force single-threaded BLAS during Python-level threading."""
        env_vars = [
            "OMP_NUM_THREADS",
            "OPENBLAS_NUM_THREADS",
            "MKL_NUM_THREADS",
            "NUMEXPR_NUM_THREADS",
            "VECLIB_MAXIMUM_THREADS",
            "BLIS_NUM_THREADS",
        ]
        original: Dict[str, Optional[str]] = {}
        for var in env_vars:
            original[var] = os.environ.get(var)
            os.environ[var] = "1"
        return original

    def _restore_blas_threads(self, state: Dict[str, Optional[str]]) -> None:
        for var, value in state.items():
            if value is None:
                os.environ.pop(var, None)
            else:
                os.environ[var] = value

    def _resolve_parallel_workers(self, requested: Optional[int]) -> int:
        """Determine how many patient-chunk workers to spawn by default.
        
        âš¡ æ€§èƒ½ä¼˜åŒ–: ç”±äºŽPython GILå’Œé”ç«žäº‰ï¼Œå¤šçº¿ç¨‹åè€Œä¼šé™ä½Žæ€§èƒ½
        é»˜è®¤ä½¿ç”¨å•çº¿ç¨‹ï¼Œé™¤éžæ˜Žç¡®æŒ‡å®š
        """
        if isinstance(requested, int) and requested > 0:
            return requested

        env_value = os.getenv("PYRICU_PARALLEL_WORKERS")
        if env_value:
            try:
                env_workers = int(env_value)
                if env_workers > 0:
                    return env_workers
            except ValueError:
                logger.warning("Invalid PYRICU_PARALLEL_WORKERS=%s, ignoring", env_value)

        # âš¡ é»˜è®¤å•çº¿ç¨‹ä»¥é¿å…GILç«žäº‰å’Œé”å¼€é”€
        # ç”¨æˆ·å¯é€šè¿‡çŽ¯å¢ƒå˜é‡PYRICU_PARALLEL_WORKERSæˆ–å‚æ•°æ˜¾å¼å¯ç”¨å¹¶è¡Œ
        default_workers = 1
        
        return default_workers

    def _resolve_parallel_backend(self, backend: Optional[str]) -> str:
        """Select execution backend for patient chunk parallelism."""
        if backend:
            normalized = backend.strip().lower()
            if normalized in {"thread", "process"}:
                return normalized
            if normalized != "auto":
                logger.warning("Unknown parallel_backend '%s', falling back to auto", backend)

        env_backend = os.getenv("PYRICU_PARALLEL_BACKEND")
        if env_backend:
            normalized = env_backend.strip().lower()
            if normalized in {"thread", "process"}:
                return normalized

        if os.name == "nt":
            return "thread"
        return "process"

    def _load_dict_source(self, source: Union[str, Path, ConceptDictionary]) -> ConceptDictionary:
        """Load a dictionary from a custom source."""
        if isinstance(source, ConceptDictionary):
            return source

        if isinstance(source, (str, Path)):
            path = Path(str(source))
            if path.exists():
                if path.is_dir():
                    return load_dictionary(directories=[path])
                if path.is_file():
                    return ConceptDictionary.from_json(path)

        # Fallback to treating the string as a packaged resource name
        return load_dictionary(str(source))

    def load_concepts(
        self,
        concepts: Union[str, List[str]],
        patient_ids: Optional[List] = None,
        interval: Optional[Union[str, pd.Timedelta]] = '1h',  # ricué»˜è®¤: hours(1L)
        win_length: Optional[Union[str, pd.Timedelta]] = None,
        aggregate: Optional[Union[str, Dict]] = None,
        keep_components: bool = False,
        merge: bool = True,
        ricu_compatible: bool = True,  # é»˜è®¤å¯ç”¨ricu.Rå…¼å®¹æ¨¡å¼
        chunk_size: Optional[int] = None,
        progress: bool = False,
        parallel_workers: Optional[int] = None,
        concept_workers: Optional[int] = None,  # æ”¹ä¸ºOptionalï¼Œæ”¯æŒè‡ªåŠ¨æ£€æµ‹
        parallel_backend: str = "auto",
        **kwargs
    ) -> Union[pd.DataFrame, Dict[str, pd.DataFrame]]:
        """
        Load concept data using the unified interface.

        This method consolidates the loading logic from multiple API implementations.
        """
        try:
            kwargs = dict(kwargs)
            if isinstance(concepts, str):
                concepts = [concepts]

            if isinstance(interval, str):
                interval = pd.Timedelta(interval)
            if isinstance(win_length, str):
                win_length = pd.Timedelta(win_length)

            # ðŸš€ æ™ºèƒ½å¹¶è¡Œé…ç½®ï¼šå¦‚æžœæœªæŒ‡å®šï¼Œæ ¹æ®æ¦‚å¿µæ•°é‡è‡ªåŠ¨ä¼˜åŒ–
            effective_concept_workers = concept_workers
            if effective_concept_workers is None:
                num_concepts = len(concepts)
                if num_concepts >= 3:
                    cpu_count = os.cpu_count() or 4
                    effective_concept_workers = min(num_concepts, max(2, cpu_count // 2))
                elif num_concepts == 2:
                    effective_concept_workers = 2
                else:
                    effective_concept_workers = 1
            
            if self.verbose:
                logger.info(f"Loading {len(concepts)} concepts: {', '.join(concepts)}")
                if effective_concept_workers > 1:
                    logger.info(f"âš¡ Auto-optimized: concept_workers={effective_concept_workers}")

            batches = self._build_patient_batches(patient_ids, chunk_size)
            if batches:
                worker_count = self._resolve_parallel_workers(parallel_workers)
                return self._load_concepts_chunked(
                    concepts,
                    batches,
                    interval,
                    win_length,
                    aggregate,
                    keep_components,
                    merge,
                    ricu_compatible,
                    progress,
                    worker_count,
                    effective_concept_workers,
                    parallel_backend,
                    kwargs,
                )

            return self._load_concepts_once(
                concepts,
                patient_ids,
                interval,
                win_length,
                aggregate,
                keep_components,
                merge,
                ricu_compatible,
                effective_concept_workers,
                kwargs,
                preserve_cache=len(concepts) > 1,  # ðŸš€ å¤šæ¦‚å¿µæ—¶ä¿ç•™ç¼“å­˜ä»¥åŠ é€Ÿ
            )

        except Exception as e:
            raise RuntimeError(f"Failed to load concepts {concepts}: {e}")
        # ðŸš€ ä¼˜åŒ–ï¼šç§»é™¤finallyä¸­çš„å¼ºåˆ¶æ¸…é™¤ç¼“å­˜
        # _load_concepts_once å·²ç»æœ‰æ¡ä»¶åœ°ç®¡ç†ç¼“å­˜ï¼Œæ— éœ€åœ¨æ­¤å†æ¸…é™¤
        # è¿™æ ·æ‰¹é‡åŠ è½½å¤šä¸ªæ¦‚å¿µæ—¶å¯ä»¥å…±äº«è¡¨ç¼“å­˜ï¼Œå¤§å¹…æå‡æ€§èƒ½

    def _merge_concepts(self, results: Dict[str, pd.DataFrame], keep_components: bool) -> pd.DataFrame:
        """Merge multiple concept DataFrames"""
        if not results:
            return pd.DataFrame()

        # Start with first DataFrame
        merged_df = None
        id_cols = None

        for concept, df in results.items():
            if df.empty:
                continue

            # Identify ID columns from first non-empty DataFrame
            if id_cols is None:
                id_cols = [col for col in df.columns if col in ['stay_id', 'subject_id', 'patientunitstayid', 'admissionid', 'patientid']]
                if not id_cols:
                    id_cols = [df.columns[0]]  # Use first column as fallback
                merged_df = df.copy()
            else:
                # Merge on ID columns
                time_cols = [col for col in df.columns if 'time' in col.lower()]
                merge_on = list(id_cols)
                if time_cols:
                    merge_on.extend(time_cols)

                # Remove duplicate merge columns from df
                df_merge = df.drop(columns=[col for col in merge_on if col in df.columns and col != merge_on[0]])

                merged_df = pd.merge(
                    merged_df,
                    df_merge,
                    on=id_cols,
                    how='outer',
                    suffixes=('', f'_{concept}')
                )

        return merged_df if merged_df is not None else pd.DataFrame()

    def _load_concepts_once(
        self,
        concepts: List[str],
        patient_ids: Optional[Union[List, Dict]],
        interval: Optional[pd.Timedelta],
        win_length: Optional[pd.Timedelta],
        aggregate: Optional[Union[str, Dict]],
        keep_components: bool,
        merge: bool,
        ricu_compatible: bool,
        concept_workers: int,
        extra_kwargs: Dict[str, Any],
        preserve_cache: bool = False,
        resolver: Optional[ConceptResolver] = None,
        use_thread_resolver: bool = False,
    ) -> Union[pd.DataFrame, Dict[str, pd.DataFrame]]:
        params = dict(extra_kwargs)
        verbose_flag = params.pop("verbose", self.verbose)
        
        # ðŸš€ ä¼˜åŒ–ï¼šå½“åŠ è½½å¤šä¸ªç›¸å…³æ¦‚å¿µæ—¶ä¿ç•™ç¼“å­˜ï¼ˆå¦‚SOFAçš„å¤šä¸ªå­æ¦‚å¿µï¼‰
        should_preserve_cache = preserve_cache or len(concepts) > 1
        
        resolver_obj: ConceptResolver
        if resolver is not None:
            resolver_obj = resolver
        elif use_thread_resolver:
            resolver_obj = self._get_thread_resolver()
        else:
            resolver_obj = self.concept_resolver

        try:
            result = resolver_obj.load_concepts(
                concepts,
                self.datasource,
                merge=merge,
                patient_ids=patient_ids,
                interval=interval,
                win_length=win_length,
                aggregate=aggregate,
                keep_components=keep_components,
                ricu_compatible=ricu_compatible,
                concept_workers=concept_workers,
                verbose=verbose_flag,
                **params,
            )
            # Compatibility fix: when running in ricu_compatible mode, R ricu
            # applies sed_impute='max' for total GCS (tgcs) when ett_gcs == TRUE.
            # Some tgcs are computed via sum_components and therefore miss the
            # sed_impute adjustment. Apply the adjustment here on the merged
            # DataFrame so tgcs matches R ricu semantics when both columns
            # are present.
            try:
                if ricu_compatible and isinstance(result, pd.DataFrame):
                    if 'tgcs' in result.columns and 'ett_gcs' in result.columns:
                        mask = result['ett_gcs'].where(result['ett_gcs'].notna(), False).astype(bool)
                        if mask.any():
                            result.loc[mask, 'tgcs'] = 15.0
            except Exception:
                # Do not fail loading if this adjustment fails
                pass
        finally:
            # ðŸš€ ä¼˜åŒ–ï¼šåªæ¸…é™¤è¡¨ç¼“å­˜ï¼Œä¿ç•™æ¦‚å¿µæ•°æ®ç¼“å­˜ä»¥åŠ é€Ÿæ‰¹é‡åŠ è½½
            # è¡¨ç¼“å­˜å¯èƒ½å¾ˆå¤§ï¼ˆåŽŸå§‹æ•°æ®ï¼‰ï¼Œä½†æ¦‚å¿µç¼“å­˜è¾ƒå°ï¼ˆèšåˆåŽçš„æ•°æ®ï¼‰
            # è¿™å…è®¸åœ¨è¿žç»­çš„ load_concepts è°ƒç”¨ä¹‹é—´å…±äº«æ¦‚å¿µç¼“å­˜ï¼ˆå¦‚ sofa å’Œ sofa2 å…±äº« fio2, plt ç­‰ï¼‰
            resolver_obj.clear_table_cache(keep_concept_cache=True)

        if isinstance(result, dict):
            if not merge:
                return result
            if self.verbose:
                logger.info("Merging concept results")
            return self._merge_concepts(result, keep_components)
        return result

    def _load_concepts_chunked(
        self,
        concepts: List[str],
        batches: List[Union[List, Dict]],
        interval: Optional[pd.Timedelta],
        win_length: Optional[pd.Timedelta],
        aggregate: Optional[Union[str, Dict]],
        keep_components: bool,
        merge: bool,
        ricu_compatible: bool,
        progress: bool,
        parallel_workers: int,
        concept_workers: int,
        parallel_backend: str,
        extra_kwargs: Dict[str, Any],
    ) -> Union[pd.DataFrame, Dict[str, pd.DataFrame]]:
        aggregated_frames: List[pd.DataFrame] = []
        aggregated_dict: Dict[str, List[pd.DataFrame]] = defaultdict(list)
        aggregated_meta: Dict[str, Dict[str, Any]] = {}
        total_batches = len(batches)
        if parallel_workers is None or parallel_workers <= 0:
            parallel_workers = 1
        if total_batches:
            parallel_workers = max(1, min(parallel_workers, total_batches))
        backend = self._resolve_parallel_backend(parallel_backend)
        
        # ðŸš€ ä¼˜åŒ–ï¼šåœ¨å¤„ç†å‰é¢„åŠ è½½å¤§è¡¨ï¼ˆé¿å…é‡å¤I/Oï¼‰
        if backend == "thread" and parallel_workers > 1 and total_batches > 1:
            logger.info(
                f"ðŸš€ å¯ç”¨å¤šçº¿ç¨‹ä¼˜åŒ–({backend}): {parallel_workers}çº¿ç¨‹å¤„ç†{total_batches}æ‰¹æ¬¡"
            )
            
            # æ™ºèƒ½é¢„åŠ è½½ï¼šåªåœ¨æ‚£è€…æ•°é‡è¶³å¤Ÿæ—¶æ‰é¢„åŠ è½½
            all_patient_ids = []
            for batch in batches:
                if isinstance(batch, dict):
                    all_patient_ids.extend(batch.get('stay_id', []))
                else:
                    all_patient_ids.extend(batch)

            # âŒ ä¸´æ—¶ç¦ç”¨é¢„åŠ è½½ï¼šé¢„åŠ è½½é€»è¾‘æœ‰bugï¼Œä¼šåœ¨load_tableæ—¶æ— é™é€’å½’
            # TODO: ä¿®å¤é¢„åŠ è½½é€»è¾‘åŽé‡æ–°å¯ç”¨
            logger.info(f"âš¡ æ•°æ®è§„æ¨¡({len(all_patient_ids)}æ‚£è€…)ï¼Œé¢„åŠ è½½åŠŸèƒ½æš‚æ—¶ç¦ç”¨")
            # if len(all_patient_ids) >= 1000:
            #     preload_tables = ['chartevents', 'labevents', 'outputevents', 'procedureevents']
            #     logger.
            #     self.datasource.preload_tables(preload_tables, patient_ids=all_patient_ids)
            # else:
            #     logger.info(f"âš¡ å°è§„æ¨¡æ•°æ®({len(all_patient_ids)}æ‚£è€…)ï¼Œè·³è¿‡é¢„åŠ è½½ä»¥æå‡æ€§èƒ½")
        elif backend == "process" and parallel_workers > 1 and total_batches > 1:
            logger.info(
                f"ðŸš€ å¯ç”¨å¤šè¿›ç¨‹ä¼˜åŒ–: {parallel_workers}è¿›ç¨‹å¤„ç†{total_batches}æ‰¹æ¬¡"
            )

            # ä¸ºå¤šè¿›ç¨‹æ¨¡å¼ä¹Ÿé¢„åŠ è½½å¤§è¡¨ï¼Œé¿å…é‡å¤I/O
            all_patient_ids = []
            for batch in batches:
                if isinstance(batch, dict):
                    all_patient_ids.extend(batch.get('stay_id', []))
                else:
                    all_patient_ids.extend(batch)

            # âŒ ä¸´æ—¶ç¦ç”¨é¢„åŠ è½½
            logger.info(f"âš¡ å¤šè¿›ç¨‹æ¨¡å¼ï¼Œé¢„åŠ è½½åŠŸèƒ½æš‚æ—¶ç¦ç”¨")
            # preload_tables = ['chartevents', 'labevents', 'outputevents', 'procedureevents']
            # logger.info(f"ðŸ“¦ å¤šè¿›ç¨‹æ¨¡å¼é¢„åŠ è½½å¤§è¡¨: {', '.join(preload_tables)}")
            # self.datasource.preload_tables(preload_tables, patient_ids=all_patient_ids)

        def _capture_meta(table: ICUTable) -> Dict[str, Any]:
            return {
                "id_columns": list(table.id_columns),
                "index_column": table.index_column,
                "value_column": table.value_column,
                "unit_column": table.unit_column,
                "time_columns": list(table.time_columns),
            }

        def _accumulate(chunk_result):
            if isinstance(chunk_result, dict):
                for name, frame in chunk_result.items():
                    if frame is None:
                        continue
                    meta = None
                    if isinstance(frame, ICUTable):
                        meta = _capture_meta(frame)
                        frame_data = frame.data
                    else:
                        frame_data = frame
                    if frame_data is not None and not getattr(frame_data, "empty", False):
                        aggregated_dict[name].append(frame_data)
                        if meta and name not in aggregated_meta:
                            aggregated_meta[name] = meta
            else:
                if chunk_result is not None and not getattr(chunk_result, "empty", False):
                    aggregated_frames.append(chunk_result)

        if parallel_workers and parallel_workers > 1:
            blas_state: Optional[Dict[str, Optional[str]]] = None
            if backend == "thread":
                blas_state = self._limit_blas_threads()
            try:
                if backend == "process":
                    worker_payload = {
                        "database": self.database,
                        "data_path": str(self.data_path) if self.data_path is not None else None,
                        "dict_path": self._dict_path,
                        "use_sofa2": getattr(self, "_use_sofa2", False),
                        "datasource_config": self.datasource.config,
                        "concept_dict": self.concept_dict,
                        "verbose": self.verbose,
                    }
                    with ProcessPoolExecutor(
                        max_workers=parallel_workers,
                        initializer=_init_parallel_chunk_worker,
                        initargs=(worker_payload,)
                    ) as executor:
                        future_map = {
                            executor.submit(
                                _process_chunk_task,
                                (
                                    concepts,
                                    batch_ids,
                                    interval,
                                    win_length,
                                    aggregate,
                                    keep_components,
                                    merge,
                                    ricu_compatible,
                                    concept_workers,
                                    extra_kwargs,
                                ),
                            ): idx
                            for idx, batch_ids in enumerate(batches, start=1)
                        }
                        for future in as_completed(future_map):
                            idx = future_map[future]
                            chunk_result = future.result()
                            _accumulate(chunk_result)
                            if progress:
                                pct = (idx / total_batches) * 100.0
                                logger.info(
                                    "Chunked load %s: %d/%d (%.1f%%)",
                                    ", ".join(concepts),
                                    idx,
                                    total_batches,
                                    pct,
                                )
                else:
                    executor_params = dict(max_workers=parallel_workers)
                    with ThreadPoolExecutor(**executor_params) as executor:
                        future_map = {
                            executor.submit(
                                self._load_concepts_once,
                                concepts,
                                batch_ids,
                                interval,
                                win_length,
                                aggregate,
                                keep_components,
                                merge,
                                ricu_compatible,
                                concept_workers,
                                extra_kwargs,
                                True,
                                None,
                                True,
                            ): idx
                            for idx, batch_ids in enumerate(batches, start=1)
                        }
                        for future in as_completed(future_map):
                            idx = future_map[future]
                            chunk_result = future.result()
                            _accumulate(chunk_result)
                            if progress:
                                pct = (idx / total_batches) * 100.0
                                logger.info(
                                    "Chunked load %s: %d/%d (%.1f%%)",
                                    ", ".join(concepts),
                                    idx,
                                    total_batches,
                                    pct,
                                )
            finally:
                if blas_state is not None:
                    self._restore_blas_threads(blas_state)
        else:
            for idx, batch_ids in enumerate(batches, start=1):
                chunk_result = self._load_concepts_once(
                    concepts,
                    batch_ids,
                    interval,
                    win_length,
                    aggregate,
                    keep_components,
                    merge,
                    ricu_compatible,
                    concept_workers,
                    extra_kwargs,
                    preserve_cache=True,
                    resolver=self.concept_resolver,
                )
                _accumulate(chunk_result)
                if progress:
                    pct = (idx / total_batches) * 100.0
                    logger.info(
                        "Chunked load %s: %d/%d (%.1f%%)",
                        ", ".join(concepts),
                        idx,
                        total_batches,
                        pct,
                    )

        self.concept_resolver.clear_table_cache()

        if aggregated_dict:
            combined: Dict[str, Any] = {}
            for name, frames in aggregated_dict.items():
                combined_frame = (
                    pd.concat(frames, ignore_index=True)
                    if len(frames) > 1
                    else frames[0]
                )
                if not merge and name in aggregated_meta:
                    meta = aggregated_meta[name]
                    combined[name] = ICUTable(
                        data=combined_frame,
                        id_columns=meta.get("id_columns") or [],
                        index_column=meta.get("index_column"),
                        value_column=meta.get("value_column"),
                        unit_column=meta.get("unit_column"),
                        time_columns=meta.get("time_columns") or [],
                    )
                else:
                    combined[name] = combined_frame
            if merge:
                return self._merge_concepts(combined, keep_components)
            return combined

        if aggregated_frames:
            return (
                pd.concat(aggregated_frames, ignore_index=True)
                if len(aggregated_frames) > 1
                else aggregated_frames[0]
            )

        return pd.DataFrame()

    def _build_patient_batches(
        self,
        patient_ids: Optional[Union[List, Dict]],
        chunk_size: Optional[int],
    ) -> Optional[List[Union[List, Dict]]]:
        if not chunk_size or chunk_size <= 0 or patient_ids is None:
            return None

        if isinstance(patient_ids, dict):
            if len(patient_ids) != 1:
                return None
            key, values = next(iter(patient_ids.items()))
            seq = self._normalize_patient_ids(values)
            if seq is None or len(seq) <= chunk_size:
                return None
            return [
                {key: seq[i : i + chunk_size]}
                for i in range(0, len(seq), chunk_size)
            ]

        if isinstance(patient_ids, Sequence) and not isinstance(patient_ids, (str, bytes)):
            seq = self._normalize_patient_ids(patient_ids)
            if seq is None or len(seq) <= chunk_size:
                return None
            return [seq[i : i + chunk_size] for i in range(0, len(seq), chunk_size)]

        return None

    @staticmethod
    def _normalize_patient_ids(values: Union[Sequence, pd.Series]) -> Optional[List]:
        if values is None:
            return None
        try:
            seq = list(dict.fromkeys(values))
        except TypeError:
            seq = list(values)
        return seq

def get_default_data_path(database: str) -> Optional[Path]:
    """Get default data path for database (convenience function)"""
    loader = BaseICULoader(database=database, verbose=False)
    return loader.data_path

def detect_database_type() -> str:
    """Auto-detect database type from environment (convenience function)"""
    loader = BaseICULoader(verbose=False)
    return loader.database

_PROCESS_WORKER_LOADER: Optional[BaseICULoader] = None

def _init_parallel_chunk_worker(payload: Dict[str, Any]) -> None:
    """Initializer for process-based chunk workers."""
    global _PROCESS_WORKER_LOADER
    loader = BaseICULoader.__new__(BaseICULoader)
    loader.verbose = payload.get("verbose", False)
    loader.database = payload.get("database")
    data_path = payload.get("data_path")
    loader.data_path = Path(data_path) if data_path else None
    loader._dict_path = payload.get("dict_path")
    loader._use_sofa2 = payload.get("use_sofa2", False)
    loader.datasource = ICUDataSource(
        config=payload["datasource_config"],
        base_path=loader.data_path,
    )
    loader.concept_dict = payload["concept_dict"]
    loader.concept_resolver = ConceptResolver(dictionary=loader.concept_dict)
    loader._thread_local_resolver = threading.local()
    _PROCESS_WORKER_LOADER = loader

def _process_chunk_task(args: tuple) -> Union[pd.DataFrame, Dict[str, pd.DataFrame]]:
    """Execute a patient chunk inside a worker process."""
    if _PROCESS_WORKER_LOADER is None:
        raise RuntimeError("Chunk worker not initialized")

    (
        concepts,
        batch_ids,
        interval,
        win_length,
        aggregate,
        keep_components,
        merge,
        ricu_compatible,
        concept_workers,
        extra_kwargs,
    ) = args

    return _PROCESS_WORKER_LOADER._load_concepts_once(
        concepts,
        batch_ids,
        interval,
        win_length,
        aggregate,
        keep_components,
        merge,
        ricu_compatible,
        concept_workers,
        extra_kwargs,
        preserve_cache=True,
    )
