"""
åˆ†æ¡¶è½¬æ¢å™¨ - å°†å¤§è¡¨æŒ‰å˜é‡IDåˆ†æ¡¶å­˜å‚¨

æ ¸å¿ƒæ€æƒ³ï¼š
1. hash(itemid) % num_buckets å°†æ•°æ®åˆ†åˆ°å›ºå®šæ•°é‡çš„æ¡¶
2. æ¯ä¸ªæ¡¶å†…æ•°æ®æŒ‰itemidæ’åºï¼Œåˆ©ç”¨Parquet Row Groupç»Ÿè®¡ä¿¡æ¯å®ç°è°“è¯ä¸‹æ¨
3. è¯»å–æ—¶æ ¹æ®ç›®æ ‡itemidè®¡ç®—æ¡¶å·ï¼Œåªæ‰«æç›¸å…³æ¡¶

ç®—æ³•ä¼˜åŒ–ï¼š
- ä½¿ç”¨DuckDBè¿›è¡Œè½¬æ¢ï¼Œå……åˆ†åˆ©ç”¨å¤šæ ¸å’Œå‘é‡åŒ–æ‰§è¡Œ
- 100ä¸ªæ¡¶æ˜¯æœ€ä½³å¹³è¡¡ç‚¹ï¼šæ¯ä¸ªæ¡¶çº¦800MBï¼ˆå¯¹äº80GBçš„è¡¨ï¼‰
- Row Groupå¤§å°100,000è¡Œï¼Œä¾¿äºç»†ç²’åº¦è°“è¯ä¸‹æ¨
- write_statistics=true ç¡®ä¿Row Groupç»Ÿè®¡ä¿¡æ¯ç”¨äºè°“è¯ä¸‹æ¨

16GBå†…å­˜ä¼˜åŒ–ï¼š
- è½¬æ¢æ—¶è®¾ç½®memory_limit='10GB'ï¼Œé¢„ç•™ç³»ç»Ÿå†…å­˜
- æŒ‡å®štemp_directoryåˆ°é«˜é€ŸSSDï¼Œé¿å…å†…å­˜æº¢å‡º
- è¯»å–æ—¶åˆ—æŠ•å½± + è°“è¯ä¸‹æ¨ï¼Œæœ€å°åŒ–å†…å­˜å ç”¨
"""

import logging
import os
from pathlib import Path
from typing import Optional, Set, List, Callable
from dataclasses import dataclass, field
import time

logger = logging.getLogger(__name__)


@dataclass
class BucketConfig:
    """åˆ†æ¡¶é…ç½®"""
    num_buckets: int = 100  # æ¡¶æ•°é‡
    partition_col: str = 'itemid'  # åˆ†æ¡¶åˆ—
    row_group_size: int = 1_000_000  # Row Groupå¤§å°ï¼Œ1Mè¡Œæœ€ä¼˜å¹³è¡¡
    compression: str = 'zstd'  # zstdå‹ç¼©ç‡æ›´é«˜ï¼Œé€Ÿåº¦æ¥è¿‘snappy
    memory_limit: str = '12GB'  # å……åˆ†åˆ©ç”¨å†…å­˜
    threads: int = 0  # 0=è‡ªåŠ¨æ£€æµ‹CPUæ ¸å¿ƒæ•°
    temp_directory: Optional[str] = None  # ä¸´æ—¶æ–‡ä»¶ç›®å½•ï¼Œå»ºè®®SSD
    skip_sorting: bool = True  # è·³è¿‡æ’åºï¼Œå¤§å¹…åŠ é€Ÿ


@dataclass 
class ConversionResult:
    """è½¬æ¢ç»“æœ"""
    success: bool
    num_buckets: int
    total_rows: int
    total_size_bytes: int
    elapsed_seconds: float
    output_dir: Optional[Path] = None
    error: Optional[str] = None


def _duckdb_hash(itemid: int, num_buckets: int = 100) -> int:
    """
    ä½¿ç”¨DuckDBçš„hashå‡½æ•°è®¡ç®—æ¡¶ID
    
    æ³¨æ„: Pythonçš„hash()å’ŒDuckDBçš„hash()ç»“æœä¸åŒï¼
    è½¬æ¢æ—¶ä½¿ç”¨DuckDBï¼Œè¯»å–æ—¶ä¹Ÿå¿…é¡»ä½¿ç”¨DuckDBçš„hashæ¥å®šä½æ¡¶ã€‚
    """
    import duckdb
    conn = duckdb.connect()
    result = conn.execute(f"SELECT hash({itemid}) % {num_buckets}").fetchone()[0]
    conn.close()
    return result


def _duckdb_hash_batch(itemids: Set[int], num_buckets: int = 100) -> Set[int]:
    """
    æ‰¹é‡è®¡ç®—DuckDB hashï¼Œè¿”å›ç›®æ ‡æ¡¶IDé›†åˆ
    """
    import duckdb
    conn = duckdb.connect()
    # ä½¿ç”¨ UNNEST æ‰¹é‡è®¡ç®—
    itemid_list = list(itemids)
    conn.execute("CREATE TEMP TABLE items AS SELECT UNNEST(?) as itemid", [itemid_list])
    result = conn.execute(f"SELECT DISTINCT hash(itemid) % {num_buckets} FROM items").fetchall()
    conn.close()
    return {row[0] for row in result}


def convert_to_buckets(
    source_path: Path,
    output_dir: Path,
    config: BucketConfig = BucketConfig(),
    progress_callback: Optional[Callable[[str], None]] = None,
    overwrite: bool = False
) -> ConversionResult:
    """
    å°†å¤§è¡¨è½¬æ¢ä¸ºåˆ†æ¡¶Parquetæ ¼å¼
    
    Args:
        source_path: æºæ–‡ä»¶è·¯å¾„ï¼ˆCSVæˆ–Parquetï¼‰
        output_dir: è¾“å‡ºç›®å½•
        config: åˆ†æ¡¶é…ç½®
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        overwrite: æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„ç›®å½•
    
    Returns:
        ConversionResult: è½¬æ¢ç»“æœ
    """
    import duckdb
    import shutil
    
    def log(msg: str):
        logger.info(msg)
        if progress_callback:
            progress_callback(msg)
        else:
            print(msg)
    
    start_time = time.time()
    
    # æ£€æŸ¥æºæ–‡ä»¶
    source_path = Path(source_path)
    if not source_path.exists():
        return ConversionResult(
            success=False, num_buckets=0, total_rows=0,
            total_size_bytes=0, elapsed_seconds=0,
            error=f"æºæ–‡ä»¶ä¸å­˜åœ¨: {source_path}"
        )
    
    # å‡†å¤‡è¾“å‡ºç›®å½•
    output_dir = Path(output_dir)
    if output_dir.exists():
        if overwrite:
            log(f"åˆ é™¤å·²å­˜åœ¨çš„è¾“å‡ºç›®å½•: {output_dir}")
            shutil.rmtree(output_dir)
        else:
            return ConversionResult(
                success=False, num_buckets=0, total_rows=0,
                total_size_bytes=0, elapsed_seconds=0,
                error=f"è¾“å‡ºç›®å½•å·²å­˜åœ¨: {output_dir}"
            )
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        log(f"å¼€å§‹è½¬æ¢: {source_path.name}")
        log(f"åˆ†æ¡¶æ•°: {config.num_buckets}, åˆ†æ¡¶åˆ—: {config.partition_col}")
        log(f"å†…å­˜é™åˆ¶: {config.memory_limit}, ä¸´æ—¶ç›®å½•: {config.temp_directory or 'é»˜è®¤'}")
        
        conn = duckdb.connect()
        # å¹¶è¡Œçº¿ç¨‹æ•°ï¼š0=è‡ªåŠ¨æ£€æµ‹CPUæ ¸å¿ƒæ•°
        if config.threads > 0:
            conn.execute(f"SET threads={config.threads}")
        # å†…å­˜é™åˆ¶ï¼šé˜²æ­¢OOM
        conn.execute(f"SET memory_limit='{config.memory_limit}'")
        # ç¦ç”¨ä¿åºä»¥å¯ç”¨å¹¶è¡Œå†™å…¥
        conn.execute("SET preserve_insertion_order=false")
        # å¯ç”¨è¿›åº¦æ¡
        conn.execute("SET enable_progress_bar=true")
        
        # è®¾ç½®ä¸´æ—¶ç›®å½•ï¼šå»ºè®®åœ¨é«˜é€ŸSSDä¸Šï¼Œå¤„ç†80GBæ’åºçš„ç£ç›˜æº¢å‡º
        if config.temp_directory:
            os.makedirs(config.temp_directory, exist_ok=True)
            conn.execute(f"SET temp_directory='{config.temp_directory}'")
            log(f"ä¸´æ—¶ç›®å½•è®¾ç½®ä¸º: {config.temp_directory}")
        
        # ç¡®å®šè¯»å–æ–¹å¼
        # ğŸ”§ FIX: AUMCç­‰æ•°æ®åŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼Œéœ€è¦æ›´å¼ºçš„å®¹é”™å¤„ç†
        # - null_padding=true: å¤„ç†åˆ—æ•°ä¸ä¸€è‡´çš„è¡Œ
        # - ignore_errors=true: è·³è¿‡æ— æ³•è§£æçš„è¡Œ
        # - all_varchar=false: ä¿æŒè‡ªåŠ¨ç±»å‹æ¨æ–­ï¼ˆéœ€è¦itemidä¸ºæ•´æ•°ï¼‰
        # - sample_size=-1: æ‰«æå…¨éƒ¨æ•°æ®ä»¥ç¡®å®šschema
        source_name = source_path.name.lower()
        if source_name.endswith('.csv.gz') or source_name.endswith('.csv'):
            # DuckDB è‡ªåŠ¨å¤„ç† .gz å‹ç¼©
            read_expr = f"read_csv_auto('{source_path}', sample_size=-1, ignore_errors=true, null_padding=true)"
            log(f"æºæ–‡ä»¶ç±»å‹: CSV{'ï¼ˆgzipå‹ç¼©ï¼‰' if source_name.endswith('.gz') else ''}")
        elif source_name.endswith('.parquet'):
            read_expr = f"read_parquet('{source_path}')"
            log(f"æºæ–‡ä»¶ç±»å‹: Parquet")
        else:
            return ConversionResult(
                success=False, num_buckets=0, total_rows=0,
                total_size_bytes=0, elapsed_seconds=0,
                error=f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {source_path.suffix}ï¼Œä»…æ”¯æŒ .csv, .csv.gz, .parquet"
            )
        
        # ä½¿ç”¨ DuckDB COPY + PARTITION_BY å®ç°é«˜æ•ˆåˆ†æ¡¶
        # æ³¨æ„ï¼šæ’åºæ˜¯æœ€è€—æ—¶çš„æ“ä½œï¼Œå¯é€‰æ‹©è·³è¿‡
        if config.skip_sorting:
            log("æ‰§è¡Œåˆ†æ¡¶è½¬æ¢ (æ— æ’åºï¼Œæœ€å¿«æ¨¡å¼)...")
            sql = f"""
                COPY (
                    SELECT *,
                           hash({config.partition_col}) % {config.num_buckets} as bucket_id
                    FROM {read_expr}
                )
                TO '{output_dir}'
                (FORMAT PARQUET,
                 PARTITION_BY (bucket_id),
                 COMPRESSION {config.compression.upper()},
                 ROW_GROUP_SIZE {config.row_group_size},
                 OVERWRITE_OR_IGNORE)
            """
        else:
            log("æ‰§è¡Œåˆ†æ¡¶è½¬æ¢ (æ’åº + åˆ†æ¡¶)...")
            sql = f"""
                COPY (
                    SELECT *,
                           hash({config.partition_col}) % {config.num_buckets} as bucket_id
                    FROM {read_expr}
                    ORDER BY {config.partition_col}
                )
                TO '{output_dir}'
                (FORMAT PARQUET,
                 PARTITION_BY (bucket_id),
                 COMPRESSION {config.compression.upper()},
                 ROW_GROUP_SIZE {config.row_group_size},
                 OVERWRITE_OR_IGNORE)
            """
        
        conn.execute(sql)
        
        # ç»Ÿè®¡ç»“æœ
        elapsed = time.time() - start_time
        
        # è®¡ç®—æ€»è¡Œæ•°
        row_count = conn.execute(f"SELECT COUNT(*) FROM {read_expr}").fetchone()[0]
        
        # è®¡ç®—æ€»å¤§å°
        total_size = sum(f.stat().st_size for f in output_dir.rglob('*.parquet'))
        actual_buckets = len([d for d in output_dir.iterdir() if d.is_dir()])
        
        conn.close()
        
        log(f"è½¬æ¢å®Œæˆ! è€—æ—¶: {elapsed:.1f}ç§’")
        log(f"æ€»è¡Œæ•°: {row_count:,}")
        log(f"åˆ†æ¡¶æ•°: {actual_buckets}")
        log(f"æ€»å¤§å°: {total_size / 1024**3:.2f} GB")
        log(f"å¹³å‡æ¡¶å¤§å°: {total_size / max(actual_buckets, 1) / 1024**2:.1f} MB")
        
        return ConversionResult(
            success=True,
            num_buckets=actual_buckets,
            total_rows=row_count,
            total_size_bytes=total_size,
            elapsed_seconds=elapsed,
            output_dir=output_dir
        )
        
    except Exception as e:
        elapsed = time.time() - start_time
        logger.exception("è½¬æ¢å¤±è´¥")
        return ConversionResult(
            success=False, num_buckets=0, total_rows=0,
            total_size_bytes=0, elapsed_seconds=elapsed,
            error=str(e)
        )


def read_from_buckets(
    bucket_dir: Path,
    itemids: Optional[Set[int]] = None,
    columns: Optional[List[str]] = None,
    partition_col: str = 'itemid',
    num_buckets: int = 100,
    explain: bool = False
):
    """
    ä»åˆ†æ¡¶ç›®å½•é«˜æ•ˆè¯»å–æ•°æ®
    
    æ ¸å¿ƒä¼˜åŒ–:
    1. æ ¹æ®itemidè®¡ç®—ç›®æ ‡æ¡¶ï¼Œåªæ‰«æç›¸å…³æ¡¶ï¼ˆè·³è¿‡99%æ— å…³æ•°æ®ï¼‰
    2. ä½¿ç”¨Polars Lazy APIå®ç°è°“è¯ä¸‹æ¨ï¼ˆåˆ©ç”¨Row Groupç»Ÿè®¡ä¿¡æ¯ï¼‰
    3. åˆ—æŠ•å½±åªè¯»å–éœ€è¦çš„åˆ—ï¼ˆParquetåˆ—å¼å­˜å‚¨ä¼˜åŠ¿ï¼‰
    
    16GBå†…å­˜å®‰å…¨:
    - åªåŠ è½½éœ€è¦çš„æ¡¶ + éœ€è¦çš„åˆ— + ç¬¦åˆæ¡ä»¶çš„è¡Œ
    - å³ä½¿æ€»æ•°æ®80GBï¼Œå®é™…å†…å­˜å ç”¨é€šå¸¸ < 1GB
    
    Args:
        bucket_dir: åˆ†æ¡¶ç›®å½•
        itemids: è¦è¯»å–çš„itemidé›†åˆï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
        columns: è¦è¯»å–çš„åˆ—ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼Œä½†æ’é™¤bucket_idï¼‰
        partition_col: åˆ†æ¡¶åˆ—å
        num_buckets: æ¡¶æ•°é‡
        explain: æ˜¯å¦æ‰“å°æŸ¥è¯¢è®¡åˆ’ï¼ˆç”¨äºéªŒè¯è°“è¯ä¸‹æ¨ç”Ÿæ•ˆï¼‰
    
    Returns:
        polars.DataFrame
    """
    import polars as pl
    
    bucket_dir = Path(bucket_dir)
    
    if itemids:
        # ä½¿ç”¨DuckDB hashè®¡ç®—ç›®æ ‡æ¡¶ï¼ˆä¸è½¬æ¢æ—¶ä¸€è‡´ï¼‰
        target_buckets = _duckdb_hash_batch(itemids, num_buckets)
        logger.info(f"ç›®æ ‡itemid: {len(itemids)}ä¸ª, å®šä½åˆ°{len(target_buckets)}ä¸ªæ¡¶")
        
        # åªè¯»å–ç›®æ ‡æ¡¶
        parquet_files = []
        for bucket_id in target_buckets:
            bucket_path = bucket_dir / f"bucket_id={bucket_id}"
            if bucket_path.exists():
                parquet_files.extend(bucket_path.glob("*.parquet"))
        
        if not parquet_files:
            # è¿”å›ç©ºDataFrame
            return pl.DataFrame()
        
        # ä½¿ç”¨Lazy APIå®ç°è°“è¯ä¸‹æ¨
        # Polarsä¼šåˆ©ç”¨Parquet Row Groupç»Ÿè®¡ä¿¡æ¯è·³è¿‡ä¸åŒ¹é…çš„Row Group
        lf = pl.scan_parquet(parquet_files)
        lf = lf.filter(pl.col(partition_col).is_in(list(itemids)))
    else:
        # è¯»å–æ‰€æœ‰æ¡¶ï¼ˆä»å—ç›Šäºåˆ—æŠ•å½±ï¼‰
        lf = pl.scan_parquet(str(bucket_dir / "**/*.parquet"))
    
    # åˆ—æŠ•å½±ï¼šåªè¯»å–éœ€è¦çš„åˆ—ï¼Œå¤§å¹…å‡å°‘å†…å­˜
    if columns:
        # æ’é™¤ bucket_id åˆ—ï¼Œåªé€‰æ‹©ç”¨æˆ·éœ€è¦çš„åˆ—
        available_cols = [c for c in columns if c != 'bucket_id']
        lf = lf.select(available_cols)
    else:
        # æ’é™¤ bucket_id åˆ—
        lf = lf.select(pl.exclude('bucket_id'))
    
    # éªŒè¯æŸ¥è¯¢è®¡åˆ’ï¼ˆè°ƒè¯•ç”¨ï¼‰
    if explain:
        print("=== Polars æŸ¥è¯¢è®¡åˆ’ ===")
        print(lf.explain())
        print()
        print("=== ä¼˜åŒ–åæŸ¥è¯¢è®¡åˆ’ ===")
        print(lf.explain(optimized=True))
    
    return lf.collect()


def read_from_buckets_streaming(
    bucket_dir: Path,
    itemids: Optional[Set[int]] = None,
    columns: Optional[List[str]] = None,
    partition_col: str = 'itemid',
    num_buckets: int = 100,
    batch_size: int = 1_000_000
):
    """
    æµå¼è¯»å–åˆ†æ¡¶æ•°æ®ï¼Œç”¨äºè¶…å¤§ç»“æœé›†
    
    å½“é¢„æœŸç»“æœè¶…è¿‡å¯ç”¨å†…å­˜æ—¶ä½¿ç”¨æ­¤å‡½æ•°ã€‚
    æ¯æ¬¡yieldä¸€ä¸ªbatchï¼Œç”±è°ƒç”¨è€…å†³å®šå¦‚ä½•å¤„ç†ã€‚
    
    Args:
        bucket_dir: åˆ†æ¡¶ç›®å½•
        itemids: è¦è¯»å–çš„itemidé›†åˆ
        columns: è¦è¯»å–çš„åˆ—
        partition_col: åˆ†æ¡¶åˆ—å
        num_buckets: æ¡¶æ•°é‡
        batch_size: æ¯æ‰¹è¡Œæ•°
    
    Yields:
        polars.DataFrame: æ¯æ‰¹æ•°æ®
    """
    import polars as pl
    
    bucket_dir = Path(bucket_dir)
    
    if itemids:
        target_buckets = _duckdb_hash_batch(itemids, num_buckets)
        
        for bucket_id in sorted(target_buckets):
            bucket_path = bucket_dir / f"bucket_id={bucket_id}"
            if not bucket_path.exists():
                continue
                
            parquet_files = list(bucket_path.glob("*.parquet"))
            if not parquet_files:
                continue
            
            # é€æ¡¶è¯»å–
            lf = pl.scan_parquet(parquet_files)
            lf = lf.filter(pl.col(partition_col).is_in(list(itemids)))
            
            if columns:
                available_cols = [c for c in columns if c != 'bucket_id']
                lf = lf.select(available_cols)
            else:
                lf = lf.select(pl.exclude('bucket_id'))
            
            # ä½¿ç”¨ sink çš„æ–¹å¼åˆ†æ‰¹è¿”å›
            df = lf.collect()
            
            # åˆ†æ‰¹ yield
            for i in range(0, len(df), batch_size):
                yield df.slice(i, batch_size)
    else:
        # å…¨é‡è¯»å–ä¹Ÿåˆ†æ‰¹
        all_dirs = sorted(bucket_dir.iterdir())
        for bucket_path in all_dirs:
            if not bucket_path.is_dir():
                continue
            
            parquet_files = list(bucket_path.glob("*.parquet"))
            if not parquet_files:
                continue
            
            lf = pl.scan_parquet(parquet_files)
            
            if columns:
                available_cols = [c for c in columns if c != 'bucket_id']
                lf = lf.select(available_cols)
            else:
                lf = lf.select(pl.exclude('bucket_id'))
            
            df = lf.collect()
            for i in range(0, len(df), batch_size):
                yield df.slice(i, batch_size)


# === AUMC numericitems ä¸“ç”¨è½¬æ¢å‡½æ•° ===

def convert_aumc_numericitems(
    data_path: str = '/home/zhuhb/icudb/aumc/1.0.2',
    num_buckets: int = 100,
    overwrite: bool = False
) -> ConversionResult:
    """
    è½¬æ¢ AUMC numericitems åˆ°åˆ†æ¡¶æ ¼å¼
    
    AUMC numericitems.csv åŒ…å«ç‰¹æ®Šç¼–ç å­—ç¬¦ï¼ˆå¦‚ Âµmolï¼‰ï¼Œéœ€è¦ä½¿ç”¨æ˜¾å¼ schema
    æ¥é¿å… DuckDB çš„ç±»å‹æ¨æ–­å› ç‰¹æ®Šå­—ç¬¦è€Œè·³è¿‡è¡Œã€‚
    
    Args:
        data_path: AUMCæ•°æ®ç›®å½•
        num_buckets: æ¡¶æ•°é‡ï¼ˆé»˜è®¤100ï¼‰
        overwrite: æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„ç›®å½•
    """
    import duckdb
    import shutil
    
    start_time = time.time()
    data_path = Path(data_path)
    source = data_path / 'numericitems.csv'
    output = data_path / 'numericitems_bucket'
    
    def log(msg: str):
        logger.info(msg)
        print(msg)
    
    if not source.exists():
        return ConversionResult(
            success=False, num_buckets=0, total_rows=0,
            total_size_bytes=0, elapsed_seconds=0,
            error=f"æºæ–‡ä»¶ä¸å­˜åœ¨: {source}"
        )
    
    if output.exists():
        if overwrite:
            log(f"åˆ é™¤å·²å­˜åœ¨çš„è¾“å‡ºç›®å½•: {output}")
            shutil.rmtree(output)
        else:
            return ConversionResult(
                success=False, num_buckets=0, total_rows=0,
                total_size_bytes=0, elapsed_seconds=0,
                error=f"è¾“å‡ºç›®å½•å·²å­˜åœ¨: {output}"
            )
    
    output.mkdir(parents=True, exist_ok=True)
    
    try:
        log(f"å¼€å§‹è½¬æ¢: {source.name}")
        log(f"åˆ†æ¡¶æ•°: {num_buckets}, åˆ†æ¡¶åˆ—: itemid")
        
        conn = duckdb.connect()
        conn.execute("SET threads=16")
        conn.execute("SET memory_limit='10GB'")
        
        # AUMC numericitems.csv ä½¿ç”¨ Latin-1 ç¼–ç ï¼ˆåŒ…å« Âµmol ç­‰ç‰¹æ®Šå­—ç¬¦ï¼‰
        # å¿…é¡»æŒ‡å®š encoding='latin-1'ï¼Œå¦åˆ™ DuckDB ä¼šåœ¨ COPY/CREATE TABLE æ—¶ä¸¢å¤±æ•°æ®
        # è¿˜éœ€è¦ null_padding=true å’Œ strict_mode=false å¤„ç†æ ¼å¼ä¸è§„èŒƒçš„è¡Œ
        read_expr = f"""read_csv_auto(
            '{source}',
            ignore_errors=true,
            encoding='latin-1',
            null_padding=true,
            strict_mode=false
        )"""
        
        log("æ‰§è¡Œåˆ†æ¡¶è½¬æ¢ (encoding=latin-1 + æ’åº + åˆ†æ¡¶)...")
        
        sql = f"""
            COPY (
                SELECT *,
                       hash(itemid) % {num_buckets} as bucket_id
                FROM {read_expr}
                ORDER BY itemid
            )
            TO '{output}'
            (FORMAT PARQUET,
             PARTITION_BY (bucket_id),
             COMPRESSION SNAPPY,
             ROW_GROUP_SIZE 100000,
             OVERWRITE_OR_IGNORE)
        """
        
        conn.execute(sql)
        
        # ç»Ÿè®¡ç»“æœ
        elapsed = time.time() - start_time
        
        # è®¡ç®—æ€»è¡Œæ•°
        row_count = conn.execute(f"SELECT COUNT(*) FROM {read_expr}").fetchone()[0]
        
        # è®¡ç®—æ€»å¤§å°
        total_size = sum(f.stat().st_size for f in output.rglob('*.parquet'))
        actual_buckets = len([d for d in output.iterdir() if d.is_dir()])
        
        conn.close()
        
        log(f"è½¬æ¢å®Œæˆ! è€—æ—¶: {elapsed:.1f}ç§’")
        log(f"æ€»è¡Œæ•°: {row_count:,}")
        log(f"åˆ†æ¡¶æ•°: {actual_buckets}")
        log(f"æ€»å¤§å°: {total_size / 1024**3:.2f} GB")
        
        return ConversionResult(
            success=True,
            num_buckets=actual_buckets,
            total_rows=row_count,
            total_size_bytes=total_size,
            elapsed_seconds=elapsed,
            output_dir=output
        )
        
    except Exception as e:
        elapsed = time.time() - start_time
        logger.exception("è½¬æ¢å¤±è´¥")
        return ConversionResult(
            success=False, num_buckets=0, total_rows=0,
            total_size_bytes=0, elapsed_seconds=elapsed,
            error=str(e)
        )


def convert_aumc_listitems(
    data_path: str = '/home/zhuhb/icudb/aumc/1.0.2',
    num_buckets: int = 50,
    overwrite: bool = False
) -> ConversionResult:
    """
    è½¬æ¢ AUMC listitems åˆ°åˆ†æ¡¶æ ¼å¼
    """
    data_path = Path(data_path)
    source = data_path / 'listitems.csv'
    output = data_path / 'listitems_bucket'
    
    config = BucketConfig(
        num_buckets=num_buckets,
        partition_col='itemid',
        row_group_size=100_000,
        compression='snappy'
    )
    
    return convert_to_buckets(source, output, config, overwrite=overwrite)


def convert_parquet_directory_to_buckets(
    source_dir: Path,
    output_dir: Path,
    partition_col: str,
    num_buckets: int = 100,
    overwrite: bool = False,
    progress_callback: Optional[Callable[[str], None]] = None
) -> ConversionResult:
    """
    å°†å·²æœ‰çš„ Parquet ç›®å½•ï¼ˆå¦‚ HiRID observationsï¼‰è½¬æ¢ä¸ºåˆ†æ¡¶æ ¼å¼
    
    è¿™ä¸ªå‡½æ•°ä¸“é—¨å¤„ç†å·²æœ‰å¤šä¸ª parquet åˆ†ç‰‡çš„æƒ…å†µï¼Œä¾‹å¦‚ï¼š
    - HiRID observations: 250ä¸ªæŒ‰æ‚£è€…åˆ†ç‰‡çš„parquet â†’ æŒ‰variableidåˆ†æ¡¶
    - MIIV chartevents: 30ä¸ªæ•°å­—åˆ†ç‰‡çš„parquet â†’ æŒ‰itemidåˆ†æ¡¶
    
    æ ¸å¿ƒä¼˜åŒ–ï¼š
    - ä½¿ç”¨ DuckDB glob æ¨¡å¼è¯»å–æ‰€æœ‰åˆ†ç‰‡
    - ä¸€æ¬¡æ€§æ’åºå¹¶åˆ†æ¡¶è¾“å‡º
    - 16GB å†…å­˜å®‰å…¨ï¼šè®¾ç½® memory_limit å’Œ temp_directory
    
    Args:
        source_dir: æºç›®å½•ï¼ˆåŒ…å«å¤šä¸ª parquet æ–‡ä»¶ï¼‰
        output_dir: è¾“å‡ºç›®å½•ï¼ˆå°†åˆ›å»º bucket_id=* å­ç›®å½•ï¼‰
        partition_col: åˆ†æ¡¶åˆ—ï¼ˆå¦‚ variableid æˆ– itemidï¼‰
        num_buckets: æ¡¶æ•°é‡
        overwrite: æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„ç›®å½•
        progress_callback: è¿›åº¦å›è°ƒ
    
    Returns:
        ConversionResult
    """
    import duckdb
    import shutil
    
    def log(msg: str):
        logger.info(msg)
        if progress_callback:
            progress_callback(msg)
        else:
            print(msg)
    
    start_time = time.time()
    source_dir = Path(source_dir)
    output_dir = Path(output_dir)
    
    # æ£€æŸ¥æºç›®å½•
    if not source_dir.is_dir():
        return ConversionResult(
            success=False, num_buckets=0, total_rows=0,
            total_size_bytes=0, elapsed_seconds=0,
            error=f"æºç›®å½•ä¸å­˜åœ¨: {source_dir}"
        )
    
    parquet_files = list(source_dir.glob("*.parquet"))
    if not parquet_files:
        return ConversionResult(
            success=False, num_buckets=0, total_rows=0,
            total_size_bytes=0, elapsed_seconds=0,
            error=f"æºç›®å½•æ²¡æœ‰ parquet æ–‡ä»¶: {source_dir}"
        )
    
    log(f"å‘ç° {len(parquet_files)} ä¸ª parquet æ–‡ä»¶")
    
    # å‡†å¤‡è¾“å‡ºç›®å½•
    if output_dir.exists():
        if overwrite:
            log(f"åˆ é™¤å·²å­˜åœ¨çš„è¾“å‡ºç›®å½•: {output_dir}")
            shutil.rmtree(output_dir)
        else:
            return ConversionResult(
                success=False, num_buckets=0, total_rows=0,
                total_size_bytes=0, elapsed_seconds=0,
                error=f"è¾“å‡ºç›®å½•å·²å­˜åœ¨: {output_dir}"
            )
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        log(f"å¼€å§‹è½¬æ¢: {source_dir.name} â†’ {output_dir.name}")
        log(f"åˆ†æ¡¶æ•°: {num_buckets}, åˆ†æ¡¶åˆ—: {partition_col}")
        
        conn = duckdb.connect()
        conn.execute("SET threads=16")
        conn.execute("SET memory_limit='10GB'")
        
        # ä½¿ç”¨ä¸´æ—¶ç›®å½•é˜²æ­¢å†…å­˜æº¢å‡º
        temp_dir = output_dir.parent / f".{output_dir.name}_temp"
        temp_dir.mkdir(exist_ok=True)
        conn.execute(f"SET temp_directory='{temp_dir}'")
        log(f"ä¸´æ—¶ç›®å½•: {temp_dir}")
        
        # ä½¿ç”¨ glob è¯»å–æ‰€æœ‰ parquetï¼Œunion_by_name å¤„ç† schema å·®å¼‚
        glob_pattern = str(source_dir / "*.parquet")
        read_expr = f"read_parquet('{glob_pattern}', union_by_name=true)"
        
        # åˆ†æ¡¶è½¬æ¢
        log("æ‰§è¡Œåˆ†æ¡¶è½¬æ¢ (è¯»å– â†’ æ’åº â†’ åˆ†æ¡¶)...")
        
        sql = f"""
            COPY (
                SELECT *,
                       hash({partition_col}) % {num_buckets} as bucket_id
                FROM {read_expr}
                ORDER BY {partition_col}
            )
            TO '{output_dir}'
            (FORMAT PARQUET,
             PARTITION_BY (bucket_id),
             COMPRESSION SNAPPY,
             ROW_GROUP_SIZE 100000,
             OVERWRITE_OR_IGNORE)
        """
        
        conn.execute(sql)
        
        # ç»Ÿè®¡ç»“æœ
        elapsed = time.time() - start_time
        
        # è®¡ç®—æ€»è¡Œæ•°
        row_count = conn.execute(f"SELECT COUNT(*) FROM {read_expr}").fetchone()[0]
        
        # è®¡ç®—æ€»å¤§å°
        total_size = sum(f.stat().st_size for f in output_dir.rglob('*.parquet'))
        actual_buckets = len([d for d in output_dir.iterdir() if d.is_dir()])
        
        conn.close()
        
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        if temp_dir.exists():
            shutil.rmtree(temp_dir, ignore_errors=True)
        
        log(f"è½¬æ¢å®Œæˆ! è€—æ—¶: {elapsed:.1f}ç§’")
        log(f"æ€»è¡Œæ•°: {row_count:,}")
        log(f"åˆ†æ¡¶æ•°: {actual_buckets}")
        log(f"æ€»å¤§å°: {total_size / 1024**3:.2f} GB")
        
        return ConversionResult(
            success=True,
            num_buckets=actual_buckets,
            total_rows=row_count,
            total_size_bytes=total_size,
            elapsed_seconds=elapsed,
            output_dir=output_dir
        )
        
    except Exception as e:
        elapsed = time.time() - start_time
        logger.exception("è½¬æ¢å¤±è´¥")
        return ConversionResult(
            success=False, num_buckets=0, total_rows=0,
            total_size_bytes=0, elapsed_seconds=elapsed,
            error=str(e)
        )


def convert_hirid_observations(
    data_path: str = '/home/zhuhb/icudb/hirid/1.1.1',
    num_buckets: int = 100,
    overwrite: bool = False,
    progress_callback: Optional[Callable[[str], None]] = None
) -> ConversionResult:
    """
    å°† HiRID observations è½¬æ¢ä¸ºæŒ‰ variableid åˆ†æ¡¶æ ¼å¼
    
    HiRID å®˜æ–¹æä¾›çš„ 250 ä¸ªåˆ†ç‰‡æ˜¯æŒ‰æ‚£è€…åˆ†çš„ï¼Œ
    è¿™æ„å‘³ç€æå–å•ä¸ªå˜é‡æ—¶ä»éœ€æ‰«ææ‰€æœ‰ 250 ä¸ªåˆ†ç‰‡ã€‚
    
    æŒ‰ variableid åˆ†æ¡¶åï¼š
    - æå–å•å˜é‡åªéœ€æ‰«æ 1 ä¸ªæ¡¶ï¼ˆè·³è¿‡ 99% æ— å…³æ•°æ®ï¼‰
    - é¢„æœŸæ€§èƒ½æå‡ 10-100x
    - å†…å­˜å³°å€¼å¤§å¹…é™ä½
    
    Args:
        data_path: HiRID æ•°æ®ç›®å½•
        num_buckets: æ¡¶æ•°é‡ï¼ˆé»˜è®¤ 100ï¼‰
        overwrite: æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„ç›®å½•
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
    """
    data_path = Path(data_path)
    source = data_path / 'observations'
    output = data_path / 'observations_bucket'
    
    return convert_parquet_directory_to_buckets(
        source, output, 
        partition_col='variableid',
        num_buckets=num_buckets,
        overwrite=overwrite,
        progress_callback=progress_callback
    )


def convert_miiv_chartevents(
    data_path: str = '/home/zhuhb/icudb/mimiciv/3.1/icu',
    num_buckets: int = 100,
    overwrite: bool = False,
    progress_callback: Optional[Callable[[str], None]] = None
) -> ConversionResult:
    """
    å°† MIIV chartevents ç›´æ¥ä» csv.gz è½¬æ¢ä¸ºæŒ‰ itemid åˆ†æ¡¶æ ¼å¼
    
    ä¸€æ­¥åˆ°ä½ï¼šcsv.gz â†’ åˆ†æ¡¶ parquetï¼ˆæ— éœ€å…ˆè½¬æˆå•ä¸ª parquetï¼‰
    
    Args:
        data_path: MIIV ICU æ•°æ®ç›®å½•ï¼ˆå« chartevents.csv.gzï¼‰
        num_buckets: æ¡¶æ•°é‡ï¼ˆé»˜è®¤ 100ï¼‰
        overwrite: æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„ç›®å½•
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
    """
    data_path = Path(data_path)
    
    # æŒ‰ä¼˜å…ˆçº§æ£€æŸ¥æºæ–‡ä»¶ï¼šcsv.gz > parquet
    csv_gz = data_path / 'chartevents.csv.gz'
    parquet = data_path / 'chartevents.parquet'
    
    if csv_gz.exists():
        source = csv_gz
    elif parquet.exists():
        source = parquet
    else:
        return ConversionResult(
            success=False, num_buckets=0, total_rows=0,
            total_size_bytes=0, elapsed_seconds=0,
            error=f"æœªæ‰¾åˆ° chartevents æºæ–‡ä»¶ï¼Œå·²æ£€æŸ¥:\n  - {csv_gz}\n  - {parquet}"
        )
    
    # è¾“å‡ºåˆ°åŒçº§ç›®å½•çš„ _bucket ç›®å½•
    output = data_path / 'chartevents_bucket'
    
    config = BucketConfig(
        num_buckets=num_buckets,
        partition_col='itemid',
        row_group_size=100_000,
        compression='snappy'
    )
    
    return convert_to_buckets(source, output, config, progress_callback=progress_callback, overwrite=overwrite)


def convert_eicu_nursecharting(
    data_path: str = '/home/zhuhb/icudb/eicu/2.0.1',
    num_buckets: int = 50,
    overwrite: bool = False,
    progress_callback: Optional[Callable[[str], None]] = None
) -> ConversionResult:
    """
    å°† eICU nursecharting è½¬æ¢ä¸ºæŒ‰ nursingchartcelltypevalname åˆ†æ¡¶æ ¼å¼
    
    eICU ä½¿ç”¨å­—ç¬¦ä¸²ä½œä¸ºå˜é‡æ ‡è¯†ï¼ˆå¦‚ 'Heart Rate', 'O2 Saturation'ï¼‰
    æ”¯æŒä» csv.gz æˆ–å·²è½¬æ¢çš„ parquet ç›®å½•è½¬æ¢
    
    Args:
        data_path: eICU æ•°æ®ç›®å½•
        num_buckets: æ¡¶æ•°é‡
        overwrite: æ˜¯å¦è¦†ç›–
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
    """
    data_path = Path(data_path)
    
    # æŒ‰ä¼˜å…ˆçº§æ£€æŸ¥æºæ–‡ä»¶ï¼šcsv.gz > å·²æœ‰ parquet ç›®å½•
    csv_gz = data_path / 'nurseCharting.csv.gz'
    parquet_dir = data_path / 'nursecharting'
    
    output = data_path / 'nursecharting_bucket'
    
    if csv_gz.exists():
        # ä» csv.gz ç›´æ¥è½¬æ¢
        config = BucketConfig(
            num_buckets=num_buckets,
            partition_col='nursingchartcelltypevalname',
            row_group_size=100_000,
            compression='snappy'
        )
        return convert_to_buckets(csv_gz, output, config, progress_callback=progress_callback, overwrite=overwrite)
    elif parquet_dir.exists() and parquet_dir.is_dir():
        # ä»å·²æœ‰ parquet ç›®å½•è½¬æ¢
        return convert_parquet_directory_to_buckets(
            parquet_dir, output,
            partition_col='nursingchartcelltypevalname',
            num_buckets=num_buckets,
            overwrite=overwrite,
            progress_callback=progress_callback
        )
    else:
        return ConversionResult(
            success=False, num_buckets=0, total_rows=0,
            total_size_bytes=0, elapsed_seconds=0,
            error=f"æœªæ‰¾åˆ° nursecharting æºæ–‡ä»¶ï¼Œå·²æ£€æŸ¥:\n  - {csv_gz}\n  - {parquet_dir}/"
        )


def convert_miiv_labevents(
    data_path: str = '/home/zhuhb/icudb/mimiciv/3.1/hosp',
    num_buckets: int = 100,
    overwrite: bool = False,
    progress_callback: Optional[Callable[[str], None]] = None
) -> ConversionResult:
    """
    å°† MIIV labevents ç›´æ¥ä» csv.gz è½¬æ¢ä¸ºæŒ‰ itemid åˆ†æ¡¶æ ¼å¼
    
    ä¸€æ­¥åˆ°ä½ï¼šcsv.gz â†’ åˆ†æ¡¶ parquet
    
    Args:
        data_path: MIIV hosp æ•°æ®ç›®å½•ï¼ˆå« labevents.csv.gzï¼‰
        num_buckets: æ¡¶æ•°é‡ï¼ˆé»˜è®¤ 100ï¼‰
        overwrite: æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„ç›®å½•
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
    """
    data_path = Path(data_path)
    
    # æŒ‰ä¼˜å…ˆçº§æ£€æŸ¥æºæ–‡ä»¶ï¼šcsv.gz > parquet
    csv_gz = data_path / 'labevents.csv.gz'
    parquet = data_path / 'labevents.parquet'
    parquet_dir = data_path / 'labevents'  # å¯èƒ½æ˜¯ç›®å½•å½¢å¼
    
    if csv_gz.exists():
        source = csv_gz
    elif parquet.exists():
        source = parquet
    elif parquet_dir.exists() and parquet_dir.is_dir():
        # ä»å·²æœ‰ç›®å½•è½¬æ¢
        return convert_parquet_directory_to_buckets(
            parquet_dir, data_path / 'labevents_bucket',
            partition_col='itemid',
            num_buckets=num_buckets,
            overwrite=overwrite,
            progress_callback=progress_callback
        )
    else:
        return ConversionResult(
            success=False, num_buckets=0, total_rows=0,
            total_size_bytes=0, elapsed_seconds=0,
            error=f"æœªæ‰¾åˆ° labevents æºæ–‡ä»¶ï¼Œå·²æ£€æŸ¥:\n  - {csv_gz}\n  - {parquet}\n  - {parquet_dir}/"
        )
    
    output = data_path / 'labevents_bucket'
    
    config = BucketConfig(
        num_buckets=num_buckets,
        partition_col='itemid',
        row_group_size=100_000,
        compression='snappy'
    )
    
    return convert_to_buckets(source, output, config, progress_callback=progress_callback, overwrite=overwrite)


def convert_eicu_lab(
    data_path: str = '/home/zhuhb/icudb/eicu/2.0.1',
    num_buckets: int = 50,
    overwrite: bool = False,
    progress_callback: Optional[Callable[[str], None]] = None
) -> ConversionResult:
    """
    å°† eICU lab è½¬æ¢ä¸ºæŒ‰ labname åˆ†æ¡¶æ ¼å¼
    
    eICU ä½¿ç”¨å­—ç¬¦ä¸²ï¼ˆå¦‚ 'glucose', 'creatinine'ï¼‰ä½œä¸ºå˜é‡æ ‡è¯†
    
    Args:
        data_path: eICU æ•°æ®ç›®å½•
        num_buckets: æ¡¶æ•°é‡ï¼ˆé»˜è®¤ 50ï¼Œå› ä¸ºåªæœ‰ 158 ä¸ªå”¯ä¸€ labnameï¼‰
        overwrite: æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„ç›®å½•
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
    """
    data_path = Path(data_path)
    
    # æŒ‰ä¼˜å…ˆçº§æ£€æŸ¥æºæ–‡ä»¶ï¼šcsv.gz > parquet > ç›®å½•
    csv_gz = data_path / 'lab.csv.gz'
    parquet = data_path / 'lab.parquet'
    parquet_dir = data_path / 'lab'
    
    output = data_path / 'lab_bucket'
    
    if csv_gz.exists():
        source = csv_gz
    elif parquet.exists():
        source = parquet
    elif parquet_dir.exists() and parquet_dir.is_dir():
        # ä»å·²æœ‰ç›®å½•è½¬æ¢
        return convert_parquet_directory_to_buckets(
            parquet_dir, output,
            partition_col='labname',
            num_buckets=num_buckets,
            overwrite=overwrite,
            progress_callback=progress_callback
        )
    else:
        return ConversionResult(
            success=False, num_buckets=0, total_rows=0,
            total_size_bytes=0, elapsed_seconds=0,
            error=f"æœªæ‰¾åˆ° lab æºæ–‡ä»¶ï¼Œå·²æ£€æŸ¥:\n  - {csv_gz}\n  - {parquet}\n  - {parquet_dir}/"
        )
    
    config = BucketConfig(
        num_buckets=num_buckets,
        partition_col='labname',
        row_group_size=100_000,
        compression='snappy'
    )
    
    return convert_to_buckets(source, output, config, progress_callback=progress_callback, overwrite=overwrite)


def convert_miiv_inputevents(
    data_path: str = '/home/zhuhb/icudb/mimiciv/3.1/icu',
    num_buckets: int = 50,
    overwrite: bool = False,
    progress_callback: Optional[Callable[[str], None]] = None
) -> ConversionResult:
    """
    å°† MIIV inputevents è½¬æ¢ä¸ºæŒ‰ itemid åˆ†æ¡¶æ ¼å¼
    
    inputevents åŒ…å«è¡€ç®¡æ´»æ€§è¯ç‰©ç­‰é‡è¦æ¦‚å¿µï¼ˆçº¦13ä¸ªï¼‰
    
    Args:
        data_path: MIIV ICU æ•°æ®ç›®å½•ï¼ˆå« inputevents.csv.gzï¼‰
        num_buckets: æ¡¶æ•°é‡ï¼ˆé»˜è®¤ 50ï¼Œå› ä¸ºæ¦‚å¿µæ•°è¾ƒå°‘ï¼‰
        overwrite: æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„ç›®å½•
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
    """
    data_path = Path(data_path)
    
    # æŒ‰ä¼˜å…ˆçº§æ£€æŸ¥æºæ–‡ä»¶
    csv_gz = data_path / 'inputevents.csv.gz'
    parquet = data_path / 'inputevents.parquet'
    
    output = data_path / 'inputevents_bucket'
    
    if csv_gz.exists():
        source = csv_gz
    elif parquet.exists():
        source = parquet
    else:
        return ConversionResult(
            success=False, num_buckets=0, total_rows=0,
            total_size_bytes=0, elapsed_seconds=0,
            error=f"æœªæ‰¾åˆ° inputevents æºæ–‡ä»¶ï¼Œå·²æ£€æŸ¥:\n  - {csv_gz}\n  - {parquet}"
        )
    
    config = BucketConfig(
        num_buckets=num_buckets,
        partition_col='itemid',
        row_group_size=100_000,
        compression='snappy'
    )
    
    return convert_to_buckets(source, output, config, progress_callback=progress_callback, overwrite=overwrite)


def convert_hirid_pharma(
    data_path: str = '/home/zhuhb/icudb/hirid/1.1.1',
    num_buckets: int = 50,
    overwrite: bool = False,
    progress_callback: Optional[Callable[[str], None]] = None
) -> ConversionResult:
    """
    å°† HiRID pharma è½¬æ¢ä¸ºæŒ‰ pharmaid åˆ†æ¡¶æ ¼å¼
    
    pharma è¡¨åŒ…å«è¯ç‰©ç›¸å…³æ¦‚å¿µï¼ˆçº¦11ä¸ªï¼‰
    
    Args:
        data_path: HiRID æ•°æ®ç›®å½•
        num_buckets: æ¡¶æ•°é‡ï¼ˆé»˜è®¤ 50ï¼‰
        overwrite: æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„ç›®å½•
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
    """
    data_path = Path(data_path)
    source = data_path / 'pharma'
    output = data_path / 'pharma_bucket'
    
    if not source.is_dir():
        return ConversionResult(
            success=False, num_buckets=0, total_rows=0,
            total_size_bytes=0, elapsed_seconds=0,
            error=f"pharma ç›®å½•ä¸å­˜åœ¨: {source}"
        )
    
    return convert_parquet_directory_to_buckets(
        source, output,
        partition_col='pharmaid',
        num_buckets=num_buckets,
        overwrite=overwrite,
        progress_callback=progress_callback
    )


def convert_mimic3_chartevents(
    data_path: str = '/home/zhuhb/icudb/mimiciii/1.4',
    num_buckets: int = 100,
    overwrite: bool = False,
    progress_callback: Optional[Callable[[str], None]] = None
) -> ConversionResult:
    """
    å°† MIMIC-III chartevents è½¬æ¢ä¸ºæŒ‰ itemid åˆ†æ¡¶æ ¼å¼
    
    MIMIC-III çš„ chartevents è¡¨ç»“æ„ä¸ MIMIC-IV ç±»ä¼¼ï¼Œçº¦3.3äº¿è¡Œ
    
    Args:
        data_path: MIMIC-III æ•°æ®ç›®å½•
        num_buckets: æ¡¶æ•°é‡ï¼ˆé»˜è®¤ 100ï¼‰
        overwrite: æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„ç›®å½•
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
    """
    data_path = Path(data_path)
    
    # ä¼˜å…ˆæ£€æŸ¥å·²æœ‰åˆ†æ¡¶ç›®å½•
    bucket_dir = data_path / 'chartevents_bucket'
    if bucket_dir.exists() and not overwrite:
        return ConversionResult(
            success=True, num_buckets=num_buckets, total_rows=0,
            total_size_bytes=0, elapsed_seconds=0,
            error=f"åˆ†æ¡¶ç›®å½•å·²å­˜åœ¨: {bucket_dir}"
        )
    
    # æŸ¥æ‰¾æºæ–‡ä»¶
    source = None
    for name in ['chartevents.csv.gz', 'chartevents.csv', 'chartevents.parquet']:
        p = data_path / name
        if p.exists():
            source = p
            break
    
    if not source:
        return ConversionResult(
            success=False, num_buckets=0, total_rows=0,
            total_size_bytes=0, elapsed_seconds=0,
            error=f"chartevents ä¸å­˜åœ¨äº {data_path}"
        )
    
    config = BucketConfig(
        num_buckets=num_buckets,
        partition_col='itemid',
        row_group_size=100_000
    )
    
    return convert_to_buckets(source, bucket_dir, config, progress_callback=progress_callback, overwrite=overwrite)


def convert_mimic3_labevents(
    data_path: str = '/home/zhuhb/icudb/mimiciii/1.4',
    num_buckets: int = 100,
    overwrite: bool = False,
    progress_callback: Optional[Callable[[str], None]] = None
) -> ConversionResult:
    """
    å°† MIMIC-III labevents è½¬æ¢ä¸ºæŒ‰ itemid åˆ†æ¡¶æ ¼å¼
    
    Args:
        data_path: MIMIC-III æ•°æ®ç›®å½•
        num_buckets: æ¡¶æ•°é‡ï¼ˆé»˜è®¤ 100ï¼‰
        overwrite: æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„ç›®å½•
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
    """
    data_path = Path(data_path)
    
    bucket_dir = data_path / 'labevents_bucket'
    if bucket_dir.exists() and not overwrite:
        return ConversionResult(
            success=True, num_buckets=num_buckets, total_rows=0,
            total_size_bytes=0, elapsed_seconds=0,
            error=f"åˆ†æ¡¶ç›®å½•å·²å­˜åœ¨: {bucket_dir}"
        )
    
    source = None
    for name in ['labevents.csv.gz', 'labevents.csv', 'labevents.parquet']:
        p = data_path / name
        if p.exists():
            source = p
            break
    
    if not source:
        return ConversionResult(
            success=False, num_buckets=0, total_rows=0,
            total_size_bytes=0, elapsed_seconds=0,
            error=f"labevents ä¸å­˜åœ¨äº {data_path}"
        )
    
    config = BucketConfig(
        num_buckets=num_buckets,
        partition_col='itemid',
        row_group_size=100_000
    )
    
    return convert_to_buckets(source, bucket_dir, config, progress_callback=progress_callback, overwrite=overwrite)


def convert_sic_data_float_h(
    data_path: str = '/home/zhuhb/icudb/sicdb/1.0.6',
    num_buckets: int = 50,
    overwrite: bool = False,
    progress_callback: Optional[Callable[[str], None]] = None
) -> ConversionResult:
    """
    å°† SICdb data_float_h è½¬æ¢ä¸ºæŒ‰ DataID åˆ†æ¡¶æ ¼å¼
    
    data_float_h æ˜¯ SICdb çš„ä¸»è¦ç”Ÿå‘½ä½“å¾è¡¨ï¼ˆçº¦3.1GBï¼‰
    
    Args:
        data_path: SICdb æ•°æ®ç›®å½•
        num_buckets: æ¡¶æ•°é‡ï¼ˆé»˜è®¤ 50ï¼‰
        overwrite: æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„ç›®å½•
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
    """
    data_path = Path(data_path)
    
    bucket_dir = data_path / 'data_float_h_bucket'
    if bucket_dir.exists() and not overwrite:
        return ConversionResult(
            success=True, num_buckets=num_buckets, total_rows=0,
            total_size_bytes=0, elapsed_seconds=0,
            error=f"åˆ†æ¡¶ç›®å½•å·²å­˜åœ¨: {bucket_dir}"
        )
    
    source = None
    for name in ['data_float_h.csv.gz', 'data_float_h.csv', 'data_float_h.parquet']:
        p = data_path / name
        if p.exists():
            source = p
            break
    
    if not source:
        return ConversionResult(
            success=False, num_buckets=0, total_rows=0,
            total_size_bytes=0, elapsed_seconds=0,
            error=f"data_float_h ä¸å­˜åœ¨äº {data_path}"
        )
    
    config = BucketConfig(
        num_buckets=num_buckets,
        partition_col='DataID',  # SICdb ä½¿ç”¨å¤§å†™åˆ—å
        row_group_size=100_000
    )
    
    return convert_to_buckets(source, bucket_dir, config, progress_callback=progress_callback, overwrite=overwrite)


def convert_sic_laboratory(
    data_path: str = '/home/zhuhb/icudb/sicdb/1.0.6',
    num_buckets: int = 50,
    overwrite: bool = False,
    progress_callback: Optional[Callable[[str], None]] = None
) -> ConversionResult:
    """
    å°† SICdb laboratory è½¬æ¢ä¸ºæŒ‰ LaboratoryID åˆ†æ¡¶æ ¼å¼
    
    Args:
        data_path: SICdb æ•°æ®ç›®å½•
        num_buckets: æ¡¶æ•°é‡ï¼ˆé»˜è®¤ 50ï¼‰
        overwrite: æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„ç›®å½•
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
    """
    data_path = Path(data_path)
    
    bucket_dir = data_path / 'laboratory_bucket'
    if bucket_dir.exists() and not overwrite:
        return ConversionResult(
            success=True, num_buckets=num_buckets, total_rows=0,
            total_size_bytes=0, elapsed_seconds=0,
            error=f"åˆ†æ¡¶ç›®å½•å·²å­˜åœ¨: {bucket_dir}"
        )
    
    source = None
    for name in ['laboratory.csv.gz', 'laboratory.csv', 'laboratory.parquet']:
        p = data_path / name
        if p.exists():
            source = p
            break
    
    if not source:
        return ConversionResult(
            success=False, num_buckets=0, total_rows=0,
            total_size_bytes=0, elapsed_seconds=0,
            error=f"laboratory ä¸å­˜åœ¨äº {data_path}"
        )
    
    config = BucketConfig(
        num_buckets=num_buckets,
        partition_col='LaboratoryID',  # SICdb ä½¿ç”¨å¤§å†™åˆ—å
        row_group_size=100_000
    )
    
    return convert_to_buckets(source, bucket_dir, config, progress_callback=progress_callback, overwrite=overwrite)


def convert_sic_medication(
    data_path: str = '/home/zhuhb/icudb/sicdb/1.0.6',
    num_buckets: int = 50,
    overwrite: bool = False,
    progress_callback: Optional[Callable[[str], None]] = None
) -> ConversionResult:
    """
    å°† SICdb medication è½¬æ¢ä¸ºæŒ‰ DrugID åˆ†æ¡¶æ ¼å¼
    
    Args:
        data_path: SICdb æ•°æ®ç›®å½•
        num_buckets: æ¡¶æ•°é‡ï¼ˆé»˜è®¤ 50ï¼‰
        overwrite: æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„ç›®å½•
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
    """
    data_path = Path(data_path)
    
    bucket_dir = data_path / 'medication_bucket'
    if bucket_dir.exists() and not overwrite:
        return ConversionResult(
            success=True, num_buckets=num_buckets, total_rows=0,
            total_size_bytes=0, elapsed_seconds=0,
            error=f"åˆ†æ¡¶ç›®å½•å·²å­˜åœ¨: {bucket_dir}"
        )
    
    source = None
    for name in ['medication.csv.gz', 'medication.csv', 'medication.parquet']:
        p = data_path / name
        if p.exists():
            source = p
            break
    
    if not source:
        return ConversionResult(
            success=False, num_buckets=0, total_rows=0,
            total_size_bytes=0, elapsed_seconds=0,
            error=f"medication ä¸å­˜åœ¨äº {data_path}"
        )
    
    config = BucketConfig(
        num_buckets=num_buckets,
        partition_col='DrugID',  # SICdb ä½¿ç”¨å¤§å†™åˆ—å
        row_group_size=100_000
    )
    
    return convert_to_buckets(source, bucket_dir, config, progress_callback=progress_callback, overwrite=overwrite)


def verify_query_plan(
    bucket_dir: Path,
    itemids: Set[int],
    columns: List[str],
    partition_col: str = 'itemid',
    num_buckets: int = 100
) -> dict:
    """
    éªŒè¯æŸ¥è¯¢è®¡åˆ’æ˜¯å¦æ­£ç¡®åº”ç”¨äº†è°“è¯ä¸‹æ¨å’Œåˆ—æŠ•å½±
    
    ç”¨äºè°ƒè¯•å’Œæ€§èƒ½éªŒè¯ï¼š
    1. æ£€æŸ¥ FILTER æ˜¯å¦å‡ºç°åœ¨è®¡åˆ’ä¸­ï¼ˆè°“è¯ä¸‹æ¨ï¼‰
    2. æ£€æŸ¥ PROJECT æ˜¯å¦åªåŒ…å«éœ€è¦çš„åˆ—ï¼ˆåˆ—æŠ•å½±ï¼‰
    3. ä¼°ç®—å®é™…æ‰«æçš„æ•°æ®é‡ vs å…¨é‡æ•°æ®
    
    Args:
        bucket_dir: åˆ†æ¡¶ç›®å½•
        itemids: ç›®æ ‡itemidé›†åˆ
        columns: éœ€è¦çš„åˆ—
        partition_col: åˆ†æ¡¶åˆ—å
        num_buckets: æ¡¶æ•°é‡
    
    Returns:
        dict: åŒ…å«æŸ¥è¯¢è®¡åˆ’å’Œä¼˜åŒ–ä¿¡æ¯
    """
    import polars as pl
    
    bucket_dir = Path(bucket_dir)
    # ä½¿ç”¨DuckDB hashè®¡ç®—ç›®æ ‡æ¡¶ï¼ˆä¸è½¬æ¢æ—¶ä¸€è‡´ï¼‰
    target_buckets = _duckdb_hash_batch(itemids, num_buckets)
    
    # æ”¶é›†ç›®æ ‡æ¡¶æ–‡ä»¶
    parquet_files = []
    for bucket_id in target_buckets:
        bucket_path = bucket_dir / f"bucket_id={bucket_id}"
        if bucket_path.exists():
            parquet_files.extend(bucket_path.glob("*.parquet"))
    
    if not parquet_files:
        return {"error": "æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ¡¶"}
    
    # æ„å»ºæŸ¥è¯¢
    lf = pl.scan_parquet(parquet_files)
    lf = lf.filter(pl.col(partition_col).is_in(list(itemids)))
    available_cols = [c for c in columns if c != 'bucket_id']
    lf = lf.select(available_cols)
    
    # è·å–æŸ¥è¯¢è®¡åˆ’
    raw_plan = lf.explain()
    optimized_plan = lf.explain(optimized=True)
    
    # åˆ†æä¼˜åŒ–æ•ˆæœ
    bucket_reduction = f"{len(target_buckets)}/{num_buckets} æ¡¶ ({100*len(target_buckets)/num_buckets:.1f}%)"
    column_reduction = f"{len(available_cols)} åˆ—"
    
    return {
        "raw_plan": raw_plan,
        "optimized_plan": optimized_plan,
        "target_buckets": len(target_buckets),
        "total_buckets": num_buckets,
        "bucket_reduction": bucket_reduction,
        "column_reduction": column_reduction,
        "files_to_scan": len(parquet_files),
        # Polars ä½¿ç”¨ SELECTION è¡¨ç¤ºè°“è¯ä¸‹æ¨ï¼ŒFILTER è¡¨ç¤ºåç½®è¿‡æ»¤
        "predicate_pushdown": "SELECTION" in optimized_plan or "selection" in optimized_plan.lower(),
        "column_projection": "PROJECT" in optimized_plan or "project" in optimized_plan.lower()
    }


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='åˆ†æ¡¶è½¬æ¢å™¨')
    subparsers = parser.add_subparsers(dest='command', help='å­å‘½ä»¤')
    
    # è½¬æ¢å‘½ä»¤
    convert_parser = subparsers.add_parser('convert', help='è½¬æ¢æ–‡ä»¶åˆ°åˆ†æ¡¶æ ¼å¼')
    convert_parser.add_argument('source', help='æºæ–‡ä»¶è·¯å¾„')
    convert_parser.add_argument('output', help='è¾“å‡ºç›®å½•')
    convert_parser.add_argument('--buckets', type=int, default=100, help='æ¡¶æ•°é‡')
    convert_parser.add_argument('--column', default='itemid', help='åˆ†æ¡¶åˆ—')
    convert_parser.add_argument('--overwrite', action='store_true', help='è¦†ç›–å·²å­˜åœ¨çš„ç›®å½•')
    convert_parser.add_argument('--temp-dir', help='ä¸´æ—¶ç›®å½•ï¼ˆå»ºè®®SSDï¼‰')
    convert_parser.add_argument('--memory', default='10GB', help='å†…å­˜é™åˆ¶')
    
    # éªŒè¯å‘½ä»¤
    verify_parser = subparsers.add_parser('verify', help='éªŒè¯æŸ¥è¯¢è®¡åˆ’')
    verify_parser.add_argument('bucket_dir', help='åˆ†æ¡¶ç›®å½•')
    verify_parser.add_argument('--itemids', type=int, nargs='+', required=True, help='æµ‹è¯•itemid')
    verify_parser.add_argument('--columns', nargs='+', default=['value'], help='æµ‹è¯•åˆ—')
    verify_parser.add_argument('--buckets', type=int, default=100, help='æ¡¶æ•°é‡')
    
    args = parser.parse_args()
    
    if args.command == 'convert':
        config = BucketConfig(
            num_buckets=args.buckets,
            partition_col=args.column,
            memory_limit=args.memory,
            temp_directory=args.temp_dir
        )
        
        result = convert_to_buckets(
            Path(args.source),
            Path(args.output),
            config,
            overwrite=args.overwrite
        )
        
        if result.success:
            print(f"\nâœ… è½¬æ¢æˆåŠŸ!")
            print(f"   è¾“å‡ºç›®å½•: {result.output_dir}")
            print(f"   æ€»è¡Œæ•°: {result.total_rows:,}")
            print(f"   åˆ†æ¡¶æ•°: {result.num_buckets}")
            print(f"   æ€»å¤§å°: {result.total_size_bytes / 1024**3:.2f} GB")
        else:
            print(f"\nâŒ è½¬æ¢å¤±è´¥: {result.error}")
    
    elif args.command == 'verify':
        result = verify_query_plan(
            Path(args.bucket_dir),
            set(args.itemids),
            args.columns,
            num_buckets=args.buckets
        )
        
        print("\n=== æŸ¥è¯¢è®¡åˆ’éªŒè¯ ===")
        print(f"ç›®æ ‡æ¡¶æ•°: {result['bucket_reduction']}")
        print(f"åˆ—æ•°: {result['column_reduction']}")
        print(f"è°“è¯ä¸‹æ¨ (SELECTION): {'âœ…' if result['predicate_pushdown'] else 'âŒ'}")
        print(f"åˆ—æŠ•å½± (PROJECT): {'âœ…' if result['column_projection'] else 'âŒ'}")
        print(f"\nä¼˜åŒ–åæŸ¥è¯¢è®¡åˆ’:\n{result['optimized_plan']}")
    
    else:
        parser.print_help()
