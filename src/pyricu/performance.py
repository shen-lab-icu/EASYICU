"""
å¤§è§„æ¨¡æ•°æ®åŠ è½½ä¼˜åŒ–æ¨¡å—
æ•´åˆäº†å¤šç§æ€§èƒ½ä¼˜åŒ–ç­–ç•¥ï¼Œå¯ä»¥åœ¨pyricuå†…éƒ¨ä½¿ç”¨
"""

from __future__ import annotations

import logging
import time
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
import multiprocessing as mp

import pandas as pd
import pyarrow.dataset as ds

logger = logging.getLogger(__name__)


class TablePreloader:
    """
    è¡¨æ•°æ®é¢„åŠ è½½å™¨
    
    ä¼˜åŒ–ç­–ç•¥:
    1. ä¸€æ¬¡æ€§è¯»å–å¤§è¡¨ï¼Œé¿å…é‡å¤I/O
    2. åªåŠ è½½å¿…è¦çš„åˆ—
    3. è¿‡æ»¤ç›®æ ‡æ‚£è€…æ•°æ®
    4. ç¼“å­˜åœ¨å†…å­˜ä¸­ä¾›åŽç»­ä½¿ç”¨
    """
    
    def __init__(self, data_path: Path, enable_preload: bool = True):
        self.data_path = data_path
        self.enable_preload = enable_preload
        self.cache: Dict[str, pd.DataFrame] = {}
        self.stats: Dict[str, Dict] = {}
    
    def preload_for_patients(
        self, 
        patient_ids: List[int],
        tables: Optional[Dict[str, List[str]]] = None
    ) -> None:
        """
        ä¸ºæŒ‡å®šæ‚£è€…é¢„åŠ è½½è¡¨æ•°æ®
        
        Args:
            patient_ids: æ‚£è€…IDåˆ—è¡¨
            tables: è¦é¢„åŠ è½½çš„è¡¨åŠå…¶åˆ—ï¼Œæ ¼å¼ä¸º {table_name: [columns]}
                   å¦‚æžœä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤è¡¨åˆ—è¡¨
        """
        if not self.enable_preload:
            logger.info("é¢„åŠ è½½å·²ç¦ç”¨")
            return
        
        # é»˜è®¤é¢„åŠ è½½é…ç½®
        if tables is None:
            tables = self._get_default_tables()
        
        logger.info(f"ðŸ“¦ å¼€å§‹é¢„åŠ è½½ {len(tables)} ä¸ªè¡¨...")
        patient_set = set(patient_ids)
        total_start = time.perf_counter()
        
        for table_name, columns in tables.items():
            start_time = time.perf_counter()
            
            try:
                table_path = self.data_path / f"{table_name}.parquet"
                
                # å¦‚æžœæ˜¯å•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•åˆ†åŒºç›®å½•
                if not table_path.exists():
                    table_path = self.data_path / table_name
                
                # è¯»å–æ•°æ®
                if table_path.is_dir():
                    # åˆ†åŒºè¡¨ - ä½¿ç”¨pyarrow datasetè¯»å–ï¼ˆå¿½ç•¥.fstæ–‡ä»¶ï¼‰
                    dataset = ds.dataset(
                        table_path,
                        format='parquet',
                        partitioning=None,
                        exclude_invalid_files=True
                    )
                    df = dataset.to_table(columns=columns).to_pandas()
                else:
                    # å•æ–‡ä»¶è¡¨
                    df = pd.read_parquet(table_path, columns=columns)
                
                # å¦‚æžœè¡¨æœ‰stay_idåˆ—ï¼Œåªä¿ç•™ç›®æ ‡æ‚£è€…
                if 'stay_id' in df.columns:
                    original_rows = len(df)
                    df = df[df['stay_id'].isin(patient_set)]
                    filtered_rows = len(df)
                    filter_ratio = (1 - filtered_rows/original_rows) * 100 if original_rows > 0 else 0
                else:
                    filtered_rows = len(df)
                    filter_ratio = 0
                
                # ç¼“å­˜æ•°æ®
                self.cache[table_name] = df
                
                elapsed = time.perf_counter() - start_time
                memory_mb = df.memory_usage(deep=True).sum() / 1024 / 1024
                
                self.stats[table_name] = {
                    'rows': filtered_rows,
                    'columns': len(df.columns),
                    'memory_mb': memory_mb,
                    'load_time': elapsed,
                    'filter_ratio': filter_ratio
                }
                
                if filter_ratio > 0:
                    logger.info(
                        f"  âœ… {table_name}: {filtered_rows:,}è¡Œ "
                        f"({filter_ratio:.1f}% è¿‡æ»¤), {memory_mb:.1f}MB, {elapsed:.2f}s"
                    )
                else:
                    logger.info(
                        f"  âœ… {table_name}: {filtered_rows:,}è¡Œ, "
                        f"{memory_mb:.1f}MB, {elapsed:.2f}s"
                    )
                
            except Exception as e:
                logger.warning(f"  âš ï¸  {table_name} è·³è¿‡: {e}")
                self.cache[table_name] = None
        
        total_time = time.perf_counter() - total_start
        total_memory = sum(s['memory_mb'] for s in self.stats.values())
        total_rows = sum(s['rows'] for s in self.stats.values())
        
        logger.info(
            f"\nðŸ“Š é¢„åŠ è½½å®Œæˆ: {total_rows:,}è¡Œ, {total_memory:.1f}MB, "
            f"{total_time:.2f}s ({total_time/len(patient_ids)*1000:.1f}ms/æ‚£è€…)"
        )
    
    def get_table(self, table_name: str) -> Optional[pd.DataFrame]:
        """èŽ·å–é¢„åŠ è½½çš„è¡¨æ•°æ®"""
        return self.cache.get(table_name)
    
    def is_preloaded(self, table_name: str) -> bool:
        """æ£€æŸ¥è¡¨æ˜¯å¦å·²é¢„åŠ è½½"""
        return table_name in self.cache and self.cache[table_name] is not None
    
    def clear_cache(self):
        """æ¸…é™¤ç¼“å­˜"""
        self.cache.clear()
        self.stats.clear()
    
    @staticmethod
    def _get_default_tables() -> Dict[str, List[str]]:
        """
        èŽ·å–é»˜è®¤çš„é¢„åŠ è½½è¡¨é…ç½®
        åªåŒ…å«æœ‰stay_idçš„å¤§è¡¨ï¼Œä»¥åŠå¿…è¦çš„å­—å…¸è¡¨
        """
        return {
            'chartevents': ['stay_id', 'charttime', 'itemid', 'valuenum', 'valueuom', 'value'],
            # labeventsæ²¡æœ‰stay_idï¼Œéœ€è¦å…³è”ï¼Œä¸é¢„åŠ è½½
            'outputevents': ['stay_id', 'charttime', 'itemid', 'value'],
            'procedureevents': ['stay_id', 'starttime', 'itemid', 'value'],
            'datetimeevents': ['stay_id', 'charttime', 'itemid', 'value'],
            'icustays': ['stay_id', 'hadm_id', 'subject_id', 'intime', 'outtime', 'los'],
            'd_items': ['itemid', 'label', 'category'],
        }


class BatchProcessor:
    """
    æ‰¹å¤„ç†å™¨
    
    ä¼˜åŒ–ç­–ç•¥:
    1. å°†å¤§é‡æ‚£è€…åˆ†æˆå°æ‰¹æ¬¡
    2. æŽ§åˆ¶å†…å­˜ä½¿ç”¨
    3. æ”¯æŒå¹¶è¡Œå¤„ç†
    4. å¢žé‡ä¿å­˜ç»“æžœ
    """
    
    def __init__(
        self,
        batch_size: int = 100,
        enable_parallel: bool = True,
        num_workers: Optional[int] = None
    ):
        self.batch_size = batch_size
        self.enable_parallel = enable_parallel
        
        if num_workers is None:
            # é»˜è®¤ä½¿ç”¨CPUæ ¸å¿ƒæ•°-1ï¼Œè‡³å°‘ä¸º1
            self.num_workers = max(1, mp.cpu_count() - 1)
        else:
            self.num_workers = num_workers
    
    def create_batches(self, patient_ids: List[int]) -> List[List[int]]:
        """å°†æ‚£è€…IDåˆ—è¡¨åˆ†æˆæ‰¹æ¬¡"""
        batches = []
        for i in range(0, len(patient_ids), self.batch_size):
            batches.append(patient_ids[i:i+self.batch_size])
        return batches
    
    def process_batches(
        self,
        batches: List[List[int]],
        process_func: callable,
        **kwargs
    ) -> List:
        """
        å¤„ç†æ‰¹æ¬¡
        
        Args:
            batches: æ‚£è€…IDæ‰¹æ¬¡åˆ—è¡¨
            process_func: å¤„ç†å‡½æ•°ï¼Œç­¾åä¸º func(batch_ids, batch_idx, **kwargs)
            **kwargs: ä¼ é€’ç»™process_funcçš„é¢å¤–å‚æ•°
        
        Returns:
            å¤„ç†ç»“æžœåˆ—è¡¨
        """
        total_batches = len(batches)
        results = []
        
        logger.info(
            f"ðŸ”„ å¼€å§‹æ‰¹å¤„ç†: {total_batches}ä¸ªæ‰¹æ¬¡, "
            f"{'å¹¶è¡Œ' if self.enable_parallel and self.num_workers > 1 else 'ä¸²è¡Œ'}æ¨¡å¼"
        )
        
        if self.enable_parallel and self.num_workers > 1:
            # å¹¶è¡Œå¤„ç†
            with ThreadPoolExecutor(max_workers=self.num_workers) as executor:
                futures = {}
                
                for idx, batch in enumerate(batches):
                    future = executor.submit(
                        process_func,
                        batch,
                        idx,
                        total_batches,
                        **kwargs
                    )
                    futures[future] = idx
                
                # æ”¶é›†ç»“æžœ
                for future in as_completed(futures):
                    try:
                        result = future.result()
                        results.append(result)
                    except Exception as e:
                        batch_idx = futures[future]
                        logger.error(f"æ‰¹æ¬¡ {batch_idx+1}/{total_batches} å¤±è´¥: {e}")
                        results.append(None)
        else:
            # ä¸²è¡Œå¤„ç†
            for idx, batch in enumerate(batches):
                try:
                    result = process_func(batch, idx, total_batches, **kwargs)
                    results.append(result)
                except Exception as e:
                    logger.error(f"æ‰¹æ¬¡ {idx+1}/{total_batches} å¤±è´¥: {e}")
                    results.append(None)
        
        # è¿‡æ»¤å¤±è´¥çš„æ‰¹æ¬¡
        results = [r for r in results if r is not None]
        
        logger.info(f"âœ… æ‰¹å¤„ç†å®Œæˆ: {len(results)}/{total_batches} ä¸ªæ‰¹æ¬¡æˆåŠŸ")
        
        return results


class PerformanceOptimizer:
    """
    æ€§èƒ½ä¼˜åŒ–å™¨ä¸»ç±»
    æ•´åˆé¢„åŠ è½½ã€æ‰¹å¤„ç†ç­‰ä¼˜åŒ–ç­–ç•¥
    """
    
    def __init__(
        self,
        data_path: Path,
        enable_preload: bool = True,
        enable_batch: bool = True,
        batch_size: int = 100,
        num_workers: Optional[int] = None
    ):
        self.data_path = data_path
        self.enable_preload = enable_preload
        self.enable_batch = enable_batch
        
        self.preloader = TablePreloader(data_path, enable_preload)
        self.batch_processor = BatchProcessor(batch_size, enable_batch, num_workers)
    
    def optimize_loading(
        self,
        patient_ids: List[int],
        preload_tables: Optional[Dict[str, List[str]]] = None
    ) -> None:
        """
        ä¼˜åŒ–æ•°æ®åŠ è½½
        
        Args:
            patient_ids: æ‚£è€…IDåˆ—è¡¨
            preload_tables: è¦é¢„åŠ è½½çš„è¡¨é…ç½®
        """
        if self.enable_preload and len(patient_ids) >= 100:
            # åªæœ‰æ‚£è€…æ•°é‡è¶³å¤Ÿå¤šæ—¶æ‰é¢„åŠ è½½
            logger.info(f"ðŸš€ å¯ç”¨é¢„åŠ è½½ä¼˜åŒ–ï¼ˆ{len(patient_ids)}åæ‚£è€…ï¼‰")
            self.preloader.preload_for_patients(patient_ids, preload_tables)
        else:
            logger.info("é¢„åŠ è½½ä¼˜åŒ–å·²ç¦ç”¨æˆ–æ‚£è€…æ•°é‡è¿‡å°‘")
    
    def get_preloaded_table(self, table_name: str) -> Optional[pd.DataFrame]:
        """èŽ·å–é¢„åŠ è½½çš„è¡¨æ•°æ®"""
        return self.preloader.get_table(table_name)
    
    def is_preloaded(self, table_name: str) -> bool:
        """æ£€æŸ¥è¡¨æ˜¯å¦å·²é¢„åŠ è½½"""
        return self.preloader.is_preloaded(table_name)
    
    def create_batches(self, patient_ids: List[int]) -> List[List[int]]:
        """åˆ›å»ºæ‰¹æ¬¡"""
        return self.batch_processor.create_batches(patient_ids)
    
    def process_batches(self, batches: List[List[int]], process_func: callable, **kwargs) -> List:
        """å¤„ç†æ‰¹æ¬¡"""
        return self.batch_processor.process_batches(batches, process_func, **kwargs)
    
    def clear_cache(self):
        """æ¸…é™¤ç¼“å­˜"""
        self.preloader.clear_cache()


# å…¨å±€ä¼˜åŒ–å™¨å®žä¾‹
_global_optimizer: Optional[PerformanceOptimizer] = None


def get_optimizer(
    data_path: Path,
    enable_preload: bool = True,
    enable_batch: bool = True,
    batch_size: int = 100,
    num_workers: Optional[int] = None
) -> PerformanceOptimizer:
    """èŽ·å–æˆ–åˆ›å»ºå…¨å±€ä¼˜åŒ–å™¨å®žä¾‹"""
    global _global_optimizer
    
    if _global_optimizer is None:
        _global_optimizer = PerformanceOptimizer(
            data_path,
            enable_preload,
            enable_batch,
            batch_size,
            num_workers
        )
    
    return _global_optimizer


def enable_performance_optimization(
    data_path: Path,
    patient_ids: List[int],
    preload_tables: Optional[Dict[str, List[str]]] = None,
    batch_size: int = 100,
    num_workers: Optional[int] = None
) -> PerformanceOptimizer:
    """
    å¯ç”¨æ€§èƒ½ä¼˜åŒ–
    
    åœ¨åŠ è½½å¤§é‡æ‚£è€…æ•°æ®å‰è°ƒç”¨æ­¤å‡½æ•°ï¼Œå¯ä»¥æ˜¾è‘—æå‡æ€§èƒ½
    
    Args:
        data_path: æ•°æ®è·¯å¾„
        patient_ids: æ‚£è€…IDåˆ—è¡¨
        preload_tables: è¦é¢„åŠ è½½çš„è¡¨é…ç½®
        batch_size: æ‰¹æ¬¡å¤§å°
        num_workers: å¹¶è¡Œworkeræ•°é‡
    
    Returns:
        ä¼˜åŒ–å™¨å®žä¾‹
    
    Example:
        >>> from pyricu.performance import enable_performance_optimization
        >>> from pyricu import load_concepts
        >>>
        >>> # å¯ç”¨ä¼˜åŒ–
        >>> optimizer = enable_performance_optimization(
        ...     data_path=Path("/path/to/data"),
        ...     patient_ids=list(range(1000)),  # 1000åæ‚£è€…
        ...     batch_size=100,
        ...     num_workers=8
        ... )
        >>>
        >>> # åŠ è½½æ•°æ®ï¼ˆè‡ªåŠ¨ä½¿ç”¨ä¼˜åŒ–ï¼‰
        >>> sofa = load_concepts('sofa', patient_ids=list(range(1000)))
    """
    optimizer = get_optimizer(
        data_path,
        enable_preload=True,
        enable_batch=True,
        batch_size=batch_size,
        num_workers=num_workers
    )
    
    optimizer.optimize_loading(patient_ids, preload_tables)
    
    return optimizer
