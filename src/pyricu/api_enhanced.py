"""
Enhanced API with caching and time alignment support
"""
from typing import List, Union, Optional, Dict
from pathlib import Path
import pandas as pd
import pickle
import hashlib
from datetime import datetime

from .concept import ConceptDictionary, ConceptResolver
from .datasource import ICUDataSource
from .config import DataSourceConfig
from .resources import load_data_sources, load_dictionary


def _get_cache_key(concepts: List[str], source: str, **kwargs) -> str:
    """Generate cache key from parameters."""
    key_str = f"{source}_{','.join(sorted(concepts))}_{str(sorted(kwargs.items()))}"
    return hashlib.md5(key_str.encode()).hexdigest()


def load_concept_cached(
    concepts: Union[str, List[str]],
    source: str,
    data_path: Union[str, Path],
    cache_dir: Optional[Union[str, Path]] = None,
    force_reload: bool = False,
    patient_ids: Optional[List] = None,
    merge: bool = True,
    align_time: bool = False,  # NEW: align to ICU admission time
    verbose: bool = True,
    use_pickle: bool = True,  # NEW: use pickle instead of CSV
    n_patients: Optional[int] = None,  # NEW: sample N patients for testing
    **kwargs,
) -> Union[pd.DataFrame, Dict[str, pd.DataFrame]]:
    """
    Load ICU concept data with caching support.
    
    Args:
        concepts: Concept name(s) to load
        source: Data source name ('mimic', 'miiv', etc.)
        data_path: Path to data source files
        cache_dir: Directory for cache files (default: data_path/cache)
        force_reload: If True, ignore cache and reload from source
        patient_ids: Optional patient ID filter
        merge: If True, merge concepts into wide format
        align_time: If True, align charttime to ICU admission (hours since admission)
        verbose: Show progress messages
        use_pickle: If True, cache as pickle; if False, use CSV
        n_patients: If provided, randomly sample N patients (for testing)
        **kwargs: Additional parameters for concept resolver
        
    Returns:
        DataFrame with concept data (and optionally time-aligned)
    """
    # Setup cache directory
    if cache_dir is None:
        cache_dir = Path(data_path) / "cache"
    cache_dir = Path(cache_dir)
    cache_dir.mkdir(parents=True, exist_ok=True)
    
    # Prepare concept list
    if isinstance(concepts, str):
        concept_list = [concepts]
    else:
        concept_list = list(concepts)
    
    # Generate cache key
    cache_params = {
        'merge': merge,
        'align_time': align_time,
        **kwargs
    }
    cache_key = _get_cache_key(concept_list, source, **cache_params)
    cache_ext = 'pkl' if use_pickle else 'csv'
    cache_file = cache_dir / f"{source}_{'_'.join(concept_list[:3])}_{cache_key[:8]}.{cache_ext}"
    
    # Try to load from cache
    if not force_reload and cache_file.exists():
        if verbose:
            print(f"ğŸ“¦ ä»ç¼“å­˜åŠ è½½: {cache_file.name}")
        try:
            if use_pickle:
                with open(cache_file, 'rb') as f:
                    result = pickle.load(f)
            else:
                result = pd.read_csv(cache_file, parse_dates=['charttime'])
            
            if verbose:
                if isinstance(result, pd.DataFrame):
                    print(f"âœ… æˆåŠŸåŠ è½½ {len(result):,} è¡Œç¼“å­˜æ•°æ®")
                else:
                    print(f"âœ… æˆåŠŸåŠ è½½ {len(result)} ä¸ªæ¦‚å¿µçš„ç¼“å­˜æ•°æ®")
            return result
        except Exception as e:
            if verbose:
                print(f"âš ï¸  ç¼“å­˜åŠ è½½å¤±è´¥: {e}ï¼Œé‡æ–°æå–...")
    
    # Load from source
    if verbose:
        print(f"ğŸ“Š ä» {source.upper()} æå– {len(concept_list)} ä¸ªæ¦‚å¿µ...")
        if concept_list:
            print(f"   æ¦‚å¿µ: {', '.join(concept_list)}")
    
    # Load data source config
    registry = load_data_sources()
    if source not in registry:
        available = [cfg.name for cfg in registry]
        raise ValueError(f"æœªçŸ¥æ•°æ®æº '{source}'ã€‚å¯ç”¨: {available}")
    
    source_config = registry.get(source)
    datasource = ICUDataSource(config=source_config, base_path=Path(data_path))
    
    # Load dictionary and create resolver
    dict_obj = load_dictionary()
    resolver = ConceptResolver(dict_obj)
    
    # Handle patient sampling for testing
    if n_patients is not None and patient_ids is None:
        if verbose:
            print(f"ğŸ² éšæœºé‡‡æ · {n_patients} ä¸ªæ‚£è€…è¿›è¡Œæµ‹è¯•...")
        
        # Load patient/stay IDs from icustays or similar table
        try:
            if source in ['miiv', 'mimic']:
                icu_table = datasource.load_table('icustays')
                # MIMIC-IVéœ€è¦åŒæ—¶è·å–stay_idå’Œsubject_idç”¨äºè¿‡æ»¤ä¸åŒçš„è¡¨
                # - charteventsç­‰ä½¿ç”¨stay_id
                # - labeventsç­‰ä½¿ç”¨subject_id
                if hasattr(icu_table, 'data'):
                    all_stay_ids = icu_table.data['stay_id'].unique()
                    if len(all_stay_ids) > n_patients:
                        import numpy as np
                        np.random.seed(42)  # å¯é‡ç°çš„éšæœºé‡‡æ ·
                        sampled_stay_ids = np.random.choice(all_stay_ids, n_patients, replace=False)
                        
                        # è·å–å¯¹åº”çš„subject_id
                        sampled_df = icu_table.data[icu_table.data['stay_id'].isin(sampled_stay_ids)]
                        patient_ids = {
                            'stay_id': sampled_stay_ids.tolist(),
                            'subject_id': sampled_df['subject_id'].unique().tolist()
                        }
                        if verbose:
                            print(f"   é‡‡æ ·äº† {len(patient_ids['stay_id'])} ä¸ªstay_id, {len(patient_ids['subject_id'])} ä¸ªsubject_id")
                    else:
                        patient_ids = {
                            'stay_id': all_stay_ids.tolist(),
                            'subject_id': icu_table.data['subject_id'].unique().tolist()
                        }
                        if verbose:
                            print(f"   æ€»å…± {len(patient_ids['stay_id'])} ä¸ªstay_id")
                else:
                    patient_ids = None
            elif source == 'eicu':
                icu_table = datasource.load_table('patient')
                id_col = 'patientunitstayid'
                if icu_table is not None and hasattr(icu_table, 'data'):
                    all_ids = icu_table.data[id_col].unique()
                    if len(all_ids) > n_patients:
                        import numpy as np
                        np.random.seed(42)
                        patient_ids = np.random.choice(all_ids, n_patients, replace=False).tolist()
                        if verbose:
                            print(f"   é‡‡æ ·äº† {len(patient_ids)} ä¸ªæ‚£è€…ID")
                    else:
                        patient_ids = all_ids.tolist()
            elif source == 'hirid':
                icu_table = datasource.load_table('general_table')
                id_col = 'patientid'
                if icu_table is not None and hasattr(icu_table, 'data'):
                    all_ids = icu_table.data[id_col].unique()
                    if len(all_ids) > n_patients:
                        import numpy as np
                        np.random.seed(42)
                        patient_ids = np.random.choice(all_ids, n_patients, replace=False).tolist()
                        if verbose:
                            print(f"   é‡‡æ ·äº† {len(patient_ids)} ä¸ªæ‚£è€…ID")
                    else:
                        patient_ids = all_ids.tolist()
            else:
                patient_ids = None
        except Exception as e:
            if verbose:
                print(f"   âš ï¸  é‡‡æ ·å¤±è´¥: {e}ï¼Œå°†åŠ è½½å…¨éƒ¨æ•°æ®")
            patient_ids = None
    
    # Load concepts
    result = resolver.load_concepts(
        concept_list,
        datasource,
        patient_ids=patient_ids,
        merge=merge,
        **kwargs,
    )
    
    # Time alignment if requested
    if align_time:
        result = align_to_icu_admission(result, datasource, source, verbose=verbose)
    
    # Save to cache
    try:
        if use_pickle:
            with open(cache_file, 'wb') as f:
                pickle.dump(result, f, protocol=pickle.HIGHEST_PROTOCOL)
        else:
            if isinstance(result, pd.DataFrame):
                result.to_csv(cache_file, index=False)
            else:
                # Can't easily cache dict to single CSV
                if verbose:
                    print("âš ï¸  å­—å…¸ç»“æœæœªç¼“å­˜ï¼ˆä»…æ”¯æŒåˆå¹¶çš„DataFrameï¼‰")
        
        if verbose:
            print(f"ğŸ’¾ å·²ç¼“å­˜åˆ°: {cache_file.name}")
    except Exception as e:
        if verbose:
            print(f"âš ï¸  ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    if verbose:
        if isinstance(result, pd.DataFrame):
            print(f"âœ… æˆåŠŸæå– {len(result):,} è¡Œæ•°æ®")
        else:
            print(f"âœ… æˆåŠŸæå– {len(result)} ä¸ªæ¦‚å¿µ")
    
    return result


def align_to_icu_admission(
    data: Union[pd.DataFrame, Dict[str, pd.DataFrame]],
    datasource: ICUDataSource,
    source: str,
    aggregate_hourly: bool = True,  # NEW: èšåˆåˆ°æ¯å°æ—¶ä¸€è¡Œ
    agg_func: str = 'median',  # NEW: èšåˆå‡½æ•° (median, mean, min, max)
    filter_icu_window: bool = True,  # NEW: è¿‡æ»¤åˆ°ICUæ—¶é—´çª—å£
    before_icu_hours: int = 0,  # NEW: å…¥ICUå‰ä¿ç•™çš„å°æ—¶æ•°
    after_icu_hours: int = 0,  # NEW: å‡ºICUåä¿ç•™çš„å°æ—¶æ•°
    verbose: bool = True,
) -> Union[pd.DataFrame, Dict[str, pd.DataFrame]]:
    """
    Align charttime to ICU admission time and aggregate to hourly intervals.
    æ ¹æ®ricuçš„stay_windowsé€»è¾‘ï¼Œé»˜è®¤åªä¿ç•™ICUä½é™¢æœŸé—´çš„æ•°æ®ã€‚
    
    Args:
        data: Concept data with charttime
        datasource: Data source instance
        source: Data source name
        aggregate_hourly: If True, aggregate multiple measurements per hour
        agg_func: Aggregation function ('median', 'mean', 'min', 'max')
        filter_icu_window: If True, filter to ICU stay window (default: True)
        before_icu_hours: Hours before ICU admission to include (default: 0)
        after_icu_hours: Hours after ICU discharge to include (default: 0)
        verbose: Show progress
        
    Returns:
        Data with charttime as integer hours since ICU admission, one row per hour
    """
    if verbose:
        print("â° å¯¹é½æ—¶é—´åˆ°ICUå…¥é™¢æ—¶é—´...")
    
    # Handle dict of DataFrames
    if isinstance(data, dict):
        return {
            name: align_to_icu_admission(df, datasource, source, aggregate_hourly, agg_func, 
                                        filter_icu_window, before_icu_hours, after_icu_hours, verbose=False)
            for name, df in data.items()
        }
    
    # Get ICU stay information (admission time)
    try:
        # Try to load icustays table
        if source in ['miiv', 'mimic']:
            icu_table_obj = datasource.load_table('icustays')
            id_col = 'stay_id'
            time_col_in = 'intime'
            time_col_out = 'outtime'
        elif source == 'eicu':
            icu_table_obj = datasource.load_table('patient')
            id_col = 'patientunitstayid'
            time_col_in = 'hospitaladmittime24'  # or unitadmittime24
            time_col_out = 'unitdischargetime24'
        elif source == 'hirid':
            icu_table_obj = datasource.load_table('general_table')
            id_col = 'patientid'
            time_col_in = 'admissiontime'
            time_col_out = 'dischargetime'
        else:
            if verbose:
                print(f"âš ï¸  æ•°æ®æº '{source}' ä¸æ”¯æŒæ—¶é—´å¯¹é½")
            return data
        
        # Extract DataFrame from ICUTable
        if hasattr(icu_table_obj, 'data'):
            icu_table = icu_table_obj.data
        else:
            icu_table = icu_table_obj
        
        # Ensure datetime types
        if time_col_in in icu_table.columns:
            icu_table[time_col_in] = pd.to_datetime(icu_table[time_col_in], errors='coerce')
        if time_col_out in icu_table.columns:
            icu_table[time_col_out] = pd.to_datetime(icu_table[time_col_out], errors='coerce')
        
        # Get admission and discharge times
        admission_times = icu_table[[id_col, time_col_in, time_col_out]].rename(
            columns={id_col: 'stay_id', time_col_in: 'admission_time', time_col_out: 'discharge_time'}
        )
        
        # Merge with data
        if 'stay_id' not in data.columns:
            if verbose:
                print("âš ï¸  æ•°æ®ä¸­æ²¡æœ‰ stay_id åˆ—ï¼Œè·³è¿‡æ—¶é—´å¯¹é½")
            return data
        
        aligned = data.merge(admission_times, on='stay_id', how='left')
        
        # Calculate hours since admission
        if 'charttime' in aligned.columns and 'admission_time' in aligned.columns:
            aligned['charttime'] = pd.to_datetime(aligned['charttime'], errors='coerce')
            aligned['admission_time'] = pd.to_datetime(aligned['admission_time'], errors='coerce')
            aligned['discharge_time'] = pd.to_datetime(aligned['discharge_time'], errors='coerce')
            
            time_diff = aligned['charttime'] - aligned['admission_time']
            hours_float = time_diff.dt.total_seconds() / 3600
            
            # Apply ICU window filter (ç±»ä¼¼ricuçš„stay_windowsé€»è¾‘)
            if filter_icu_window:
                # è®¡ç®—ICUä½é™¢æ—¶é•¿ï¼ˆå°æ—¶ï¼‰
                icu_los = (aligned['discharge_time'] - aligned['admission_time']).dt.total_seconds() / 3600
                
                # è¿‡æ»¤æ¡ä»¶ï¼š-before_icu_hours <= hours_since_admission <= icu_los + after_icu_hours
                lower_bound = -before_icu_hours
                upper_bound = icu_los + after_icu_hours
                
                mask = (hours_float >= lower_bound) & (hours_float <= upper_bound)
                before_filter = len(aligned)
                aligned = aligned[mask]
                after_filter = len(aligned)
                
                if verbose:
                    filtered = before_filter - after_filter
                    print(f"   ğŸªŸ ICUæ—¶é—´çª—å£è¿‡æ»¤: [{-before_icu_hours}h åˆ° å‡ºICU+{after_icu_hours}h]")
                    print(f"      è¿‡æ»¤å‰: {before_filter:,} è¡Œ")
                    print(f"      è¿‡æ»¤å: {after_filter:,} è¡Œ") 
                    if before_filter > 0:
                        print(f"      è¿‡æ»¤æ‰: {filtered:,} è¡Œ ({filtered/before_filter*100:.1f}%)")
                
                # é‡æ–°è®¡ç®—hours_float (å› ä¸ºè¿‡æ»¤åå¯èƒ½æœ‰å˜åŒ–)
                time_diff = aligned['charttime'] - aligned['admission_time']
                hours_float = time_diff.dt.total_seconds() / 3600
            
            # Round to nearest hour (floor)
            aligned['hours_since_admission'] = hours_float.apply(lambda x: int(x) if pd.notna(x) else None)
            
            # Drop original time columns
            aligned = aligned.drop(columns=['charttime', 'admission_time', 'discharge_time'])
            
            # Aggregate to hourly if requested
            if aggregate_hourly:
                value_cols = [col for col in aligned.columns if col not in ['stay_id', 'hours_since_admission']]
                
                if value_cols:
                    group_cols = ['stay_id', 'hours_since_admission']
                    
                    # Build aggregation dict
                    agg_dict = {}
                    for col in value_cols:
                        if aligned[col].dtype in ['float64', 'int64', 'float32', 'int32']:
                            agg_dict[col] = agg_func
                        else:
                            agg_dict[col] = 'first'  # Non-numeric: take first
                    
                    aligned = aligned.groupby(group_cols, as_index=False).agg(agg_dict)
                    
                    if verbose:
                        print(f"   âœ… æ—¶é—´å·²å¯¹é½å¹¶èšåˆåˆ°æ¯å°æ—¶ä¸€è¡Œ (ä½¿ç”¨ {agg_func})")
            else:
                if verbose:
                    print(f"   âœ… æ—¶é—´å·²å¯¹é½ä¸ºå…¥é™¢åå°æ—¶æ•°")
            
            # Rename to charttime for consistency
            aligned = aligned.rename(columns={'hours_since_admission': 'charttime'})
            
            if verbose:
                if len(aligned) > 0:
                    print(f"      æ—¶é—´èŒƒå›´: {aligned['charttime'].min():.0f}h - {aligned['charttime'].max():.0f}h")
                    print(f"      æ•°æ®å½¢çŠ¶: {aligned.shape}")
                else:
                    print(f"      âš ï¸  è¿‡æ»¤åæ— æ•°æ®")
        
        return aligned
        
    except Exception as e:
        if verbose:
            print(f"âš ï¸  æ—¶é—´å¯¹é½å¤±è´¥: {e}ï¼Œè¿”å›åŸå§‹æ•°æ®")
        import traceback
        traceback.print_exc()
        return data


def load_sofa_with_score(
    source: str,
    data_path: Union[str, Path],
    cache_dir: Optional[Union[str, Path]] = None,
    force_reload: bool = False,
    align_time: bool = True,
    win_length_hours: int = 24,
    n_patients: Optional[int] = None,  # NEW: sample for testing
    verbose: bool = True,
) -> pd.DataFrame:
    """
    Load SOFA components and calculate SOFA scores.
    
    æ ¹æ® ricu çš„å®šä¹‰ï¼ŒSOFA è¯„åˆ†éœ€è¦6ä¸ªç»„ä»¶ï¼š
    - sofa_resp:   pafi + vent_ind
    - sofa_coag:   plt
    - sofa_liver:  bili
    - sofa_cardio: map + dopa60 + norepi60 + dobu60 + epi60
    - sofa_cns:    gcs
    - sofa_renal:  crea + urine24
    
    Args:
        source: Data source name
        data_path: Path to data files
        cache_dir: Cache directory
        force_reload: Force reload from source
        align_time: Align to ICU admission time
        win_length_hours: Window length for worst value calculation (default: 24)
        n_patients: Sample N patients for testing (None = all patients)
        verbose: Show progress
        
    Returns:
        DataFrame with SOFA components and total SOFA score
    """
    if verbose:
        print("=" * 70)
        print("åŠ è½½ SOFA ç»„ä»¶")
        print("=" * 70)
    
    # SOFAå®Œæ•´ä¾èµ– (åŸºäºricuå®šä¹‰)
    # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å…ˆæå–åŸºç¡€æŒ‡æ ‡
    basic_concepts = [
        # Respiratory
        'pafi', 'vent_ind',
        # Coagulation  
        'plt',
        # Liver
        'bili',
        # Cardiovascular
        'map', 
        # 'dopa60', 'norepi60', 'dobu60', 'epi60',  # æš‚æ—¶è·³è¿‡è¯ç‰©
        # CNS
        'gcs',
        # Renal
        'crea', 
        # 'urine24',  # æš‚æ—¶è·³è¿‡å°¿é‡
    ]
    
    if verbose:
        print(f"\næå–åŸºç¡€ SOFA æŒ‡æ ‡:")
        print(f"  {', '.join(basic_concepts)}")
        if n_patients:
            print(f"  é‡‡æ · {n_patients} ä¸ªæ‚£è€…è¿›è¡Œæµ‹è¯•")
    
    # Load components
    sofa_data = load_concept_cached(
        basic_concepts,
        source,
        data_path,
        cache_dir=cache_dir,
        force_reload=force_reload,
        merge=True,
        align_time=align_time,
        n_patients=n_patients,  # ä¼ é€’é‡‡æ ·å‚æ•°
        verbose=verbose,
        use_pickle=True,
    )
    
    if verbose:
        print(f"\nâœ… SOFA åŸºç¡€æŒ‡æ ‡æå–å®Œæˆ: {sofa_data.shape}")
        print(f"   åˆ—: {list(sofa_data.columns)}")
    
    # æ³¨æ„: å®Œæ•´çš„ SOFA è¯„åˆ†è®¡ç®—éœ€è¦å®ç°æ»‘åŠ¨çª—å£å’Œç»„ä»¶è¯„åˆ†å‡½æ•°
    # è¿™é‡Œè¿”å›çš„æ˜¯åŸå§‹æŒ‡æ ‡æ•°æ®
    return sofa_data


__all__ = [
    'load_concept_cached',
    'align_to_icu_admission',
    'load_sofa_with_score',
]
