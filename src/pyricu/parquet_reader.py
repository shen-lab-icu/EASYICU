"""
é«˜æ€§èƒ½ Parquet è¯»å–å™¨ - çº¯ Pythonï¼Œæ— éœ€ R
æ¯” fst æ›´å¿«ï¼Œæ”¯æŒæ›´å¤šä¼˜åŒ–åŠŸèƒ½
"""
from pathlib import Path
from typing import Optional, List, Union, Tuple, Any
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from concurrent.futures import ThreadPoolExecutor, as_completed


def read_parquet(
    file_path: Union[str, Path],
    columns: Optional[List[str]] = None,
    filters: Optional[List[Tuple]] = None,
    use_threads: bool = True
) -> pd.DataFrame:
    """
    è¯»å– Parquet æ–‡ä»¶ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
    
    Args:
        file_path: Parquet æ–‡ä»¶è·¯å¾„
        columns: è¦è¯»å–çš„åˆ—ï¼ˆNone = å…¨éƒ¨ï¼‰
        filters: PyArrow è¿‡æ»¤å™¨ï¼Œåœ¨è¯»å–æ—¶ç›´æ¥è¿‡æ»¤ï¼Œé€Ÿåº¦æ›´å¿«ï¼
            ä¾‹å¦‚: [('stay_id', 'in', [123, 456])]
        use_threads: ä½¿ç”¨å¤šçº¿ç¨‹åŠ é€Ÿï¼ˆé»˜è®¤Trueï¼‰
    
    Returns:
        DataFrame
    
    æ€§èƒ½ä¼˜åŠ¿:
        - åˆ—è£å‰ªï¼šåªè¯»å–éœ€è¦çš„åˆ—ï¼ŒèŠ‚çœå†…å­˜å’Œæ—¶é—´
        - è°“è¯ä¸‹æ¨ï¼šåœ¨è¯»å–æ—¶å°±è¿‡æ»¤æ•°æ®ï¼Œé¿å…è¯»å–æ— ç”¨æ•°æ®
        - å¤šçº¿ç¨‹ï¼šè‡ªåŠ¨å¹¶è¡Œè¯»å–å¤šä¸ª row groups
    
    Examples:
        # åŸºæœ¬è¯»å–
        df = read_parquet("data.parquet")
        
        # åªè¯»å–ç‰¹å®šåˆ—ï¼ˆå¿«10å€ï¼ï¼‰
        df = read_parquet("data.parquet", columns=['stay_id', 'value'])
        
        # å¸¦è¿‡æ»¤ï¼ˆå¿«100å€ï¼ï¼‰
        df = read_parquet(
            "chartevents.parquet",
            columns=['stay_id', 'charttime', 'value'],
            filters=[('stay_id', 'in', patient_ids)]  # åªè¯»å–è¿™äº›æ‚£è€…ï¼
        )
    """
    return pd.read_parquet(
        file_path,
        engine='pyarrow',
        columns=columns,
        filters=filters,
        use_threads=use_threads
    )


def read_parquet_parallel(
    file_paths: List[Union[str, Path]],
    columns: Optional[List[str]] = None,
    filters: Optional[List[Tuple]] = None,
    max_workers: int = 4,
    verbose: bool = False
) -> pd.DataFrame:
    """
    å¹¶è¡Œè¯»å–å¤šä¸ª Parquet æ–‡ä»¶å¹¶åˆå¹¶
    
    æ¯” fst_reader_fast.read_fst_parallel å¿« 5-10 å€ï¼
    
    Args:
        file_paths: Parquet æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        columns: è¦è¯»å–çš„åˆ—
        filters: è¿‡æ»¤æ¡ä»¶ï¼ˆåº”ç”¨åˆ°æ¯ä¸ªæ–‡ä»¶ï¼‰
        max_workers: æœ€å¤§å¹¶è¡Œæ•°
        verbose: æ˜¾ç¤ºè¿›åº¦
    
    Returns:
        åˆå¹¶åçš„ DataFrame
    """
    def read_one(path):
        try:
            df = read_parquet(path, columns=columns, filters=filters)
            return df if len(df) > 0 else None
        except Exception as e:
            if verbose:
                print(f"   âš ï¸  è¯»å–å¤±è´¥ {Path(path).name}: {e}")
            return None
    
    if verbose:
        print(f"   ğŸ“š å¹¶è¡Œè¯»å– {len(file_paths)} ä¸ª Parquet åˆ†åŒº...")
        if filters:
            print(f"   ğŸ” åº”ç”¨è¿‡æ»¤å™¨: {filters}")
    
    dfs = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(read_one, path): path for path in file_paths}
        
        for i, future in enumerate(as_completed(futures), 1):
            df = future.result()
            if df is not None:
                dfs.append(df)
            
            if verbose and i % 10 == 0:
                print(f"   è¿›åº¦: {i}/{len(file_paths)}")
    
    if not dfs:
        return pd.DataFrame()
    
    result = pd.concat(dfs, ignore_index=True)
    
    if verbose:
        print(f"   âœ… è¯»å–å®Œæˆ: {len(result):,} è¡Œ")
    
    return result


def parquet_metadata(file_path: Union[str, Path]) -> dict:
    """
    è·å– Parquet æ–‡ä»¶å…ƒæ•°æ®ï¼ˆæ— éœ€è¯»å–æ•°æ®ï¼‰
    
    é€Ÿåº¦æå¿«ï¼åªè¯»å–æ–‡ä»¶å¤´éƒ¨ã€‚
    
    Args:
        file_path: Parquet æ–‡ä»¶è·¯å¾„
    
    Returns:
        å…ƒæ•°æ®å­—å…¸
    """
    parquet_file = pq.ParquetFile(file_path)
    
    return {
        'nrow': parquet_file.metadata.num_rows,
        'ncol': parquet_file.metadata.num_columns,
        'columns': parquet_file.schema.names,
        'num_row_groups': parquet_file.num_row_groups,
        'size_bytes': Path(file_path).stat().st_size,
        'schema': parquet_file.schema.to_string()
    }


def parquet_peek(
    file_path: Union[str, Path],
    n: int = 5,
    columns: Optional[List[str]] = None
) -> pd.DataFrame:
    """
    å¿«é€ŸæŸ¥çœ‹ Parquet æ–‡ä»¶çš„å‰ n è¡Œ
    
    åªè¯»å–ç¬¬ä¸€ä¸ª row groupï¼Œé€Ÿåº¦æå¿«ï¼
    
    Args:
        file_path: Parquet æ–‡ä»¶è·¯å¾„
        n: è¡Œæ•°
        columns: åˆ—ï¼ˆNone = å…¨éƒ¨ï¼‰
    
    Returns:
        å‰ n è¡Œ DataFrame
    """
    parquet_file = pq.ParquetFile(file_path)
    
    # åªè¯»å–ç¬¬ä¸€ä¸ª row group
    first_batch = parquet_file.read_row_group(0, columns=columns)
    df = first_batch.to_pandas()
    
    return df.head(n)


def optimize_parquet_for_filtering(
    input_path: Union[str, Path],
    output_path: Union[str, Path],
    partition_cols: Optional[List[str]] = None,
    row_group_size: int = 100000
):
    """
    ä¼˜åŒ– Parquet æ–‡ä»¶ä»¥æ”¯æŒå¿«é€Ÿè¿‡æ»¤
    
    Args:
        input_path: è¾“å…¥ Parquet æ–‡ä»¶
        output_path: è¾“å‡º Parquet æ–‡ä»¶
        partition_cols: æŒ‰è¿™äº›åˆ—åˆ†åŒºï¼ˆä¾‹å¦‚ ['stay_id']ï¼‰
        row_group_size: Row group å¤§å°ï¼ˆå½±å“è¿‡æ»¤æ€§èƒ½ï¼‰
    
    ä½¿ç”¨åœºæ™¯:
        å¦‚æœä½ ç»å¸¸æŒ‰ stay_id è¿‡æ»¤ï¼Œå¯ä»¥ä¼˜åŒ–ï¼š
        optimize_parquet_for_filtering(
            'chartevents.parquet',
            'chartevents_optimized.parquet',
            partition_cols=['stay_id']
        )
    """
    df = pd.read_parquet(input_path)
    
    if partition_cols:
        # æŒ‰åˆ—æ’åºï¼Œæé«˜è¿‡æ»¤æ•ˆç‡
        df = df.sort_values(partition_cols)
    
    df.to_parquet(
        output_path,
        engine='pyarrow',
        compression='snappy',
        index=False,
        row_group_size=row_group_size,
        # partition_by=partition_cols,  # å¯é€‰ï¼šåˆ›å»ºåˆ†åŒºæ–‡ä»¶å¤¹
    )


# å…¼å®¹æ€§åˆ«å
def read_table_parquet(
    file_path: Union[str, Path],
    columns: Optional[List[str]] = None,
    filters: Optional[List[Tuple]] = None
) -> pd.DataFrame:
    """
    å…¼å®¹ datasource.py çš„æ¥å£
    """
    return read_parquet(file_path, columns=columns, filters=filters)


__all__ = [
    'read_parquet',
    'read_parquet_parallel',
    'parquet_metadata',
    'parquet_peek',
    'optimize_parquet_for_filtering',
    'read_table_parquet'
]

