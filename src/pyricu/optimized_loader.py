"""
ä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨

åŸºäºåˆ—è£å‰ªã€itemidè¿‡æ»¤å’Œæ‚£è€…è¿‡æ»¤çš„é«˜æ•ˆæ•°æ®åŠ è½½ç³»ç»Ÿ
"""

from __future__ import annotations

import logging
import time
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Optional, Set, Union, Any
import pandas as pd

try:
    import pyarrow.parquet as pq
    import pyarrow as pa
    PYARROW_AVAILABLE = True
except ImportError:
    PYARROW_AVAILABLE = False
    logging.warning("PyArrowä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨pandasåŠ è½½ï¼ˆè¾ƒæ…¢ï¼‰")

from .datasource import ICUDataSource, FilterSpec, FilterOp
from .config import DataSourceConfig

logger = logging.getLogger(__name__)


class OptimizedICUDataSource(ICUDataSource):
    """ä¼˜åŒ–çš„ICUæ•°æ®æºï¼Œæ”¯æŒåˆ—è£å‰ªå’Œæ™ºèƒ½è¿‡æ»¤"""

    def __init__(self, config: DataSourceConfig, base_path: Optional[Path] = None,
                 enable_column_pruning: bool = True, enable_itemid_filtering: bool = True):
        """
        åˆå§‹åŒ–ä¼˜åŒ–æ•°æ®æº

        Args:
            config: æ•°æ®æºé…ç½®
            base_path: æ•°æ®åŸºç¡€è·¯å¾„
            enable_column_pruning: å¯ç”¨åˆ—è£å‰ª
            enable_itemid_filtering: å¯ç”¨itemidè¿‡æ»¤
        """
        super().__init__(config=config, base_path=base_path)
        self.enable_column_pruning = enable_column_pruning
        self.enable_itemid_filtering = enable_itemid_filtering

        # SOFAç›¸å…³çš„itemidæ˜ å°„
        self.sofa_itemid_mapping = self._init_sofa_itemid_mapping()

        # åˆ—éœ€æ±‚æ˜ å°„
        self.column_requirements = self._init_column_requirements()

    def _init_sofa_itemid_mapping(self) -> Dict[str, Dict[str, List[int]]]:
        """åˆå§‹åŒ–SOFAç»„ä»¶çš„itemidæ˜ å°„"""
        return {
            'sofa_resp': {
                'chartevents': [
                    50821, 50816, 50817, 50818, 50819, 50820, 223835,  # è¡€æ°”åˆ†æ
                    220045, 220181, 223761,                          # HR, MAP, SpO2
                    223762,                                          # æ¸©åº¦
                    220052, 225312, 52, 443, 456, 6072              # å…¶ä»–è¡€å‹ç›¸å…³
                ],
                'labevents': [
                    50821, 50816, 50817, 50818, 50819, 50820, 223835   # è¡€æ°”åˆ†æ
                ]
            },
            'sofa_coag': {
                'labevents': [51265]  # è¡€å°æ¿
            },
            'sofa_liver': {
                'labevents': [50885]  # èƒ†çº¢ç´ 
            },
            'sofa_cardio': {
                'chartevents': [
                    220052, 220181, 225312, 52, 443, 456, 6072,       # MAP, è¡€å‹
                    220045                                          # HR
                ],
                'inputevents': [
                    221906, 222315, 221289, 221662, 30131, 221749,  # è¡€ç®¡æ´»æ€§è¯ç‰©
                    226208, 226209, 226210, 226211, 226212, 226213   # æ°§ç–—ç›¸å…³
                ]
            },
            'sofa_cns': {
                'chartevents': [198, 220739, 220181]  # GCSè¯„åˆ†
            },
            'sofa_renal': {
                'labevents': [50912],                    # è‚Œé…
                'outputevents': [226559, 226558, 226560]  # å°¿é‡
            }
        }

    def _init_column_requirements(self) -> Dict[str, Set[str]]:
        """åˆå§‹åŒ–å„è¡¨çš„åŸºæœ¬åˆ—éœ€æ±‚"""
        return {
            'chartevents': {'stay_id', 'charttime', 'itemid', 'valuenum'},
            'labevents': {'stay_id', 'charttime', 'itemid', 'valuenum'},
            'inputevents': {'stay_id', 'starttime', 'endtime', 'itemid', 'amount'},
            'outputevents': {'stay_id', 'charttime', 'itemid', 'value'},
            'procedureevents': {'stay_id', 'charttime', 'itemid'},
            'icustays': {'stay_id', 'subject_id', 'intime', 'outtime'},
            'patients': {'subject_id', 'gender', 'anchor_age'}
        }

    def get_required_columns(self, table_name: str, itemids: Optional[List[int]] = None) -> List[str]:
        """
        è·å–è¡¨æ‰€éœ€çš„åˆ—

        Args:
            table_name: è¡¨å
            itemids: itemidåˆ—è¡¨ï¼ˆå¯é€‰ï¼‰

        Returns:
            éœ€è¦çš„åˆ—åˆ—è¡¨
        """
        if not self.enable_column_pruning:
            return None  # è¿”å›Noneè¡¨ç¤ºè¯»å–æ‰€æœ‰åˆ—

        # åŸºç¡€åˆ—éœ€æ±‚
        required_columns = self.column_requirements.get(table_name, set()).copy()

        # æ ¹æ®itemidæ·»åŠ ç‰¹å®šåˆ—
        if itemids and table_name == 'inputevents':
            required_columns.add('rate')      # è¾“æ¶²é€Ÿç‡
            required_columns.add('rateuom')   # é€Ÿç‡å•ä½

        return sorted(list(required_columns))

    def get_relevant_itemids(self, table_name: str, concept_name: Optional[str] = None) -> List[int]:
        """
        è·å–è¡¨ç›¸å…³çš„itemid

        Args:
            table_name: è¡¨å
            concept_name: æ¦‚å¿µåç§°ï¼ˆå¯é€‰ï¼‰

        Returns:
            ç›¸å…³çš„itemidåˆ—è¡¨
        """
        if not self.enable_itemid_filtering:
            return None  # è¿”å›Noneè¡¨ç¤ºä¸è¿‡æ»¤itemid

        if concept_name and concept_name in self.sofa_itemid_mapping:
            return self.sofa_itemid_mapping[concept_name].get(table_name, [])

        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¦‚å¿µï¼Œè¿”å›æ‰€æœ‰SOFAç›¸å…³çš„itemid
        all_itemids = set()
        for component_mapping in self.sofa_itemid_mapping.values():
            all_itemids.update(component_mapping.get(table_name, []))

        return sorted(list(all_itemids)) if all_itemids else None

    def _load_raw_frame_optimized(
        self,
        table_name: str,
        columns: Optional[Iterable[str]] = None,
        patient_ids_filter: Optional[FilterSpec] = None,
        concept_name: Optional[str] = None
    ) -> pd.DataFrame:
        """
        ä¼˜åŒ–çš„åŸå§‹æ•°æ®å¸§åŠ è½½

        Args:
            table_name: è¡¨å
            columns: åˆ—åˆ—è¡¨
            patient_ids_filter: æ‚£è€…IDè¿‡æ»¤å™¨
            concept_name: æ¦‚å¿µåç§°

        Returns:
            åŠ è½½çš„æ•°æ®å¸§
        """
        start_time = time.time()

        # è·å–ä¼˜åŒ–çš„åˆ—éœ€æ±‚
        required_columns = self.get_required_columns(table_name)
        if required_columns and columns:
            # åˆå¹¶ç”¨æˆ·æŒ‡å®šçš„åˆ—å’Œå¿…éœ€çš„åˆ—
            columns = list(set(columns) | set(required_columns))
        elif required_columns:
            columns = required_columns

        # è·å–ç›¸å…³çš„itemid
        relevant_itemids = self.get_relevant_itemids(table_name, concept_name)

        # è·å–æ–‡ä»¶è·¯å¾„
        file_path = self._resolve_loader_from_disk(table_name)
        if not file_path:
            return self._handle_missing_table(table_name, columns)

        try:
            # å°è¯•ä½¿ç”¨PyArrowä¼˜åŒ–åŠ è½½
            if PYARROW_AVAILABLE:
                df = self._load_with_pyarrow(file_path, columns, patient_ids_filter, relevant_itemids)
            else:
                df = self._load_with_pandas(file_path, columns, patient_ids_filter, relevant_itemids)

            load_time = time.time() - start_time
            logger.info(f"âœ… ä¼˜åŒ–åŠ è½½ {table_name}: {len(df):,}è¡Œ, {load_time:.2f}ç§’")

            return df

        except Exception as e:
            logger.error(f"âŒ åŠ è½½å¤±è´¥ {table_name}: {e}")
            # å›é€€åˆ°åŸå§‹æ–¹æ³•
            return super()._load_raw_frame(table_name, columns, patient_ids_filter)

    def _load_with_pyarrow(
        self,
        file_path: Path,
        columns: Optional[List[str]],
        patient_ids_filter: Optional[FilterSpec],
        relevant_itemids: Optional[List[int]]
    ) -> pd.DataFrame:
        """ä½¿ç”¨PyArrowä¼˜åŒ–åŠ è½½"""
        # æ„å»ºè¿‡æ»¤å™¨
        filters = []

        # æ‚£è€…IDè¿‡æ»¤
        if patient_ids_filter and patient_ids_filter.op == FilterOp.IN:
            filters.append((patient_ids_filter.column, 'in', patient_ids_filter.value))

        # Itemidè¿‡æ»¤
        if relevant_itemids:
            filters.append(('itemid', 'in', relevant_itemids))

        # è¯»å–æ•°æ®
        dataset = pq.ParquetDataset(file_path)

        # å°è¯•ä½¿ç”¨è¿‡æ»¤å™¨ï¼Œå¦‚æœä¸æ”¯æŒåˆ™å›é€€åˆ°pandasè¿‡æ»¤
        try:
            table = dataset.read(
                columns=columns,
                filters=filters if filters else None
            )
            df = table.to_pandas()
        except TypeError as e:
            if 'filters' in str(e):
                # è¿‡æ»¤å™¨ä¸æ”¯æŒï¼Œä½¿ç”¨pandasæ–¹å¼è¯»å–åè¿‡æ»¤
                logger.warning(f"PyArrowè¿‡æ»¤å™¨ä¸æ”¯æŒï¼Œä½¿ç”¨pandasè¿‡æ»¤: {e}")
                table = dataset.read(columns=columns)
                df = table.to_pandas()

                # æ‰‹åŠ¨åº”ç”¨è¿‡æ»¤å™¨
                for filter_col, filter_op, filter_val in filters:
                    if filter_op == 'in':
                        df = df[df[filter_col].isin(filter_val)]
            else:
                raise e

        return df

    def _load_with_pandas(
        self,
        file_path: Path,
        columns: Optional[List[str]],
        patient_ids_filter: Optional[FilterSpec],
        relevant_itemids: Optional[List[int]]
    ) -> pd.DataFrame:
        """ä½¿ç”¨pandasåŠ è½½ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        # è¯»å–æ•°æ®
        df = pd.read_parquet(file_path, columns=columns, engine='pyarrow')

        # åº”ç”¨è¿‡æ»¤å™¨
        if patient_ids_filter and patient_ids_filter.op == FilterOp.IN:
            df = df[df[patient_ids_filter.column].isin(patient_ids_filter.value)]

        if relevant_itemids:
            df = df[df['itemid'].isin(relevant_itemids)]

        return df

    def _handle_missing_table(self, table_name: str, columns: Optional[List[str]]) -> pd.DataFrame:
        """å¤„ç†ç¼ºå¤±çš„è¡¨"""
        if self.config.name == 'miiv' and table_name in self.config.tables:
            # è¿”å›ç©ºDataFrameï¼Œä¿æŒåˆ—ç»“æ„
            return pd.DataFrame(columns=columns or ['index'])
        else:
            raise KeyError(f"Table not found: {table_name}")

    def load_table_optimized(
        self,
        table_name: str,
        *,
        columns: Optional[Iterable[str]] = None,
        filters: Optional[Iterable[FilterSpec]] = None,
        concept_name: Optional[str] = None,
        verbose: bool = False
    ) -> pd.DataFrame:
        """
        ä¼˜åŒ–çš„è¡¨åŠ è½½æ–¹æ³•

        Args:
            table_name: è¡¨å
            columns: åˆ—åˆ—è¡¨
            filters: è¿‡æ»¤å™¨åˆ—è¡¨
            concept_name: æ¦‚å¿µåç§°
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

        Returns:
            åŠ è½½çš„æ•°æ®è¡¨
        """
        if verbose:
            logger.info(f"ğŸ” å¼€å§‹ä¼˜åŒ–åŠ è½½è¡¨: {table_name}")

        # æå–æ‚£è€…IDè¿‡æ»¤å™¨
        patient_ids_filter = None
        if filters:
            id_columns = ['stay_id', 'subject_id', 'icustay_id', 'hadm_id',
                         'patientunitstayid', 'admissionid', 'patientid']
            for spec in filters:
                if spec.op == FilterOp.IN and spec.column in id_columns:
                    patient_ids_filter = spec
                    break

        # ä½¿ç”¨ä¼˜åŒ–çš„åŠ è½½æ–¹æ³•
        frame = self._load_raw_frame_optimized(
            table_name=table_name,
            columns=columns,
            patient_ids_filter=patient_ids_filter,
            concept_name=concept_name
        )

        # åº”ç”¨å…¶ä»–è¿‡æ»¤å™¨
        if filters:
            for spec in filters:
                frame = spec.apply(frame)

        return frame


class OptimizedLoaderFactory:
    """ä¼˜åŒ–åŠ è½½å™¨å·¥å‚"""

    @staticmethod
    def create_optimized_datasource(
        database: str,
        data_path: Optional[Path] = None,
        **kwargs
    ) -> OptimizedICUDataSource:
        """
        åˆ›å»ºä¼˜åŒ–çš„æ•°æ®æº

        Args:
            database: æ•°æ®åº“åç§°
            data_path: æ•°æ®è·¯å¾„
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            ä¼˜åŒ–çš„æ•°æ®æºå®ä¾‹
        """
        from .config import load_data_sources

        registry = load_data_sources()
        config = registry.get(database)
        if not config:
            raise ValueError(f"Unknown database: {database}")

        return OptimizedICUDataSource(config, data_path, **kwargs)

    @staticmethod
    def benchmark_loading(
        database: str,
        data_path: Path,
        table_name: str,
        patient_ids: List[int],
        concept_name: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        å¯¹æ¯”ä¼˜åŒ–å‰åçš„åŠ è½½æ€§èƒ½

        Args:
            database: æ•°æ®åº“åç§°
            data_path: æ•°æ®è·¯å¾„
            table_name: è¡¨å
            patient_ids: æ‚£è€…IDåˆ—è¡¨
            concept_name: æ¦‚å¿µåç§°

        Returns:
            æ€§èƒ½å¯¹æ¯”ç»“æœ
        """
        logger.info(f"ğŸ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•: {table_name}")

        # æµ‹è¯•ä¼ ç»ŸåŠ è½½
        traditional_start = time.time()
        try:
            traditional_source = ICUDataSource(
                config=load_data_sources().get(database),
                base_path=data_path
            )
            traditional_df = traditional_source.load_table(
                table_name,
                filters=[FilterSpec('stay_id', FilterOp.IN, patient_ids)]
            )
            traditional_time = time.time() - traditional_start
            traditional_size = len(traditional_df)
        except Exception as e:
            logger.error(f"ä¼ ç»ŸåŠ è½½å¤±è´¥: {e}")
            traditional_time = float('inf')
            traditional_size = 0
            traditional_df = pd.DataFrame()

        # æµ‹è¯•ä¼˜åŒ–åŠ è½½
        optimized_start = time.time()
        try:
            optimized_source = OptimizedICUDataSource(
                config=load_data_sources().get(database),
                base_path=data_path
            )
            optimized_df = optimized_source.load_table_optimized(
                table_name,
                filters=[FilterSpec('stay_id', FilterOp.IN, patient_ids)],
                concept_name=concept_name
            )
            optimized_time = time.time() - optimized_start
            optimized_size = len(optimized_df)
        except Exception as e:
            logger.error(f"ä¼˜åŒ–åŠ è½½å¤±è´¥: {e}")
            optimized_time = float('inf')
            optimized_size = 0
            optimized_df = pd.DataFrame()

        # è®¡ç®—æ€§èƒ½æå‡
        speedup = traditional_time / optimized_time if optimized_time > 0 else float('inf')
        size_reduction = (traditional_size - optimized_size) / traditional_size if traditional_size > 0 else 0

        result = {
            'table_name': table_name,
            'concept_name': concept_name,
            'patient_count': len(patient_ids),
            'traditional_time': traditional_time,
            'optimized_time': optimized_time,
            'speedup': speedup,
            'traditional_rows': traditional_size,
            'optimized_rows': optimized_size,
            'size_reduction_percent': size_reduction * 100,
            'memory_saving_mb': (traditional_size - optimized_size) * 0.1  # ä¼°ç®—
        }

        logger.info(f"ğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœ {table_name}:")
        logger.info(f"   æ—¶é—´: {traditional_time:.2f}s â†’ {optimized_time:.2f}s ({speedup:.1f}x)")
        logger.info(f"   è¡Œæ•°: {traditional_size:,} â†’ {optimized_size:,} ({size_reduction*100:.1f}% å‡å°‘)")

        return result


# å…¨å±€ä¼˜åŒ–åŠ è½½å™¨å®ä¾‹
_optimized_sources = {}


def get_optimized_datasource(
    database: str,
    data_path: Optional[Path] = None,
    **kwargs
) -> OptimizedICUDataSource:
    """
    è·å–ä¼˜åŒ–çš„æ•°æ®æºå®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰

    Args:
        database: æ•°æ®åº“åç§°
        data_path: æ•°æ®è·¯å¾„
        **kwargs: å…¶ä»–å‚æ•°

    Returns:
        ä¼˜åŒ–çš„æ•°æ®æºå®ä¾‹
    """
    key = (database, str(data_path), frozenset(kwargs.items()))

    if key not in _optimized_sources:
        _optimized_sources[key] = OptimizedLoaderFactory.create_optimized_datasource(
            database, data_path, **kwargs
        )

    return _optimized_sources[key]